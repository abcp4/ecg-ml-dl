{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('../../'))\n",
    "from utils import utils\n",
    "from utils.utils import evaluate_experiment\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from timeseries_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from scipy.fftpack import fft\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import _LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch      : 1.5.1\n",
      "numpy      : 1.18.1\n",
      "pandas     : 1.0.5\n",
      "device     : GeForce GTX 1080\n"
     ]
    }
   ],
   "source": [
    "print('torch      :', torch.__version__)\n",
    "print('numpy      :', np.__version__)\n",
    "print('pandas     :', pd.__version__)\n",
    "iscuda = torch.cuda.is_available()\n",
    "if iscuda:\n",
    "    print(f'device     : {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data='../../../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.load(os.path.abspath(path_to_data+'data-002.npy'),allow_pickle=True)\n",
    "Y=np.load(path_to_data+'Y.npy',allow_pickle=True)\n",
    "labels =pd.read_csv(path_to_data+'labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fold=8\n",
    "val_fold=9\n",
    "test_fold=10\n",
    "\n",
    "# 10th fold for testing (9th for now)\n",
    "X_test = data[labels.strat_fold == test_fold]\n",
    "y_test = Y[labels.strat_fold == test_fold]\n",
    "# 9th fold for validation (8th for now)\n",
    "X_val = data[labels.strat_fold == val_fold]\n",
    "y_val = Y[labels.strat_fold == val_fold]\n",
    "# rest for training\n",
    "X_train = data[labels.strat_fold <= train_fold]\n",
    "y_train = Y[labels.strat_fold <= train_fold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17111, 1000, 12), (2156, 1000, 12))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape ,X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess signal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test = utils.preprocess_signals(X_train, X_val, X_test,'/content/')\n",
    "n_classes = y_train.shape[1]\n",
    "X_train = np.reshape(X_train,[X_train.shape[0],X_train.shape[2],X_train.shape[1]])\n",
    "X_val = np.reshape(X_val,[X_val.shape[0],X_val.shape[2],X_val.shape[1]])\n",
    "X_test = np.reshape(X_test,[X_test.shape[0],X_test.shape[2],X_test.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17111, 12, 1000), (2156, 12, 1000))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17111, 5), (2156, 5))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_eval = y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get fourier transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2156, 5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2156,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(y_val.shape)\n",
    "y_val_list = np.asarray([np.where(r==1)[0][0] for r in y_val])\n",
    "y_train_list = np.asarray([np.where(r==1)[0][0] for r in y_train])\n",
    "y_val_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([17111, 12, 1000]), torch.Size([17111, 12, 1000]), (17111, 5))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N=1000\n",
    "X_train_fft = fft(X_train)\n",
    "X_train_fft = (2/N) * np.abs(X_train_fft)\n",
    "\n",
    "X_val_fft = fft(X_val)\n",
    "X_val_fft = (2/N) * np.abs(X_val_fft)\n",
    "X_train, X_val = [torch.tensor(arr, dtype=torch.float32) for arr in (X_train, X_val)]\n",
    "X_train_fft, X_val_fft = [torch.tensor(arr, dtype=torch.float32) for arr in (X_train_fft, X_val_fft)]\n",
    "y_train_tensor, y_val_tensor = [torch.tensor(arr, dtype=torch.long) for arr in (y_train_list, y_val_list)]\n",
    "train_ds = TensorDataset(X_train, X_train_fft, y_train_tensor)\n",
    "valid_ds = TensorDataset(X_val, X_val_fft, y_val_tensor)\n",
    "\n",
    "X_train.shape, X_train_fft.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#batch size\n",
    "bs=256\n",
    "jobs=1\n",
    "train_dl = DataLoader(train_ds, bs, shuffle=True, num_workers=jobs)\n",
    "valid_dl = DataLoader(valid_ds, bs, shuffle=False, num_workers=jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _SepConv1d(nn.Module):\n",
    "    \"\"\"A simple separable convolution implementation.\n",
    "    \n",
    "    The separable convlution is a method to reduce number of the parameters \n",
    "    in the deep learning network for slight decrease in predictions quality.\n",
    "    \"\"\"\n",
    "    def __init__(self, ni, no, kernel, stride, pad):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv1d(ni, ni, kernel, stride, padding=pad, groups=ni)\n",
    "        self.pointwise = nn.Conv1d(ni, no, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pointwise(self.depthwise(x))\n",
    "\n",
    "class SepConv1d(nn.Module):\n",
    "    \"\"\"Implementes a 1-d convolution with 'batteries included'.\n",
    "    \n",
    "    The module adds (optionally) activation function and dropout layers right after\n",
    "    a separable convolution layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, ni, no, kernel, stride, pad, drop=None,\n",
    "                 activ=lambda: nn.ReLU(inplace=True)):\n",
    "    \n",
    "        super().__init__()\n",
    "        assert drop is None or (0.0 < drop < 1.0)\n",
    "        layers = [_SepConv1d(ni, no, kernel, stride, pad)]\n",
    "        if activ:\n",
    "            layers.append(activ())\n",
    "        if drop is not None:\n",
    "            layers.append(nn.Dropout(drop))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    \"\"\"Converts N-dimensional tensor into 'flat' one.\"\"\"\n",
    "\n",
    "    def __init__(self, keep_batch_dim=True):\n",
    "        super().__init__()\n",
    "        self.keep_batch_dim = keep_batch_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.keep_batch_dim:\n",
    "            return x.view(x.size(0), -1)\n",
    "        return x.view(-1)\n",
    "    \n",
    "class CyclicLR(_LRScheduler):\n",
    "    \n",
    "    def __init__(self, optimizer, schedule, last_epoch=-1):\n",
    "        assert callable(schedule)\n",
    "        self.schedule = schedule\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.schedule(self.last_epoch, lr) for lr in self.base_lrs]\n",
    "\n",
    "def cosine(t_max, eta_min=0):\n",
    "    \n",
    "    def scheduler(epoch, base_lr):\n",
    "        t = epoch % t_max\n",
    "        return eta_min + (base_lr - eta_min)*(1 + np.cos(np.pi*t/t_max))/2\n",
    "    \n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Branched Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "class Classifier(nn.Module):\n",
    "    #https://www.kaggle.com/purplejester/pytorch-deep-time-series-classification\n",
    "    def __init__(self, raw_ni, fft_ni, no, drop=.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.raw = nn.Sequential(\n",
    "            SepConv1d(raw_ni,  32, 5, 2, 3, drop=drop),\n",
    "            SepConv1d(    32,  64, 5, 4, 2, drop=drop),\n",
    "            SepConv1d(    64, 128, 5, 4, 2, drop=drop),\n",
    "            SepConv1d(   128, 256, 5, 4, 2),\n",
    "            Flatten(),\n",
    "            nn.Dropout(drop), nn.Linear(2048, 64), nn.ReLU(inplace=True), # here was 256,64\n",
    "            nn.Dropout(drop), nn.Linear(64, 64), nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.fft = nn.Sequential(\n",
    "            SepConv1d(fft_ni,  32, 5, 2, 3, drop=drop),\n",
    "            SepConv1d(    32,  64, 5, 4, 2, drop=drop),\n",
    "            SepConv1d(    64, 128, 5, 4, 2, drop=drop),\n",
    "            SepConv1d(   128, 128, 5, 4, 2, drop=drop),\n",
    "            SepConv1d(   128, 256, 5, 4, 2),\n",
    "            Flatten(),\n",
    "            nn.Dropout(drop), nn.Linear(512, 64), nn.ReLU(inplace=True), # here was 256,64\n",
    "            nn.Dropout(drop), nn.Linear(64, 64), nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(128, 64), nn.ReLU(inplace=True), nn.Linear(64, no))\n",
    "        \n",
    "    def forward(self, t_raw, t_fft):\n",
    "        raw_out = self.raw(t_raw)\n",
    "        fft_out = self.fft(t_fft)\n",
    "        t_in = torch.cat([raw_out, fft_out], dim=1)\n",
    "        out = self.out(t_in)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([17111, 12, 1000]), torch.Size([17111, 12, 1000]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = 0.01\n",
    "#lr = 0.0005\n",
    "n_epochs = 3000\n",
    "iterations_per_epoch = len(train_ds)\n",
    "num_classes = 5\n",
    "best_acc = 0\n",
    "patience, trials = 500, 0\n",
    "base = 1\n",
    "step = 2\n",
    "loss_history = []\n",
    "acc_history = []\n",
    "\n",
    "display((X_train.shape, X_train_fft.shape))\n",
    "model = Classifier(X_train.shape[1], X_train_fft.shape[1], num_classes).to(device)\n",
    "#model = fcn_wang(num_classes=5,input_channels=X_train.shape[1]).to(device)\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#opt = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "#sched = CyclicLR(opt, cosine(t_max=iterations_per_epoch * 2, eta_min=lr/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start model training\n",
      "Epoch:   1. Loss: 1.4611. Acc.: 42.67%\n",
      "Epoch 1 best model saved with accuracy: 42.67%\n",
      "Epoch:   2. Loss: 1.4449. Acc.: 42.67%\n",
      "Epoch:   3. Loss: 1.4255. Acc.: 46.20%\n",
      "Epoch 3 best model saved with accuracy: 46.20%\n",
      "Epoch:   4. Loss: 1.3645. Acc.: 48.52%\n",
      "Epoch 4 best model saved with accuracy: 48.52%\n",
      "Epoch:   5. Loss: 1.3423. Acc.: 49.30%\n",
      "Epoch 5 best model saved with accuracy: 49.30%\n",
      "Epoch:   6. Loss: 1.3296. Acc.: 49.07%\n",
      "Epoch:   7. Loss: 1.3186. Acc.: 49.77%\n",
      "Epoch 7 best model saved with accuracy: 49.77%\n",
      "Epoch:   8. Loss: 1.3143. Acc.: 49.63%\n",
      "Epoch:   9. Loss: 1.3130. Acc.: 49.68%\n",
      "Epoch:  10. Loss: 1.3089. Acc.: 49.26%\n",
      "Epoch:  11. Loss: 1.3051. Acc.: 49.21%\n",
      "Epoch:  12. Loss: 1.3032. Acc.: 50.32%\n",
      "Epoch 12 best model saved with accuracy: 50.32%\n",
      "Epoch:  13. Loss: 1.2969. Acc.: 50.42%\n",
      "Epoch 13 best model saved with accuracy: 50.42%\n",
      "Epoch:  14. Loss: 1.2958. Acc.: 50.56%\n",
      "Epoch 14 best model saved with accuracy: 50.56%\n",
      "Epoch:  15. Loss: 1.2902. Acc.: 50.32%\n",
      "Epoch:  16. Loss: 1.2910. Acc.: 50.05%\n",
      "Epoch:  17. Loss: 1.2847. Acc.: 50.60%\n",
      "Epoch 17 best model saved with accuracy: 50.60%\n",
      "Epoch:  18. Loss: 1.2832. Acc.: 50.79%\n",
      "Epoch 18 best model saved with accuracy: 50.79%\n",
      "Epoch:  19. Loss: 1.2735. Acc.: 50.05%\n",
      "Epoch:  20. Loss: 1.2691. Acc.: 50.88%\n",
      "Epoch 20 best model saved with accuracy: 50.88%\n",
      "Epoch:  21. Loss: 1.2619. Acc.: 51.53%\n",
      "Epoch 21 best model saved with accuracy: 51.53%\n",
      "Epoch:  22. Loss: 1.2552. Acc.: 50.93%\n",
      "Epoch:  23. Loss: 1.2488. Acc.: 52.37%\n",
      "Epoch 23 best model saved with accuracy: 52.37%\n",
      "Epoch:  24. Loss: 1.2443. Acc.: 50.97%\n",
      "Epoch:  25. Loss: 1.2364. Acc.: 50.88%\n",
      "Epoch:  26. Loss: 1.2283. Acc.: 50.93%\n",
      "Epoch:  27. Loss: 1.2211. Acc.: 51.35%\n",
      "Epoch:  28. Loss: 1.2108. Acc.: 51.95%\n",
      "Epoch:  29. Loss: 1.2035. Acc.: 52.04%\n",
      "Epoch:  30. Loss: 1.2006. Acc.: 51.48%\n",
      "Epoch:  31. Loss: 1.1868. Acc.: 52.88%\n",
      "Epoch 31 best model saved with accuracy: 52.88%\n",
      "Epoch:  32. Loss: 1.1865. Acc.: 52.32%\n",
      "Epoch:  33. Loss: 1.1833. Acc.: 53.99%\n",
      "Epoch 33 best model saved with accuracy: 53.99%\n",
      "Epoch:  34. Loss: 1.1717. Acc.: 53.48%\n",
      "Epoch:  35. Loss: 1.1579. Acc.: 53.85%\n",
      "Epoch:  36. Loss: 1.1567. Acc.: 54.36%\n",
      "Epoch 36 best model saved with accuracy: 54.36%\n",
      "Epoch:  37. Loss: 1.1508. Acc.: 54.78%\n",
      "Epoch 37 best model saved with accuracy: 54.78%\n",
      "Epoch:  38. Loss: 1.1437. Acc.: 53.94%\n",
      "Epoch:  39. Loss: 1.1414. Acc.: 55.01%\n",
      "Epoch 39 best model saved with accuracy: 55.01%\n",
      "Epoch:  40. Loss: 1.1307. Acc.: 54.08%\n",
      "Epoch:  41. Loss: 1.1287. Acc.: 53.80%\n",
      "Epoch:  42. Loss: 1.1262. Acc.: 53.94%\n",
      "Epoch:  43. Loss: 1.1247. Acc.: 54.50%\n",
      "Epoch:  44. Loss: 1.1171. Acc.: 53.25%\n",
      "Epoch:  45. Loss: 1.1132. Acc.: 53.53%\n",
      "Epoch:  46. Loss: 1.1118. Acc.: 52.13%\n",
      "Epoch:  47. Loss: 1.1077. Acc.: 54.59%\n",
      "Epoch:  48. Loss: 1.1071. Acc.: 54.45%\n",
      "Epoch:  49. Loss: 1.1090. Acc.: 53.06%\n",
      "Epoch:  50. Loss: 1.1038. Acc.: 54.68%\n",
      "Epoch:  51. Loss: 1.1042. Acc.: 55.06%\n",
      "Epoch 51 best model saved with accuracy: 55.06%\n",
      "Epoch:  52. Loss: 1.1011. Acc.: 54.41%\n",
      "Epoch:  53. Loss: 1.0943. Acc.: 53.90%\n",
      "Epoch:  54. Loss: 1.0913. Acc.: 54.96%\n",
      "Epoch:  55. Loss: 1.0930. Acc.: 52.78%\n",
      "Epoch:  56. Loss: 1.0863. Acc.: 53.71%\n",
      "Epoch:  57. Loss: 1.0903. Acc.: 54.82%\n",
      "Epoch:  58. Loss: 1.0885. Acc.: 53.34%\n",
      "Epoch:  59. Loss: 1.0923. Acc.: 53.99%\n",
      "Epoch:  60. Loss: 1.0883. Acc.: 52.74%\n",
      "Epoch:  61. Loss: 1.0856. Acc.: 54.31%\n",
      "Epoch:  62. Loss: 1.0852. Acc.: 54.13%\n",
      "Epoch:  63. Loss: 1.0773. Acc.: 54.55%\n",
      "Epoch:  64. Loss: 1.0802. Acc.: 54.41%\n",
      "Epoch:  65. Loss: 1.0728. Acc.: 53.53%\n",
      "Epoch:  66. Loss: 1.0729. Acc.: 54.82%\n",
      "Epoch:  67. Loss: 1.0794. Acc.: 53.85%\n",
      "Epoch:  68. Loss: 1.0701. Acc.: 53.11%\n",
      "Epoch:  69. Loss: 1.0749. Acc.: 53.20%\n",
      "Epoch:  70. Loss: 1.0703. Acc.: 53.99%\n",
      "Epoch:  71. Loss: 1.0679. Acc.: 54.27%\n",
      "Epoch:  72. Loss: 1.0675. Acc.: 55.24%\n",
      "Epoch 72 best model saved with accuracy: 55.24%\n",
      "Epoch:  73. Loss: 1.0603. Acc.: 54.55%\n",
      "Epoch:  74. Loss: 1.0563. Acc.: 54.17%\n",
      "Epoch:  75. Loss: 1.0715. Acc.: 53.11%\n",
      "Epoch:  76. Loss: 1.0690. Acc.: 54.36%\n",
      "Epoch:  77. Loss: 1.0552. Acc.: 55.43%\n",
      "Epoch 77 best model saved with accuracy: 55.43%\n",
      "Epoch:  78. Loss: 1.0579. Acc.: 53.85%\n",
      "Epoch:  79. Loss: 1.0602. Acc.: 52.23%\n",
      "Epoch:  80. Loss: 1.0537. Acc.: 54.22%\n",
      "Epoch:  81. Loss: 1.0518. Acc.: 54.59%\n",
      "Epoch:  82. Loss: 1.0560. Acc.: 54.92%\n",
      "Epoch:  83. Loss: 1.0496. Acc.: 53.85%\n",
      "Epoch:  84. Loss: 1.0497. Acc.: 54.59%\n",
      "Epoch:  85. Loss: 1.0561. Acc.: 54.13%\n",
      "Epoch:  86. Loss: 1.0479. Acc.: 53.66%\n",
      "Epoch:  87. Loss: 1.0460. Acc.: 54.64%\n",
      "Epoch:  88. Loss: 1.0462. Acc.: 53.80%\n",
      "Epoch:  89. Loss: 1.0513. Acc.: 53.80%\n",
      "Epoch:  90. Loss: 1.0487. Acc.: 54.08%\n",
      "Epoch:  91. Loss: 1.0490. Acc.: 56.68%\n",
      "Epoch 91 best model saved with accuracy: 56.68%\n",
      "Epoch:  92. Loss: 1.0430. Acc.: 54.17%\n",
      "Epoch:  93. Loss: 1.0398. Acc.: 55.06%\n",
      "Epoch:  94. Loss: 1.0444. Acc.: 53.53%\n",
      "Epoch:  95. Loss: 1.0365. Acc.: 54.45%\n",
      "Epoch:  96. Loss: 1.0430. Acc.: 54.92%\n",
      "Epoch:  97. Loss: 1.0372. Acc.: 54.96%\n",
      "Epoch:  98. Loss: 1.0371. Acc.: 54.41%\n",
      "Epoch:  99. Loss: 1.0393. Acc.: 55.66%\n",
      "Epoch: 100. Loss: 1.0352. Acc.: 56.22%\n",
      "Epoch: 101. Loss: 1.0402. Acc.: 55.10%\n",
      "Epoch: 102. Loss: 1.0331. Acc.: 55.61%\n",
      "Epoch: 103. Loss: 1.0319. Acc.: 55.71%\n",
      "Epoch: 104. Loss: 1.0372. Acc.: 56.22%\n",
      "Epoch: 105. Loss: 1.0311. Acc.: 55.19%\n",
      "Epoch: 106. Loss: 1.0331. Acc.: 55.66%\n",
      "Epoch: 107. Loss: 1.0272. Acc.: 55.10%\n",
      "Epoch: 108. Loss: 1.0296. Acc.: 55.89%\n",
      "Epoch: 109. Loss: 1.0272. Acc.: 54.64%\n",
      "Epoch: 110. Loss: 1.0277. Acc.: 52.74%\n",
      "Epoch: 111. Loss: 1.0303. Acc.: 54.87%\n",
      "Epoch: 112. Loss: 1.0253. Acc.: 56.08%\n",
      "Epoch: 113. Loss: 1.0285. Acc.: 54.92%\n",
      "Epoch: 114. Loss: 1.0308. Acc.: 54.31%\n",
      "Epoch: 115. Loss: 1.0338. Acc.: 55.84%\n",
      "Epoch: 116. Loss: 1.0314. Acc.: 54.55%\n",
      "Epoch: 117. Loss: 1.0248. Acc.: 56.45%\n",
      "Epoch: 118. Loss: 1.0252. Acc.: 55.80%\n",
      "Epoch: 119. Loss: 1.0227. Acc.: 55.29%\n",
      "Epoch: 120. Loss: 1.0194. Acc.: 56.40%\n",
      "Epoch: 121. Loss: 1.0219. Acc.: 56.40%\n",
      "Epoch: 122. Loss: 1.0218. Acc.: 56.31%\n",
      "Epoch: 123. Loss: 1.0232. Acc.: 53.85%\n",
      "Epoch: 124. Loss: 1.0137. Acc.: 55.94%\n",
      "Epoch: 125. Loss: 1.0235. Acc.: 55.75%\n",
      "Epoch: 126. Loss: 1.0154. Acc.: 55.19%\n",
      "Epoch: 127. Loss: 1.0151. Acc.: 53.48%\n",
      "Epoch: 128. Loss: 1.0208. Acc.: 56.12%\n",
      "Epoch: 129. Loss: 1.0180. Acc.: 56.45%\n",
      "Epoch: 130. Loss: 1.0145. Acc.: 56.03%\n",
      "Epoch: 131. Loss: 1.0178. Acc.: 55.10%\n",
      "Epoch: 132. Loss: 1.0214. Acc.: 55.15%\n",
      "Epoch: 133. Loss: 1.0110. Acc.: 56.03%\n",
      "Epoch: 134. Loss: 1.0103. Acc.: 56.26%\n",
      "Epoch: 135. Loss: 1.0123. Acc.: 55.47%\n",
      "Epoch: 136. Loss: 1.0076. Acc.: 55.89%\n",
      "Epoch: 137. Loss: 1.0102. Acc.: 54.87%\n",
      "Epoch: 138. Loss: 1.0088. Acc.: 55.66%\n",
      "Epoch: 139. Loss: 1.0074. Acc.: 55.89%\n",
      "Epoch: 140. Loss: 1.0122. Acc.: 55.38%\n",
      "Epoch: 141. Loss: 1.0103. Acc.: 55.61%\n",
      "Epoch: 142. Loss: 1.0080. Acc.: 56.17%\n",
      "Epoch: 143. Loss: 1.0092. Acc.: 56.40%\n",
      "Epoch: 144. Loss: 1.0070. Acc.: 55.71%\n",
      "Epoch: 145. Loss: 1.0097. Acc.: 56.73%\n",
      "Epoch 145 best model saved with accuracy: 56.73%\n",
      "Epoch: 146. Loss: 1.0090. Acc.: 56.59%\n",
      "Epoch: 147. Loss: 1.0039. Acc.: 55.89%\n",
      "Epoch: 148. Loss: 1.0036. Acc.: 55.06%\n",
      "Epoch: 149. Loss: 1.0064. Acc.: 53.80%\n",
      "Epoch: 150. Loss: 0.9991. Acc.: 56.91%\n",
      "Epoch 150 best model saved with accuracy: 56.91%\n",
      "Epoch: 151. Loss: 1.0093. Acc.: 55.47%\n",
      "Epoch: 152. Loss: 1.0022. Acc.: 56.03%\n",
      "Epoch: 153. Loss: 1.0042. Acc.: 56.17%\n",
      "Epoch: 154. Loss: 1.0016. Acc.: 56.03%\n",
      "Epoch: 155. Loss: 1.0025. Acc.: 55.57%\n",
      "Epoch: 156. Loss: 0.9972. Acc.: 55.01%\n",
      "Epoch: 157. Loss: 0.9997. Acc.: 55.71%\n",
      "Epoch: 158. Loss: 0.9967. Acc.: 55.84%\n",
      "Epoch: 159. Loss: 1.0091. Acc.: 56.12%\n",
      "Epoch: 160. Loss: 0.9987. Acc.: 56.08%\n",
      "Epoch: 161. Loss: 0.9960. Acc.: 55.06%\n",
      "Epoch: 162. Loss: 0.9999. Acc.: 55.24%\n",
      "Epoch: 163. Loss: 0.9933. Acc.: 55.47%\n",
      "Epoch: 164. Loss: 0.9959. Acc.: 55.15%\n",
      "Epoch: 165. Loss: 0.9918. Acc.: 54.55%\n",
      "Epoch: 166. Loss: 0.9939. Acc.: 55.29%\n",
      "Epoch: 167. Loss: 0.9960. Acc.: 54.96%\n",
      "Epoch: 168. Loss: 0.9897. Acc.: 56.26%\n",
      "Epoch: 169. Loss: 0.9960. Acc.: 56.91%\n",
      "Epoch: 170. Loss: 0.9893. Acc.: 55.38%\n",
      "Epoch: 171. Loss: 0.9920. Acc.: 56.12%\n",
      "Epoch: 172. Loss: 0.9966. Acc.: 55.66%\n",
      "Epoch: 173. Loss: 0.9957. Acc.: 55.52%\n",
      "Epoch: 174. Loss: 0.9896. Acc.: 55.71%\n",
      "Epoch: 175. Loss: 0.9935. Acc.: 54.13%\n",
      "Epoch: 176. Loss: 0.9925. Acc.: 55.10%\n",
      "Epoch: 177. Loss: 0.9860. Acc.: 56.08%\n",
      "Epoch: 178. Loss: 0.9956. Acc.: 56.03%\n",
      "Epoch: 179. Loss: 0.9935. Acc.: 56.26%\n",
      "Epoch: 180. Loss: 0.9930. Acc.: 57.33%\n",
      "Epoch 180 best model saved with accuracy: 57.33%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 181. Loss: 0.9893. Acc.: 56.26%\n",
      "Epoch: 182. Loss: 0.9938. Acc.: 56.49%\n",
      "Epoch: 183. Loss: 0.9926. Acc.: 57.19%\n",
      "Epoch: 184. Loss: 0.9909. Acc.: 55.98%\n",
      "Epoch: 185. Loss: 0.9843. Acc.: 57.28%\n",
      "Epoch: 186. Loss: 0.9841. Acc.: 56.31%\n",
      "Epoch: 187. Loss: 0.9878. Acc.: 56.22%\n",
      "Epoch: 188. Loss: 0.9843. Acc.: 55.89%\n",
      "Epoch: 189. Loss: 0.9844. Acc.: 55.38%\n",
      "Epoch: 190. Loss: 0.9828. Acc.: 55.75%\n",
      "Epoch: 191. Loss: 0.9803. Acc.: 56.54%\n",
      "Epoch: 192. Loss: 0.9788. Acc.: 55.66%\n",
      "Epoch: 193. Loss: 0.9803. Acc.: 56.22%\n",
      "Epoch: 194. Loss: 0.9897. Acc.: 56.59%\n",
      "Epoch: 195. Loss: 0.9856. Acc.: 56.54%\n",
      "Epoch: 196. Loss: 0.9774. Acc.: 56.35%\n",
      "Epoch: 197. Loss: 0.9836. Acc.: 55.15%\n",
      "Epoch: 198. Loss: 0.9808. Acc.: 55.66%\n",
      "Epoch: 199. Loss: 0.9828. Acc.: 55.24%\n",
      "Epoch: 200. Loss: 0.9843. Acc.: 55.80%\n",
      "Epoch: 201. Loss: 0.9759. Acc.: 56.96%\n",
      "Epoch: 202. Loss: 0.9854. Acc.: 56.17%\n",
      "Epoch: 203. Loss: 0.9815. Acc.: 55.29%\n",
      "Epoch: 204. Loss: 0.9776. Acc.: 56.86%\n",
      "Epoch: 205. Loss: 0.9849. Acc.: 55.57%\n",
      "Epoch: 206. Loss: 0.9800. Acc.: 56.86%\n",
      "Epoch: 207. Loss: 0.9766. Acc.: 56.31%\n",
      "Epoch: 208. Loss: 0.9760. Acc.: 55.57%\n",
      "Epoch: 209. Loss: 0.9772. Acc.: 55.57%\n",
      "Epoch: 210. Loss: 0.9736. Acc.: 56.40%\n",
      "Epoch: 211. Loss: 0.9802. Acc.: 56.08%\n",
      "Epoch: 212. Loss: 0.9733. Acc.: 55.84%\n",
      "Epoch: 213. Loss: 0.9740. Acc.: 57.00%\n",
      "Epoch: 214. Loss: 0.9781. Acc.: 56.77%\n",
      "Epoch: 215. Loss: 0.9783. Acc.: 56.12%\n",
      "Epoch: 216. Loss: 0.9744. Acc.: 56.12%\n",
      "Epoch: 217. Loss: 0.9740. Acc.: 55.24%\n",
      "Epoch: 218. Loss: 0.9692. Acc.: 56.12%\n",
      "Epoch: 219. Loss: 0.9722. Acc.: 54.73%\n",
      "Epoch: 220. Loss: 0.9737. Acc.: 55.66%\n",
      "Epoch: 221. Loss: 0.9704. Acc.: 55.01%\n",
      "Epoch: 222. Loss: 0.9729. Acc.: 55.80%\n",
      "Epoch: 223. Loss: 0.9741. Acc.: 56.49%\n",
      "Epoch: 224. Loss: 0.9678. Acc.: 56.45%\n",
      "Epoch: 225. Loss: 0.9680. Acc.: 55.94%\n",
      "Epoch: 226. Loss: 0.9713. Acc.: 56.12%\n",
      "Epoch: 227. Loss: 0.9717. Acc.: 57.05%\n",
      "Epoch: 228. Loss: 0.9750. Acc.: 55.10%\n",
      "Epoch: 229. Loss: 0.9690. Acc.: 57.61%\n",
      "Epoch 229 best model saved with accuracy: 57.61%\n",
      "Epoch: 230. Loss: 0.9652. Acc.: 56.12%\n",
      "Epoch: 231. Loss: 0.9644. Acc.: 56.45%\n",
      "Epoch: 232. Loss: 0.9731. Acc.: 56.96%\n",
      "Epoch: 233. Loss: 0.9703. Acc.: 56.08%\n",
      "Epoch: 234. Loss: 0.9685. Acc.: 55.84%\n",
      "Epoch: 235. Loss: 0.9687. Acc.: 56.17%\n",
      "Epoch: 236. Loss: 0.9670. Acc.: 57.05%\n",
      "Epoch: 237. Loss: 0.9697. Acc.: 55.47%\n",
      "Epoch: 238. Loss: 0.9696. Acc.: 56.35%\n",
      "Epoch: 239. Loss: 0.9613. Acc.: 56.08%\n",
      "Epoch: 240. Loss: 0.9667. Acc.: 55.84%\n",
      "Epoch: 241. Loss: 0.9692. Acc.: 57.19%\n",
      "Epoch: 242. Loss: 0.9615. Acc.: 54.45%\n",
      "Epoch: 243. Loss: 0.9600. Acc.: 54.92%\n",
      "Epoch: 244. Loss: 0.9631. Acc.: 55.94%\n",
      "Epoch: 245. Loss: 0.9675. Acc.: 54.41%\n",
      "Epoch: 246. Loss: 0.9623. Acc.: 54.55%\n",
      "Epoch: 247. Loss: 0.9587. Acc.: 55.24%\n",
      "Epoch: 248. Loss: 0.9653. Acc.: 55.75%\n",
      "Epoch: 249. Loss: 0.9615. Acc.: 55.98%\n",
      "Epoch: 250. Loss: 0.9620. Acc.: 55.15%\n",
      "Epoch: 251. Loss: 0.9578. Acc.: 54.50%\n",
      "Epoch: 252. Loss: 0.9619. Acc.: 55.52%\n",
      "Epoch: 253. Loss: 0.9635. Acc.: 56.86%\n",
      "Epoch: 254. Loss: 0.9526. Acc.: 55.94%\n",
      "Epoch: 255. Loss: 0.9611. Acc.: 55.47%\n",
      "Epoch: 256. Loss: 0.9568. Acc.: 55.84%\n",
      "Epoch: 257. Loss: 0.9558. Acc.: 56.45%\n",
      "Epoch: 258. Loss: 0.9565. Acc.: 56.68%\n",
      "Epoch: 259. Loss: 0.9582. Acc.: 56.35%\n",
      "Epoch: 260. Loss: 0.9565. Acc.: 57.19%\n",
      "Epoch: 261. Loss: 0.9607. Acc.: 57.14%\n",
      "Epoch: 262. Loss: 0.9610. Acc.: 55.06%\n",
      "Epoch: 263. Loss: 0.9579. Acc.: 56.03%\n",
      "Epoch: 264. Loss: 0.9624. Acc.: 56.59%\n",
      "Epoch: 265. Loss: 0.9677. Acc.: 56.59%\n",
      "Epoch: 266. Loss: 0.9562. Acc.: 55.89%\n",
      "Epoch: 267. Loss: 0.9593. Acc.: 55.75%\n",
      "Epoch: 268. Loss: 0.9540. Acc.: 55.33%\n",
      "Epoch: 269. Loss: 0.9549. Acc.: 54.96%\n",
      "Epoch: 270. Loss: 0.9551. Acc.: 56.26%\n",
      "Epoch: 271. Loss: 0.9599. Acc.: 54.68%\n",
      "Epoch: 272. Loss: 0.9521. Acc.: 56.17%\n",
      "Epoch: 273. Loss: 0.9533. Acc.: 55.71%\n",
      "Epoch: 274. Loss: 0.9513. Acc.: 56.59%\n",
      "Epoch: 275. Loss: 0.9512. Acc.: 55.89%\n",
      "Epoch: 276. Loss: 0.9556. Acc.: 56.40%\n",
      "Epoch: 277. Loss: 0.9518. Acc.: 55.80%\n",
      "Epoch: 278. Loss: 0.9579. Acc.: 55.52%\n",
      "Epoch: 279. Loss: 0.9540. Acc.: 55.52%\n",
      "Epoch: 280. Loss: 0.9546. Acc.: 56.17%\n",
      "Epoch: 281. Loss: 0.9522. Acc.: 56.31%\n",
      "Epoch: 282. Loss: 0.9541. Acc.: 55.33%\n",
      "Epoch: 283. Loss: 0.9553. Acc.: 55.57%\n",
      "Epoch: 284. Loss: 0.9541. Acc.: 55.15%\n",
      "Epoch: 285. Loss: 0.9569. Acc.: 56.31%\n",
      "Epoch: 286. Loss: 0.9525. Acc.: 56.31%\n",
      "Epoch: 287. Loss: 0.9517. Acc.: 55.71%\n",
      "Epoch: 288. Loss: 0.9541. Acc.: 55.06%\n",
      "Epoch: 289. Loss: 0.9507. Acc.: 55.47%\n",
      "Epoch: 290. Loss: 0.9483. Acc.: 54.41%\n",
      "Epoch: 291. Loss: 0.9534. Acc.: 55.71%\n",
      "Epoch: 292. Loss: 0.9520. Acc.: 56.63%\n",
      "Epoch: 293. Loss: 0.9545. Acc.: 54.73%\n",
      "Epoch: 294. Loss: 0.9576. Acc.: 55.57%\n",
      "Epoch: 295. Loss: 0.9467. Acc.: 55.01%\n",
      "Epoch: 296. Loss: 0.9456. Acc.: 55.57%\n",
      "Epoch: 297. Loss: 0.9493. Acc.: 56.12%\n",
      "Epoch: 298. Loss: 0.9503. Acc.: 56.59%\n",
      "Epoch: 299. Loss: 0.9509. Acc.: 56.17%\n",
      "Epoch: 300. Loss: 0.9517. Acc.: 55.66%\n",
      "Epoch: 301. Loss: 0.9547. Acc.: 56.49%\n",
      "Epoch: 302. Loss: 0.9455. Acc.: 56.49%\n",
      "Epoch: 303. Loss: 0.9477. Acc.: 55.38%\n",
      "Epoch: 304. Loss: 0.9520. Acc.: 55.94%\n",
      "Epoch: 305. Loss: 0.9489. Acc.: 55.52%\n",
      "Epoch: 306. Loss: 0.9469. Acc.: 55.84%\n",
      "Epoch: 307. Loss: 0.9506. Acc.: 55.84%\n",
      "Epoch: 308. Loss: 0.9391. Acc.: 55.89%\n",
      "Epoch: 309. Loss: 0.9474. Acc.: 56.73%\n",
      "Epoch: 310. Loss: 0.9505. Acc.: 56.17%\n",
      "Epoch: 311. Loss: 0.9476. Acc.: 55.29%\n",
      "Epoch: 312. Loss: 0.9463. Acc.: 55.19%\n",
      "Epoch: 313. Loss: 0.9419. Acc.: 55.71%\n",
      "Epoch: 314. Loss: 0.9483. Acc.: 55.47%\n",
      "Epoch: 315. Loss: 0.9423. Acc.: 55.84%\n",
      "Epoch: 316. Loss: 0.9498. Acc.: 55.38%\n",
      "Epoch: 317. Loss: 0.9470. Acc.: 55.61%\n",
      "Epoch: 318. Loss: 0.9450. Acc.: 55.52%\n",
      "Epoch: 319. Loss: 0.9420. Acc.: 54.82%\n",
      "Epoch: 320. Loss: 0.9444. Acc.: 55.66%\n",
      "Epoch: 321. Loss: 0.9437. Acc.: 55.89%\n",
      "Epoch: 322. Loss: 0.9385. Acc.: 55.80%\n",
      "Epoch: 323. Loss: 0.9432. Acc.: 55.71%\n",
      "Epoch: 324. Loss: 0.9415. Acc.: 55.94%\n",
      "Epoch: 325. Loss: 0.9373. Acc.: 56.03%\n",
      "Epoch: 326. Loss: 0.9392. Acc.: 55.98%\n",
      "Epoch: 327. Loss: 0.9448. Acc.: 55.52%\n",
      "Epoch: 328. Loss: 0.9440. Acc.: 56.68%\n",
      "Epoch: 329. Loss: 0.9431. Acc.: 55.89%\n",
      "Epoch: 330. Loss: 0.9426. Acc.: 55.43%\n",
      "Epoch: 331. Loss: 0.9375. Acc.: 56.17%\n",
      "Epoch: 332. Loss: 0.9352. Acc.: 55.80%\n",
      "Epoch: 333. Loss: 0.9402. Acc.: 55.57%\n",
      "Epoch: 334. Loss: 0.9454. Acc.: 54.87%\n",
      "Epoch: 335. Loss: 0.9372. Acc.: 56.03%\n",
      "Epoch: 336. Loss: 0.9428. Acc.: 55.29%\n",
      "Epoch: 337. Loss: 0.9381. Acc.: 56.22%\n",
      "Epoch: 338. Loss: 0.9449. Acc.: 56.35%\n",
      "Epoch: 339. Loss: 0.9497. Acc.: 55.61%\n",
      "Epoch: 340. Loss: 0.9422. Acc.: 55.01%\n",
      "Epoch: 341. Loss: 0.9420. Acc.: 55.24%\n",
      "Epoch: 342. Loss: 0.9418. Acc.: 55.89%\n",
      "Epoch: 343. Loss: 0.9384. Acc.: 55.01%\n",
      "Epoch: 344. Loss: 0.9385. Acc.: 55.80%\n",
      "Epoch: 345. Loss: 0.9418. Acc.: 56.08%\n",
      "Epoch: 346. Loss: 0.9375. Acc.: 56.26%\n",
      "Epoch: 347. Loss: 0.9420. Acc.: 56.54%\n",
      "Epoch: 348. Loss: 0.9329. Acc.: 56.08%\n",
      "Epoch: 349. Loss: 0.9400. Acc.: 56.63%\n",
      "Epoch: 350. Loss: 0.9406. Acc.: 55.10%\n",
      "Epoch: 351. Loss: 0.9421. Acc.: 56.08%\n",
      "Epoch: 352. Loss: 0.9388. Acc.: 55.57%\n",
      "Epoch: 353. Loss: 0.9407. Acc.: 55.38%\n",
      "Epoch: 354. Loss: 0.9417. Acc.: 56.49%\n",
      "Epoch: 355. Loss: 0.9415. Acc.: 55.98%\n",
      "Epoch: 356. Loss: 0.9421. Acc.: 55.80%\n",
      "Epoch: 357. Loss: 0.9331. Acc.: 56.26%\n",
      "Epoch: 358. Loss: 0.9406. Acc.: 55.94%\n",
      "Epoch: 359. Loss: 0.9374. Acc.: 56.26%\n",
      "Epoch: 360. Loss: 0.9386. Acc.: 55.38%\n",
      "Epoch: 361. Loss: 0.9389. Acc.: 56.73%\n",
      "Epoch: 362. Loss: 0.9342. Acc.: 55.61%\n",
      "Epoch: 363. Loss: 0.9400. Acc.: 56.26%\n",
      "Epoch: 364. Loss: 0.9368. Acc.: 55.47%\n",
      "Epoch: 365. Loss: 0.9376. Acc.: 56.59%\n",
      "Epoch: 366. Loss: 0.9385. Acc.: 55.66%\n",
      "Epoch: 367. Loss: 0.9328. Acc.: 56.73%\n",
      "Epoch: 368. Loss: 0.9395. Acc.: 55.75%\n",
      "Epoch: 369. Loss: 0.9360. Acc.: 56.40%\n",
      "Epoch: 370. Loss: 0.9411. Acc.: 55.80%\n",
      "Epoch: 371. Loss: 0.9364. Acc.: 55.57%\n",
      "Epoch: 372. Loss: 0.9305. Acc.: 56.03%\n",
      "Epoch: 373. Loss: 0.9316. Acc.: 55.01%\n",
      "Epoch: 374. Loss: 0.9328. Acc.: 55.38%\n",
      "Epoch: 375. Loss: 0.9372. Acc.: 54.78%\n",
      "Epoch: 376. Loss: 0.9328. Acc.: 56.40%\n",
      "Epoch: 377. Loss: 0.9356. Acc.: 56.17%\n",
      "Epoch: 378. Loss: 0.9365. Acc.: 55.75%\n",
      "Epoch: 379. Loss: 0.9404. Acc.: 55.10%\n",
      "Epoch: 380. Loss: 0.9367. Acc.: 55.57%\n",
      "Epoch: 381. Loss: 0.9309. Acc.: 55.10%\n",
      "Epoch: 382. Loss: 0.9350. Acc.: 55.43%\n",
      "Epoch: 383. Loss: 0.9358. Acc.: 56.08%\n",
      "Epoch: 384. Loss: 0.9350. Acc.: 55.43%\n",
      "Epoch: 385. Loss: 0.9320. Acc.: 55.75%\n",
      "Epoch: 386. Loss: 0.9359. Acc.: 55.38%\n",
      "Epoch: 387. Loss: 0.9310. Acc.: 55.94%\n",
      "Epoch: 388. Loss: 0.9302. Acc.: 55.52%\n",
      "Epoch: 389. Loss: 0.9325. Acc.: 56.17%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 390. Loss: 0.9340. Acc.: 55.84%\n",
      "Epoch: 391. Loss: 0.9299. Acc.: 55.61%\n",
      "Epoch: 392. Loss: 0.9342. Acc.: 56.17%\n",
      "Epoch: 393. Loss: 0.9323. Acc.: 56.08%\n",
      "Epoch: 394. Loss: 0.9330. Acc.: 56.08%\n",
      "Epoch: 395. Loss: 0.9300. Acc.: 56.22%\n",
      "Epoch: 396. Loss: 0.9287. Acc.: 55.80%\n",
      "Epoch: 397. Loss: 0.9350. Acc.: 55.19%\n",
      "Epoch: 398. Loss: 0.9324. Acc.: 56.03%\n",
      "Epoch: 399. Loss: 0.9321. Acc.: 55.66%\n",
      "Epoch: 400. Loss: 0.9277. Acc.: 55.75%\n",
      "Epoch: 401. Loss: 0.9342. Acc.: 55.75%\n",
      "Epoch: 402. Loss: 0.9309. Acc.: 55.38%\n",
      "Epoch: 403. Loss: 0.9288. Acc.: 55.98%\n",
      "Epoch: 404. Loss: 0.9259. Acc.: 55.38%\n",
      "Epoch: 405. Loss: 0.9305. Acc.: 55.52%\n",
      "Epoch: 406. Loss: 0.9347. Acc.: 55.61%\n",
      "Epoch: 407. Loss: 0.9303. Acc.: 55.98%\n",
      "Epoch: 408. Loss: 0.9253. Acc.: 55.61%\n",
      "Epoch: 409. Loss: 0.9246. Acc.: 55.89%\n",
      "Epoch: 410. Loss: 0.9334. Acc.: 55.52%\n",
      "Epoch: 411. Loss: 0.9290. Acc.: 55.33%\n",
      "Epoch: 412. Loss: 0.9289. Acc.: 55.24%\n",
      "Epoch: 413. Loss: 0.9333. Acc.: 56.08%\n",
      "Epoch: 414. Loss: 0.9285. Acc.: 56.03%\n",
      "Epoch: 415. Loss: 0.9325. Acc.: 55.43%\n",
      "Epoch: 416. Loss: 0.9297. Acc.: 55.89%\n",
      "Epoch: 417. Loss: 0.9375. Acc.: 56.31%\n",
      "Epoch: 418. Loss: 0.9273. Acc.: 55.57%\n",
      "Epoch: 419. Loss: 0.9393. Acc.: 55.84%\n",
      "Epoch: 420. Loss: 0.9364. Acc.: 55.33%\n",
      "Epoch: 421. Loss: 0.9293. Acc.: 56.17%\n",
      "Epoch: 422. Loss: 0.9318. Acc.: 55.38%\n",
      "Epoch: 423. Loss: 0.9307. Acc.: 56.08%\n",
      "Epoch: 424. Loss: 0.9261. Acc.: 55.66%\n",
      "Epoch: 425. Loss: 0.9284. Acc.: 55.61%\n",
      "Epoch: 426. Loss: 0.9285. Acc.: 55.75%\n",
      "Epoch: 427. Loss: 0.9340. Acc.: 56.03%\n",
      "Epoch: 428. Loss: 0.9350. Acc.: 55.71%\n",
      "Epoch: 429. Loss: 0.9282. Acc.: 55.57%\n",
      "Epoch: 430. Loss: 0.9300. Acc.: 55.52%\n",
      "Epoch: 431. Loss: 0.9305. Acc.: 55.89%\n",
      "Epoch: 432. Loss: 0.9254. Acc.: 55.57%\n",
      "Epoch: 433. Loss: 0.9316. Acc.: 55.89%\n",
      "Epoch: 434. Loss: 0.9267. Acc.: 55.94%\n",
      "Epoch: 435. Loss: 0.9284. Acc.: 55.94%\n",
      "Epoch: 436. Loss: 0.9277. Acc.: 56.17%\n",
      "Epoch: 437. Loss: 0.9334. Acc.: 55.57%\n",
      "Epoch: 438. Loss: 0.9275. Acc.: 56.12%\n",
      "Epoch: 439. Loss: 0.9348. Acc.: 55.84%\n",
      "Epoch: 440. Loss: 0.9236. Acc.: 55.52%\n",
      "Epoch: 441. Loss: 0.9275. Acc.: 55.71%\n",
      "Epoch: 442. Loss: 0.9266. Acc.: 56.12%\n",
      "Epoch: 443. Loss: 0.9311. Acc.: 55.84%\n",
      "Epoch: 444. Loss: 0.9249. Acc.: 56.17%\n",
      "Epoch: 445. Loss: 0.9280. Acc.: 55.43%\n",
      "Epoch: 446. Loss: 0.9309. Acc.: 55.33%\n",
      "Epoch: 447. Loss: 0.9270. Acc.: 56.03%\n",
      "Epoch: 448. Loss: 0.9302. Acc.: 56.22%\n",
      "Epoch: 449. Loss: 0.9349. Acc.: 55.24%\n",
      "Epoch: 450. Loss: 0.9300. Acc.: 55.89%\n",
      "Epoch: 451. Loss: 0.9240. Acc.: 55.38%\n",
      "Epoch: 452. Loss: 0.9293. Acc.: 55.38%\n",
      "Epoch: 453. Loss: 0.9266. Acc.: 55.24%\n",
      "Epoch: 454. Loss: 0.9283. Acc.: 55.89%\n",
      "Epoch: 455. Loss: 0.9241. Acc.: 55.43%\n",
      "Epoch: 456. Loss: 0.9301. Acc.: 55.52%\n",
      "Epoch: 457. Loss: 0.9280. Acc.: 55.43%\n",
      "Epoch: 458. Loss: 0.9230. Acc.: 55.52%\n",
      "Epoch: 459. Loss: 0.9329. Acc.: 55.75%\n",
      "Epoch: 460. Loss: 0.9341. Acc.: 55.38%\n",
      "Epoch: 461. Loss: 0.9308. Acc.: 55.57%\n",
      "Epoch: 462. Loss: 0.9256. Acc.: 55.75%\n",
      "Epoch: 463. Loss: 0.9300. Acc.: 55.94%\n",
      "Epoch: 464. Loss: 0.9285. Acc.: 55.89%\n",
      "Epoch: 465. Loss: 0.9238. Acc.: 55.94%\n",
      "Epoch: 466. Loss: 0.9292. Acc.: 55.75%\n",
      "Epoch: 467. Loss: 0.9307. Acc.: 55.71%\n",
      "Epoch: 468. Loss: 0.9257. Acc.: 55.52%\n",
      "Epoch: 469. Loss: 0.9239. Acc.: 55.75%\n",
      "Epoch: 470. Loss: 0.9271. Acc.: 55.61%\n",
      "Epoch: 471. Loss: 0.9294. Acc.: 55.80%\n",
      "Epoch: 472. Loss: 0.9277. Acc.: 55.29%\n",
      "Epoch: 473. Loss: 0.9285. Acc.: 55.80%\n",
      "Epoch: 474. Loss: 0.9219. Acc.: 55.52%\n",
      "Epoch: 475. Loss: 0.9256. Acc.: 55.52%\n",
      "Epoch: 476. Loss: 0.9269. Acc.: 55.75%\n",
      "Epoch: 477. Loss: 0.9201. Acc.: 55.84%\n",
      "Epoch: 478. Loss: 0.9248. Acc.: 55.61%\n",
      "Epoch: 479. Loss: 0.9263. Acc.: 55.57%\n",
      "Epoch: 480. Loss: 0.9204. Acc.: 55.84%\n",
      "Epoch: 481. Loss: 0.9321. Acc.: 55.89%\n",
      "Epoch: 482. Loss: 0.9239. Acc.: 55.84%\n",
      "Epoch: 483. Loss: 0.9281. Acc.: 55.89%\n",
      "Epoch: 484. Loss: 0.9262. Acc.: 55.57%\n",
      "Epoch: 485. Loss: 0.9305. Acc.: 55.80%\n",
      "Epoch: 486. Loss: 0.9236. Acc.: 55.61%\n",
      "Epoch: 487. Loss: 0.9241. Acc.: 55.47%\n",
      "Epoch: 488. Loss: 0.9243. Acc.: 55.47%\n",
      "Epoch: 489. Loss: 0.9234. Acc.: 55.66%\n",
      "Epoch: 490. Loss: 0.9261. Acc.: 55.71%\n",
      "Epoch: 491. Loss: 0.9346. Acc.: 55.71%\n",
      "Epoch: 492. Loss: 0.9278. Acc.: 55.52%\n",
      "Epoch: 493. Loss: 0.9215. Acc.: 55.57%\n",
      "Epoch: 494. Loss: 0.9268. Acc.: 55.66%\n",
      "Epoch: 495. Loss: 0.9356. Acc.: 55.57%\n",
      "Epoch: 496. Loss: 0.9238. Acc.: 55.66%\n",
      "Epoch: 497. Loss: 0.9228. Acc.: 55.52%\n",
      "Epoch: 498. Loss: 0.9252. Acc.: 55.57%\n",
      "Epoch: 499. Loss: 0.9216. Acc.: 55.52%\n",
      "Epoch: 500. Loss: 0.9259. Acc.: 55.66%\n",
      "Epoch: 501. Loss: 0.9296. Acc.: 55.66%\n",
      "Epoch: 502. Loss: 0.9275. Acc.: 55.71%\n",
      "Epoch: 503. Loss: 0.9276. Acc.: 55.71%\n",
      "Epoch: 504. Loss: 0.9298. Acc.: 55.61%\n",
      "Epoch: 505. Loss: 0.9204. Acc.: 55.57%\n",
      "Epoch: 506. Loss: 0.9358. Acc.: 55.75%\n",
      "Epoch: 507. Loss: 0.9279. Acc.: 55.66%\n",
      "Epoch: 508. Loss: 0.9243. Acc.: 55.61%\n",
      "Epoch: 509. Loss: 0.9205. Acc.: 55.57%\n",
      "Epoch: 510. Loss: 0.9249. Acc.: 55.61%\n",
      "Epoch: 511. Loss: 0.9273. Acc.: 56.03%\n",
      "Epoch: 512. Loss: 0.9480. Acc.: 56.82%\n",
      "Epoch: 513. Loss: 0.9437. Acc.: 54.87%\n",
      "Epoch: 514. Loss: 0.9538. Acc.: 57.00%\n",
      "Epoch: 515. Loss: 0.9520. Acc.: 55.15%\n",
      "Epoch: 516. Loss: 0.9467. Acc.: 55.15%\n",
      "Epoch: 517. Loss: 0.9497. Acc.: 55.43%\n",
      "Epoch: 518. Loss: 0.9401. Acc.: 56.40%\n",
      "Epoch: 519. Loss: 0.9502. Acc.: 55.61%\n",
      "Epoch: 520. Loss: 0.9507. Acc.: 57.51%\n",
      "Epoch: 521. Loss: 0.9524. Acc.: 55.61%\n",
      "Epoch: 522. Loss: 0.9534. Acc.: 55.43%\n",
      "Epoch: 523. Loss: 0.9663. Acc.: 56.22%\n",
      "Epoch: 524. Loss: 0.9488. Acc.: 54.17%\n",
      "Epoch: 525. Loss: 0.9507. Acc.: 56.22%\n",
      "Epoch: 526. Loss: 0.9516. Acc.: 57.10%\n",
      "Epoch: 527. Loss: 0.9427. Acc.: 55.24%\n",
      "Epoch: 528. Loss: 0.9496. Acc.: 55.89%\n",
      "Epoch: 529. Loss: 0.9496. Acc.: 56.22%\n",
      "Epoch: 530. Loss: 0.9490. Acc.: 55.80%\n",
      "Epoch: 531. Loss: 0.9423. Acc.: 56.08%\n",
      "Epoch: 532. Loss: 0.9500. Acc.: 55.61%\n",
      "Epoch: 533. Loss: 0.9529. Acc.: 55.29%\n",
      "Epoch: 534. Loss: 0.9487. Acc.: 55.43%\n",
      "Epoch: 535. Loss: 0.9473. Acc.: 55.66%\n",
      "Epoch: 536. Loss: 0.9464. Acc.: 57.14%\n",
      "Epoch: 537. Loss: 0.9452. Acc.: 55.89%\n",
      "Epoch: 538. Loss: 0.9520. Acc.: 55.84%\n",
      "Epoch: 539. Loss: 0.9496. Acc.: 56.77%\n",
      "Epoch: 540. Loss: 0.9559. Acc.: 55.29%\n",
      "Epoch: 541. Loss: 0.9460. Acc.: 56.86%\n",
      "Epoch: 542. Loss: 0.9509. Acc.: 56.59%\n",
      "Epoch: 543. Loss: 0.9464. Acc.: 56.26%\n",
      "Epoch: 544. Loss: 0.9372. Acc.: 55.84%\n",
      "Epoch: 545. Loss: 0.9461. Acc.: 55.38%\n",
      "Epoch: 546. Loss: 0.9461. Acc.: 56.82%\n",
      "Epoch: 547. Loss: 0.9482. Acc.: 54.27%\n",
      "Epoch: 548. Loss: 0.9497. Acc.: 54.92%\n",
      "Epoch: 549. Loss: 0.9435. Acc.: 57.37%\n",
      "Epoch: 550. Loss: 0.9380. Acc.: 55.98%\n",
      "Epoch: 551. Loss: 0.9402. Acc.: 54.82%\n",
      "Epoch: 552. Loss: 0.9486. Acc.: 56.22%\n",
      "Epoch: 553. Loss: 0.9414. Acc.: 56.86%\n",
      "Epoch: 554. Loss: 0.9374. Acc.: 57.70%\n",
      "Epoch 554 best model saved with accuracy: 57.70%\n",
      "Epoch: 555. Loss: 0.9492. Acc.: 55.71%\n",
      "Epoch: 556. Loss: 0.9439. Acc.: 55.24%\n",
      "Epoch: 557. Loss: 0.9462. Acc.: 55.71%\n",
      "Epoch: 558. Loss: 0.9434. Acc.: 55.15%\n",
      "Epoch: 559. Loss: 0.9363. Acc.: 55.75%\n",
      "Epoch: 560. Loss: 0.9446. Acc.: 54.45%\n",
      "Epoch: 561. Loss: 0.9449. Acc.: 55.66%\n",
      "Epoch: 562. Loss: 0.9401. Acc.: 56.45%\n",
      "Epoch: 563. Loss: 0.9435. Acc.: 55.15%\n",
      "Epoch: 564. Loss: 0.9474. Acc.: 56.45%\n",
      "Epoch: 565. Loss: 0.9430. Acc.: 57.28%\n",
      "Epoch: 566. Loss: 0.9419. Acc.: 55.89%\n",
      "Epoch: 567. Loss: 0.9392. Acc.: 56.31%\n",
      "Epoch: 568. Loss: 0.9443. Acc.: 57.19%\n",
      "Epoch: 569. Loss: 0.9407. Acc.: 54.41%\n",
      "Epoch: 570. Loss: 0.9400. Acc.: 55.43%\n",
      "Epoch: 571. Loss: 0.9419. Acc.: 54.59%\n",
      "Epoch: 572. Loss: 0.9445. Acc.: 57.56%\n",
      "Epoch: 573. Loss: 0.9393. Acc.: 57.10%\n",
      "Epoch: 574. Loss: 0.9445. Acc.: 56.26%\n",
      "Epoch: 575. Loss: 0.9422. Acc.: 55.57%\n",
      "Epoch: 576. Loss: 0.9389. Acc.: 55.33%\n",
      "Epoch: 577. Loss: 0.9426. Acc.: 57.56%\n",
      "Epoch: 578. Loss: 0.9392. Acc.: 57.56%\n",
      "Epoch: 579. Loss: 0.9440. Acc.: 55.43%\n",
      "Epoch: 580. Loss: 0.9372. Acc.: 56.91%\n",
      "Epoch: 581. Loss: 0.9355. Acc.: 55.61%\n",
      "Epoch: 582. Loss: 0.9432. Acc.: 55.10%\n",
      "Epoch: 583. Loss: 0.9369. Acc.: 57.19%\n",
      "Epoch: 584. Loss: 0.9327. Acc.: 56.31%\n",
      "Epoch: 585. Loss: 0.9351. Acc.: 56.08%\n",
      "Epoch: 586. Loss: 0.9363. Acc.: 55.80%\n",
      "Epoch: 587. Loss: 0.9364. Acc.: 55.89%\n",
      "Epoch: 588. Loss: 0.9359. Acc.: 55.98%\n",
      "Epoch: 589. Loss: 0.9425. Acc.: 56.08%\n",
      "Epoch: 590. Loss: 0.9366. Acc.: 54.50%\n",
      "Epoch: 591. Loss: 0.9402. Acc.: 56.73%\n",
      "Epoch: 592. Loss: 0.9327. Acc.: 57.00%\n",
      "Epoch: 593. Loss: 0.9384. Acc.: 54.17%\n",
      "Epoch: 594. Loss: 0.9407. Acc.: 56.68%\n",
      "Epoch: 595. Loss: 0.9342. Acc.: 55.66%\n",
      "Epoch: 596. Loss: 0.9310. Acc.: 56.12%\n",
      "Epoch: 597. Loss: 0.9400. Acc.: 56.54%\n",
      "Epoch: 598. Loss: 0.9333. Acc.: 56.03%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 599. Loss: 0.9399. Acc.: 55.75%\n",
      "Epoch: 600. Loss: 0.9338. Acc.: 55.84%\n",
      "Epoch: 601. Loss: 0.9309. Acc.: 57.28%\n",
      "Epoch: 602. Loss: 0.9385. Acc.: 56.45%\n",
      "Epoch: 603. Loss: 0.9363. Acc.: 57.37%\n",
      "Epoch: 604. Loss: 0.9405. Acc.: 56.82%\n",
      "Epoch: 605. Loss: 0.9356. Acc.: 56.17%\n",
      "Epoch: 606. Loss: 0.9297. Acc.: 57.33%\n",
      "Epoch: 607. Loss: 0.9356. Acc.: 55.98%\n",
      "Epoch: 608. Loss: 0.9283. Acc.: 55.84%\n",
      "Epoch: 609. Loss: 0.9319. Acc.: 56.08%\n",
      "Epoch: 610. Loss: 0.9333. Acc.: 55.15%\n",
      "Epoch: 611. Loss: 0.9377. Acc.: 54.92%\n",
      "Epoch: 612. Loss: 0.9385. Acc.: 56.68%\n",
      "Epoch: 613. Loss: 0.9286. Acc.: 56.54%\n",
      "Epoch: 614. Loss: 0.9317. Acc.: 56.91%\n",
      "Epoch: 615. Loss: 0.9331. Acc.: 54.92%\n",
      "Epoch: 616. Loss: 0.9410. Acc.: 55.84%\n",
      "Epoch: 617. Loss: 0.9380. Acc.: 56.86%\n",
      "Epoch: 618. Loss: 0.9343. Acc.: 56.35%\n",
      "Epoch: 619. Loss: 0.9296. Acc.: 56.96%\n",
      "Epoch: 620. Loss: 0.9399. Acc.: 55.61%\n",
      "Epoch: 621. Loss: 0.9324. Acc.: 56.22%\n",
      "Epoch: 622. Loss: 0.9282. Acc.: 57.61%\n",
      "Epoch: 623. Loss: 0.9293. Acc.: 56.35%\n",
      "Epoch: 624. Loss: 0.9313. Acc.: 55.80%\n",
      "Epoch: 625. Loss: 0.9365. Acc.: 57.10%\n",
      "Epoch: 626. Loss: 0.9332. Acc.: 56.96%\n",
      "Epoch: 627. Loss: 0.9270. Acc.: 55.71%\n",
      "Epoch: 628. Loss: 0.9358. Acc.: 56.96%\n",
      "Epoch: 629. Loss: 0.9303. Acc.: 57.05%\n",
      "Epoch: 630. Loss: 0.9253. Acc.: 56.31%\n",
      "Epoch: 631. Loss: 0.9202. Acc.: 57.61%\n",
      "Epoch: 632. Loss: 0.9299. Acc.: 56.12%\n",
      "Epoch: 633. Loss: 0.9283. Acc.: 56.49%\n",
      "Epoch: 634. Loss: 0.9304. Acc.: 57.19%\n",
      "Epoch: 635. Loss: 0.9342. Acc.: 56.73%\n",
      "Epoch: 636. Loss: 0.9282. Acc.: 58.02%\n",
      "Epoch 636 best model saved with accuracy: 58.02%\n",
      "Epoch: 637. Loss: 0.9265. Acc.: 57.51%\n",
      "Epoch: 638. Loss: 0.9324. Acc.: 57.56%\n",
      "Epoch: 639. Loss: 0.9288. Acc.: 55.75%\n",
      "Epoch: 640. Loss: 0.9260. Acc.: 56.68%\n",
      "Epoch: 641. Loss: 0.9229. Acc.: 56.68%\n",
      "Epoch: 642. Loss: 0.9201. Acc.: 56.59%\n",
      "Epoch: 643. Loss: 0.9301. Acc.: 56.68%\n",
      "Epoch: 644. Loss: 0.9332. Acc.: 56.73%\n",
      "Epoch: 645. Loss: 0.9209. Acc.: 55.71%\n",
      "Epoch: 646. Loss: 0.9216. Acc.: 57.24%\n",
      "Epoch: 647. Loss: 0.9272. Acc.: 55.98%\n",
      "Epoch: 648. Loss: 0.9199. Acc.: 57.93%\n",
      "Epoch: 649. Loss: 0.9264. Acc.: 57.47%\n",
      "Epoch: 650. Loss: 0.9208. Acc.: 56.45%\n",
      "Epoch: 651. Loss: 0.9207. Acc.: 56.31%\n",
      "Epoch: 652. Loss: 0.9206. Acc.: 57.75%\n",
      "Epoch: 653. Loss: 0.9221. Acc.: 57.00%\n",
      "Epoch: 654. Loss: 0.9239. Acc.: 55.24%\n",
      "Epoch: 655. Loss: 0.9237. Acc.: 55.10%\n",
      "Epoch: 656. Loss: 0.9277. Acc.: 57.79%\n",
      "Epoch: 657. Loss: 0.9251. Acc.: 57.28%\n",
      "Epoch: 658. Loss: 0.9201. Acc.: 56.82%\n",
      "Epoch: 659. Loss: 0.9187. Acc.: 57.24%\n",
      "Epoch: 660. Loss: 0.9221. Acc.: 56.86%\n",
      "Epoch: 661. Loss: 0.9196. Acc.: 56.68%\n",
      "Epoch: 662. Loss: 0.9210. Acc.: 57.37%\n",
      "Epoch: 663. Loss: 0.9197. Acc.: 57.47%\n",
      "Epoch: 664. Loss: 0.9128. Acc.: 56.96%\n",
      "Epoch: 665. Loss: 0.9177. Acc.: 56.08%\n",
      "Epoch: 666. Loss: 0.9229. Acc.: 57.28%\n",
      "Epoch: 667. Loss: 0.9213. Acc.: 57.47%\n",
      "Epoch: 668. Loss: 0.9219. Acc.: 56.40%\n",
      "Epoch: 669. Loss: 0.9227. Acc.: 57.05%\n",
      "Epoch: 670. Loss: 0.9133. Acc.: 55.57%\n",
      "Epoch: 671. Loss: 0.9211. Acc.: 55.94%\n",
      "Epoch: 672. Loss: 0.9191. Acc.: 56.82%\n",
      "Epoch: 673. Loss: 0.9237. Acc.: 58.77%\n",
      "Epoch 673 best model saved with accuracy: 58.77%\n",
      "Epoch: 674. Loss: 0.9273. Acc.: 56.91%\n",
      "Epoch: 675. Loss: 0.9159. Acc.: 56.73%\n",
      "Epoch: 676. Loss: 0.9202. Acc.: 56.91%\n",
      "Epoch: 677. Loss: 0.9227. Acc.: 56.82%\n",
      "Epoch: 678. Loss: 0.9115. Acc.: 57.33%\n",
      "Epoch: 679. Loss: 0.9222. Acc.: 56.40%\n",
      "Epoch: 680. Loss: 0.9154. Acc.: 55.71%\n",
      "Epoch: 681. Loss: 0.9182. Acc.: 56.12%\n",
      "Epoch: 682. Loss: 0.9097. Acc.: 56.82%\n",
      "Epoch: 683. Loss: 0.9178. Acc.: 57.19%\n",
      "Epoch: 684. Loss: 0.9187. Acc.: 57.14%\n",
      "Epoch: 685. Loss: 0.9168. Acc.: 57.88%\n",
      "Epoch: 686. Loss: 0.9138. Acc.: 55.57%\n",
      "Epoch: 687. Loss: 0.9169. Acc.: 56.08%\n",
      "Epoch: 688. Loss: 0.9177. Acc.: 57.00%\n",
      "Epoch: 689. Loss: 0.9169. Acc.: 55.84%\n",
      "Epoch: 690. Loss: 0.9085. Acc.: 55.98%\n",
      "Epoch: 691. Loss: 0.9155. Acc.: 56.40%\n",
      "Epoch: 692. Loss: 0.9108. Acc.: 56.40%\n",
      "Epoch: 693. Loss: 0.9193. Acc.: 56.17%\n",
      "Epoch: 694. Loss: 0.9204. Acc.: 56.31%\n",
      "Epoch: 695. Loss: 0.9225. Acc.: 57.00%\n",
      "Epoch: 696. Loss: 0.9105. Acc.: 57.28%\n",
      "Epoch: 697. Loss: 0.9187. Acc.: 56.49%\n",
      "Epoch: 698. Loss: 0.9119. Acc.: 56.12%\n",
      "Epoch: 699. Loss: 0.9092. Acc.: 56.73%\n",
      "Epoch: 700. Loss: 0.9132. Acc.: 55.57%\n",
      "Epoch: 701. Loss: 0.9148. Acc.: 57.14%\n",
      "Epoch: 702. Loss: 0.9171. Acc.: 56.77%\n",
      "Epoch: 703. Loss: 0.9155. Acc.: 58.21%\n",
      "Epoch: 704. Loss: 0.9149. Acc.: 55.71%\n",
      "Epoch: 705. Loss: 0.9104. Acc.: 56.96%\n",
      "Epoch: 706. Loss: 0.9187. Acc.: 56.35%\n",
      "Epoch: 707. Loss: 0.9127. Acc.: 56.03%\n",
      "Epoch: 708. Loss: 0.9127. Acc.: 57.79%\n",
      "Epoch: 709. Loss: 0.9179. Acc.: 57.37%\n",
      "Epoch: 710. Loss: 0.9117. Acc.: 57.24%\n",
      "Epoch: 711. Loss: 0.9021. Acc.: 58.30%\n",
      "Epoch: 712. Loss: 0.9091. Acc.: 57.61%\n",
      "Epoch: 713. Loss: 0.9086. Acc.: 56.31%\n",
      "Epoch: 714. Loss: 0.9107. Acc.: 57.00%\n",
      "Epoch: 715. Loss: 0.9120. Acc.: 56.91%\n",
      "Epoch: 716. Loss: 0.9135. Acc.: 56.03%\n",
      "Epoch: 717. Loss: 0.9020. Acc.: 56.03%\n",
      "Epoch: 718. Loss: 0.9101. Acc.: 55.71%\n",
      "Epoch: 719. Loss: 0.9068. Acc.: 57.10%\n",
      "Epoch: 720. Loss: 0.9009. Acc.: 56.59%\n",
      "Epoch: 721. Loss: 0.9065. Acc.: 57.28%\n",
      "Epoch: 722. Loss: 0.9081. Acc.: 56.54%\n",
      "Epoch: 723. Loss: 0.9076. Acc.: 57.37%\n",
      "Epoch: 724. Loss: 0.9035. Acc.: 56.17%\n",
      "Epoch: 725. Loss: 0.9045. Acc.: 57.00%\n",
      "Epoch: 726. Loss: 0.9094. Acc.: 56.73%\n",
      "Epoch: 727. Loss: 0.9062. Acc.: 57.00%\n",
      "Epoch: 728. Loss: 0.9051. Acc.: 56.35%\n",
      "Epoch: 729. Loss: 0.9065. Acc.: 57.33%\n",
      "Epoch: 730. Loss: 0.9040. Acc.: 56.22%\n",
      "Epoch: 731. Loss: 0.9069. Acc.: 56.54%\n",
      "Epoch: 732. Loss: 0.9009. Acc.: 57.05%\n",
      "Epoch: 733. Loss: 0.9089. Acc.: 57.42%\n",
      "Epoch: 734. Loss: 0.8992. Acc.: 56.77%\n",
      "Epoch: 735. Loss: 0.9108. Acc.: 55.71%\n",
      "Epoch: 736. Loss: 0.9063. Acc.: 56.82%\n",
      "Epoch: 737. Loss: 0.9025. Acc.: 57.51%\n",
      "Epoch: 738. Loss: 0.9060. Acc.: 57.10%\n",
      "Epoch: 739. Loss: 0.9048. Acc.: 57.14%\n",
      "Epoch: 740. Loss: 0.8941. Acc.: 57.65%\n",
      "Epoch: 741. Loss: 0.9048. Acc.: 56.45%\n",
      "Epoch: 742. Loss: 0.9029. Acc.: 56.54%\n",
      "Epoch: 743. Loss: 0.9050. Acc.: 56.31%\n",
      "Epoch: 744. Loss: 0.9054. Acc.: 56.35%\n",
      "Epoch: 745. Loss: 0.8944. Acc.: 56.31%\n",
      "Epoch: 746. Loss: 0.9053. Acc.: 56.45%\n",
      "Epoch: 747. Loss: 0.9011. Acc.: 56.59%\n",
      "Epoch: 748. Loss: 0.9004. Acc.: 57.19%\n",
      "Epoch: 749. Loss: 0.8968. Acc.: 57.05%\n",
      "Epoch: 750. Loss: 0.9003. Acc.: 57.37%\n",
      "Epoch: 751. Loss: 0.9030. Acc.: 56.86%\n",
      "Epoch: 752. Loss: 0.9042. Acc.: 57.24%\n",
      "Epoch: 753. Loss: 0.9015. Acc.: 56.82%\n",
      "Epoch: 754. Loss: 0.9008. Acc.: 56.03%\n",
      "Epoch: 755. Loss: 0.9022. Acc.: 56.35%\n",
      "Epoch: 756. Loss: 0.9025. Acc.: 55.98%\n",
      "Epoch: 757. Loss: 0.9014. Acc.: 56.63%\n",
      "Epoch: 758. Loss: 0.9042. Acc.: 56.49%\n",
      "Epoch: 759. Loss: 0.9023. Acc.: 56.86%\n",
      "Epoch: 760. Loss: 0.8986. Acc.: 55.94%\n",
      "Epoch: 761. Loss: 0.8997. Acc.: 56.63%\n",
      "Epoch: 762. Loss: 0.8982. Acc.: 57.00%\n",
      "Epoch: 763. Loss: 0.9029. Acc.: 56.82%\n",
      "Epoch: 764. Loss: 0.8973. Acc.: 57.37%\n",
      "Epoch: 765. Loss: 0.8995. Acc.: 57.00%\n",
      "Epoch: 766. Loss: 0.9013. Acc.: 56.96%\n",
      "Epoch: 767. Loss: 0.9020. Acc.: 56.54%\n",
      "Epoch: 768. Loss: 0.8948. Acc.: 56.40%\n",
      "Epoch: 769. Loss: 0.8981. Acc.: 56.73%\n",
      "Epoch: 770. Loss: 0.8925. Acc.: 57.75%\n",
      "Epoch: 771. Loss: 0.8995. Acc.: 57.19%\n",
      "Epoch: 772. Loss: 0.8918. Acc.: 55.94%\n",
      "Epoch: 773. Loss: 0.9018. Acc.: 56.63%\n",
      "Epoch: 774. Loss: 0.8975. Acc.: 56.86%\n",
      "Epoch: 775. Loss: 0.8914. Acc.: 57.14%\n",
      "Epoch: 776. Loss: 0.8947. Acc.: 56.59%\n",
      "Epoch: 777. Loss: 0.9006. Acc.: 56.26%\n",
      "Epoch: 778. Loss: 0.8934. Acc.: 56.86%\n",
      "Epoch: 779. Loss: 0.8969. Acc.: 55.94%\n",
      "Epoch: 780. Loss: 0.8926. Acc.: 57.84%\n",
      "Epoch: 781. Loss: 0.8938. Acc.: 56.77%\n",
      "Epoch: 782. Loss: 0.8885. Acc.: 56.68%\n",
      "Epoch: 783. Loss: 0.8918. Acc.: 56.08%\n",
      "Epoch: 784. Loss: 0.8930. Acc.: 56.82%\n",
      "Epoch: 785. Loss: 0.8969. Acc.: 56.45%\n",
      "Epoch: 786. Loss: 0.8971. Acc.: 56.26%\n",
      "Epoch: 787. Loss: 0.8935. Acc.: 57.47%\n",
      "Epoch: 788. Loss: 0.8986. Acc.: 55.94%\n",
      "Epoch: 789. Loss: 0.8950. Acc.: 56.91%\n",
      "Epoch: 790. Loss: 0.8941. Acc.: 57.98%\n",
      "Epoch: 791. Loss: 0.8903. Acc.: 57.05%\n",
      "Epoch: 792. Loss: 0.8901. Acc.: 56.86%\n",
      "Epoch: 793. Loss: 0.8936. Acc.: 56.96%\n",
      "Epoch: 794. Loss: 0.8947. Acc.: 56.26%\n",
      "Epoch: 795. Loss: 0.8877. Acc.: 56.54%\n",
      "Epoch: 796. Loss: 0.8991. Acc.: 56.59%\n",
      "Epoch: 797. Loss: 0.8861. Acc.: 56.73%\n",
      "Epoch: 798. Loss: 0.8972. Acc.: 57.93%\n",
      "Epoch: 799. Loss: 0.8906. Acc.: 57.33%\n",
      "Epoch: 800. Loss: 0.8893. Acc.: 56.63%\n",
      "Epoch: 801. Loss: 0.8973. Acc.: 56.68%\n",
      "Epoch: 802. Loss: 0.8897. Acc.: 56.77%\n",
      "Epoch: 803. Loss: 0.8873. Acc.: 56.96%\n",
      "Epoch: 804. Loss: 0.8919. Acc.: 56.68%\n",
      "Epoch: 805. Loss: 0.8951. Acc.: 57.24%\n",
      "Epoch: 806. Loss: 0.8874. Acc.: 56.82%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 807. Loss: 0.8861. Acc.: 57.93%\n",
      "Epoch: 808. Loss: 0.8937. Acc.: 57.56%\n",
      "Epoch: 809. Loss: 0.8844. Acc.: 57.14%\n",
      "Epoch: 810. Loss: 0.8910. Acc.: 56.63%\n",
      "Epoch: 811. Loss: 0.8907. Acc.: 57.28%\n",
      "Epoch: 812. Loss: 0.8938. Acc.: 57.42%\n",
      "Epoch: 813. Loss: 0.8881. Acc.: 57.14%\n",
      "Epoch: 814. Loss: 0.8960. Acc.: 56.59%\n",
      "Epoch: 815. Loss: 0.8881. Acc.: 57.33%\n",
      "Epoch: 816. Loss: 0.8874. Acc.: 56.91%\n",
      "Epoch: 817. Loss: 0.8858. Acc.: 56.77%\n",
      "Epoch: 818. Loss: 0.8870. Acc.: 57.37%\n",
      "Epoch: 819. Loss: 0.8870. Acc.: 56.96%\n",
      "Epoch: 820. Loss: 0.8761. Acc.: 56.35%\n",
      "Epoch: 821. Loss: 0.8871. Acc.: 56.86%\n",
      "Epoch: 822. Loss: 0.8931. Acc.: 56.73%\n",
      "Epoch: 823. Loss: 0.8887. Acc.: 57.14%\n",
      "Epoch: 824. Loss: 0.8815. Acc.: 57.19%\n",
      "Epoch: 825. Loss: 0.8799. Acc.: 56.82%\n",
      "Epoch: 826. Loss: 0.8824. Acc.: 56.68%\n",
      "Epoch: 827. Loss: 0.8837. Acc.: 56.82%\n",
      "Epoch: 828. Loss: 0.8884. Acc.: 56.96%\n",
      "Epoch: 829. Loss: 0.8829. Acc.: 56.86%\n",
      "Epoch: 830. Loss: 0.8813. Acc.: 57.00%\n",
      "Epoch: 831. Loss: 0.8822. Acc.: 57.33%\n",
      "Epoch: 832. Loss: 0.8867. Acc.: 56.96%\n",
      "Epoch: 833. Loss: 0.8814. Acc.: 56.68%\n",
      "Epoch: 834. Loss: 0.8858. Acc.: 56.22%\n",
      "Epoch: 835. Loss: 0.8805. Acc.: 56.40%\n",
      "Epoch: 836. Loss: 0.8893. Acc.: 56.59%\n",
      "Epoch: 837. Loss: 0.8815. Acc.: 57.28%\n",
      "Epoch: 838. Loss: 0.8844. Acc.: 56.82%\n",
      "Epoch: 839. Loss: 0.8873. Acc.: 57.37%\n",
      "Epoch: 840. Loss: 0.8868. Acc.: 56.91%\n",
      "Epoch: 841. Loss: 0.8821. Acc.: 57.19%\n",
      "Epoch: 842. Loss: 0.8879. Acc.: 56.77%\n",
      "Epoch: 843. Loss: 0.8807. Acc.: 57.19%\n",
      "Epoch: 844. Loss: 0.8858. Acc.: 57.05%\n",
      "Epoch: 845. Loss: 0.8768. Acc.: 56.40%\n",
      "Epoch: 846. Loss: 0.8804. Acc.: 57.00%\n",
      "Epoch: 847. Loss: 0.8855. Acc.: 56.54%\n",
      "Epoch: 848. Loss: 0.8813. Acc.: 56.45%\n",
      "Epoch: 849. Loss: 0.8822. Acc.: 56.73%\n",
      "Epoch: 850. Loss: 0.8821. Acc.: 56.40%\n",
      "Epoch: 851. Loss: 0.8852. Acc.: 57.00%\n",
      "Epoch: 852. Loss: 0.8794. Acc.: 57.42%\n",
      "Epoch: 853. Loss: 0.8801. Acc.: 56.54%\n",
      "Epoch: 854. Loss: 0.8774. Acc.: 56.96%\n",
      "Epoch: 855. Loss: 0.8843. Acc.: 56.77%\n",
      "Epoch: 856. Loss: 0.8815. Acc.: 56.45%\n",
      "Epoch: 857. Loss: 0.8763. Acc.: 56.45%\n",
      "Epoch: 858. Loss: 0.8810. Acc.: 57.10%\n",
      "Epoch: 859. Loss: 0.8763. Acc.: 57.10%\n",
      "Epoch: 860. Loss: 0.8795. Acc.: 56.12%\n",
      "Epoch: 861. Loss: 0.8789. Acc.: 57.14%\n",
      "Epoch: 862. Loss: 0.8737. Acc.: 57.00%\n",
      "Epoch: 863. Loss: 0.8810. Acc.: 56.77%\n",
      "Epoch: 864. Loss: 0.8800. Acc.: 56.96%\n",
      "Epoch: 865. Loss: 0.8808. Acc.: 56.91%\n",
      "Epoch: 866. Loss: 0.8805. Acc.: 57.28%\n",
      "Epoch: 867. Loss: 0.8755. Acc.: 56.49%\n",
      "Epoch: 868. Loss: 0.8844. Acc.: 57.24%\n",
      "Epoch: 869. Loss: 0.8716. Acc.: 56.68%\n",
      "Epoch: 870. Loss: 0.8773. Acc.: 56.54%\n",
      "Epoch: 871. Loss: 0.8809. Acc.: 56.77%\n",
      "Epoch: 872. Loss: 0.8781. Acc.: 56.45%\n",
      "Epoch: 873. Loss: 0.8781. Acc.: 56.59%\n",
      "Epoch: 874. Loss: 0.8784. Acc.: 56.63%\n",
      "Epoch: 875. Loss: 0.8795. Acc.: 57.19%\n",
      "Epoch: 876. Loss: 0.8798. Acc.: 57.00%\n",
      "Epoch: 877. Loss: 0.8765. Acc.: 57.19%\n",
      "Epoch: 878. Loss: 0.8775. Acc.: 56.73%\n",
      "Epoch: 879. Loss: 0.8770. Acc.: 56.73%\n",
      "Epoch: 880. Loss: 0.8750. Acc.: 57.05%\n",
      "Epoch: 881. Loss: 0.8709. Acc.: 56.45%\n",
      "Epoch: 882. Loss: 0.8690. Acc.: 56.68%\n",
      "Epoch: 883. Loss: 0.8744. Acc.: 56.68%\n",
      "Epoch: 884. Loss: 0.8715. Acc.: 56.59%\n",
      "Epoch: 885. Loss: 0.8802. Acc.: 56.82%\n",
      "Epoch: 886. Loss: 0.8678. Acc.: 57.14%\n",
      "Epoch: 887. Loss: 0.8810. Acc.: 56.91%\n",
      "Epoch: 888. Loss: 0.8724. Acc.: 56.77%\n",
      "Epoch: 889. Loss: 0.8734. Acc.: 56.73%\n",
      "Epoch: 890. Loss: 0.8714. Acc.: 57.37%\n",
      "Epoch: 891. Loss: 0.8767. Acc.: 57.14%\n",
      "Epoch: 892. Loss: 0.8730. Acc.: 56.59%\n",
      "Epoch: 893. Loss: 0.8730. Acc.: 56.63%\n",
      "Epoch: 894. Loss: 0.8697. Acc.: 56.73%\n",
      "Epoch: 895. Loss: 0.8809. Acc.: 56.96%\n",
      "Epoch: 896. Loss: 0.8735. Acc.: 56.86%\n",
      "Epoch: 897. Loss: 0.8775. Acc.: 56.54%\n",
      "Epoch: 898. Loss: 0.8767. Acc.: 56.59%\n",
      "Epoch: 899. Loss: 0.8720. Acc.: 56.82%\n",
      "Epoch: 900. Loss: 0.8731. Acc.: 56.45%\n",
      "Epoch: 901. Loss: 0.8763. Acc.: 56.49%\n",
      "Epoch: 902. Loss: 0.8699. Acc.: 56.63%\n",
      "Epoch: 903. Loss: 0.8781. Acc.: 56.49%\n",
      "Epoch: 904. Loss: 0.8732. Acc.: 56.91%\n",
      "Epoch: 905. Loss: 0.8700. Acc.: 56.73%\n",
      "Epoch: 906. Loss: 0.8687. Acc.: 57.05%\n",
      "Epoch: 907. Loss: 0.8735. Acc.: 56.40%\n",
      "Epoch: 908. Loss: 0.8807. Acc.: 56.31%\n",
      "Epoch: 909. Loss: 0.8750. Acc.: 56.77%\n",
      "Epoch: 910. Loss: 0.8725. Acc.: 56.40%\n",
      "Epoch: 911. Loss: 0.8704. Acc.: 56.96%\n",
      "Epoch: 912. Loss: 0.8751. Acc.: 56.73%\n",
      "Epoch: 913. Loss: 0.8670. Acc.: 56.63%\n",
      "Epoch: 914. Loss: 0.8736. Acc.: 57.00%\n",
      "Epoch: 915. Loss: 0.8690. Acc.: 56.68%\n",
      "Epoch: 916. Loss: 0.8687. Acc.: 56.63%\n",
      "Epoch: 917. Loss: 0.8717. Acc.: 56.68%\n",
      "Epoch: 918. Loss: 0.8742. Acc.: 56.59%\n",
      "Epoch: 919. Loss: 0.8684. Acc.: 56.59%\n",
      "Epoch: 920. Loss: 0.8764. Acc.: 56.73%\n",
      "Epoch: 921. Loss: 0.8783. Acc.: 56.77%\n",
      "Epoch: 922. Loss: 0.8808. Acc.: 57.05%\n",
      "Epoch: 923. Loss: 0.8704. Acc.: 56.73%\n",
      "Epoch: 924. Loss: 0.8740. Acc.: 56.86%\n",
      "Epoch: 925. Loss: 0.8714. Acc.: 56.82%\n",
      "Epoch: 926. Loss: 0.8731. Acc.: 56.82%\n",
      "Epoch: 927. Loss: 0.8691. Acc.: 56.77%\n",
      "Epoch: 928. Loss: 0.8734. Acc.: 56.91%\n",
      "Epoch: 929. Loss: 0.8760. Acc.: 56.68%\n",
      "Epoch: 930. Loss: 0.8672. Acc.: 57.05%\n",
      "Epoch: 931. Loss: 0.8715. Acc.: 57.14%\n",
      "Epoch: 932. Loss: 0.8690. Acc.: 56.96%\n",
      "Epoch: 933. Loss: 0.8699. Acc.: 57.05%\n",
      "Epoch: 934. Loss: 0.8803. Acc.: 57.14%\n",
      "Epoch: 935. Loss: 0.8685. Acc.: 57.14%\n",
      "Epoch: 936. Loss: 0.8731. Acc.: 56.96%\n",
      "Epoch: 937. Loss: 0.8704. Acc.: 57.00%\n",
      "Epoch: 938. Loss: 0.8686. Acc.: 56.82%\n",
      "Epoch: 939. Loss: 0.8661. Acc.: 57.05%\n",
      "Epoch: 940. Loss: 0.8651. Acc.: 57.00%\n",
      "Epoch: 941. Loss: 0.8763. Acc.: 57.10%\n",
      "Epoch: 942. Loss: 0.8627. Acc.: 56.96%\n",
      "Epoch: 943. Loss: 0.8703. Acc.: 57.05%\n",
      "Epoch: 944. Loss: 0.8727. Acc.: 57.14%\n",
      "Epoch: 945. Loss: 0.8718. Acc.: 56.86%\n",
      "Epoch: 946. Loss: 0.8691. Acc.: 57.00%\n",
      "Epoch: 947. Loss: 0.8746. Acc.: 56.77%\n",
      "Epoch: 948. Loss: 0.8713. Acc.: 56.73%\n",
      "Epoch: 949. Loss: 0.8753. Acc.: 56.68%\n",
      "Epoch: 950. Loss: 0.8702. Acc.: 57.00%\n",
      "Epoch: 951. Loss: 0.8715. Acc.: 57.05%\n",
      "Epoch: 952. Loss: 0.8723. Acc.: 56.82%\n",
      "Epoch: 953. Loss: 0.8688. Acc.: 56.86%\n",
      "Epoch: 954. Loss: 0.8632. Acc.: 56.40%\n",
      "Epoch: 955. Loss: 0.8685. Acc.: 56.77%\n",
      "Epoch: 956. Loss: 0.8735. Acc.: 56.77%\n",
      "Epoch: 957. Loss: 0.8614. Acc.: 56.54%\n",
      "Epoch: 958. Loss: 0.8666. Acc.: 56.91%\n",
      "Epoch: 959. Loss: 0.8722. Acc.: 56.91%\n",
      "Epoch: 960. Loss: 0.8632. Acc.: 56.77%\n",
      "Epoch: 961. Loss: 0.8641. Acc.: 56.63%\n",
      "Epoch: 962. Loss: 0.8701. Acc.: 56.77%\n",
      "Epoch: 963. Loss: 0.8702. Acc.: 56.59%\n",
      "Epoch: 964. Loss: 0.8700. Acc.: 56.63%\n",
      "Epoch: 965. Loss: 0.8659. Acc.: 56.45%\n",
      "Epoch: 966. Loss: 0.8668. Acc.: 56.68%\n",
      "Epoch: 967. Loss: 0.8653. Acc.: 56.91%\n",
      "Epoch: 968. Loss: 0.8654. Acc.: 56.54%\n",
      "Epoch: 969. Loss: 0.8756. Acc.: 56.68%\n",
      "Epoch: 970. Loss: 0.8627. Acc.: 56.49%\n",
      "Epoch: 971. Loss: 0.8700. Acc.: 56.59%\n",
      "Epoch: 972. Loss: 0.8671. Acc.: 56.63%\n",
      "Epoch: 973. Loss: 0.8656. Acc.: 56.77%\n",
      "Epoch: 974. Loss: 0.8723. Acc.: 56.63%\n",
      "Epoch: 975. Loss: 0.8680. Acc.: 56.77%\n",
      "Epoch: 976. Loss: 0.8668. Acc.: 56.73%\n",
      "Epoch: 977. Loss: 0.8624. Acc.: 56.68%\n",
      "Epoch: 978. Loss: 0.8678. Acc.: 56.63%\n",
      "Epoch: 979. Loss: 0.8589. Acc.: 56.63%\n",
      "Epoch: 980. Loss: 0.8716. Acc.: 56.73%\n",
      "Epoch: 981. Loss: 0.8662. Acc.: 56.82%\n",
      "Epoch: 982. Loss: 0.8641. Acc.: 56.68%\n",
      "Epoch: 983. Loss: 0.8667. Acc.: 56.63%\n",
      "Epoch: 984. Loss: 0.8685. Acc.: 56.63%\n",
      "Epoch: 985. Loss: 0.8669. Acc.: 56.73%\n",
      "Epoch: 986. Loss: 0.8647. Acc.: 56.77%\n",
      "Epoch: 987. Loss: 0.8753. Acc.: 56.59%\n",
      "Epoch: 988. Loss: 0.8647. Acc.: 56.77%\n",
      "Epoch: 989. Loss: 0.8646. Acc.: 56.68%\n",
      "Epoch: 990. Loss: 0.8700. Acc.: 56.77%\n",
      "Epoch: 991. Loss: 0.8598. Acc.: 56.63%\n",
      "Epoch: 992. Loss: 0.8712. Acc.: 56.59%\n",
      "Epoch: 993. Loss: 0.8672. Acc.: 56.59%\n",
      "Epoch: 994. Loss: 0.8706. Acc.: 56.73%\n",
      "Epoch: 995. Loss: 0.8636. Acc.: 56.86%\n",
      "Epoch: 996. Loss: 0.8609. Acc.: 56.82%\n",
      "Epoch: 997. Loss: 0.8686. Acc.: 56.68%\n",
      "Epoch: 998. Loss: 0.8677. Acc.: 56.73%\n",
      "Epoch: 999. Loss: 0.8641. Acc.: 56.68%\n",
      "Epoch: 1000. Loss: 0.8708. Acc.: 56.68%\n",
      "Epoch: 1001. Loss: 0.8671. Acc.: 56.73%\n",
      "Epoch: 1002. Loss: 0.8685. Acc.: 56.73%\n",
      "Epoch: 1003. Loss: 0.8641. Acc.: 56.77%\n",
      "Epoch: 1004. Loss: 0.8654. Acc.: 56.73%\n",
      "Epoch: 1005. Loss: 0.8673. Acc.: 56.82%\n",
      "Epoch: 1006. Loss: 0.8689. Acc.: 56.77%\n",
      "Epoch: 1007. Loss: 0.8647. Acc.: 56.73%\n",
      "Epoch: 1008. Loss: 0.8649. Acc.: 56.68%\n",
      "Epoch: 1009. Loss: 0.8686. Acc.: 56.86%\n",
      "Epoch: 1010. Loss: 0.8590. Acc.: 56.77%\n",
      "Epoch: 1011. Loss: 0.8624. Acc.: 56.73%\n",
      "Epoch: 1012. Loss: 0.8700. Acc.: 56.77%\n",
      "Epoch: 1013. Loss: 0.8620. Acc.: 56.86%\n",
      "Epoch: 1014. Loss: 0.8680. Acc.: 56.82%\n",
      "Epoch: 1015. Loss: 0.8726. Acc.: 56.86%\n",
      "Epoch: 1016. Loss: 0.8629. Acc.: 56.82%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1017. Loss: 0.8633. Acc.: 56.82%\n",
      "Epoch: 1018. Loss: 0.8651. Acc.: 56.77%\n",
      "Epoch: 1019. Loss: 0.8718. Acc.: 56.82%\n",
      "Epoch: 1020. Loss: 0.8644. Acc.: 56.82%\n",
      "Epoch: 1021. Loss: 0.8610. Acc.: 56.73%\n",
      "Epoch: 1022. Loss: 0.8680. Acc.: 55.89%\n",
      "Epoch: 1023. Loss: 0.8884. Acc.: 56.68%\n",
      "Epoch: 1024. Loss: 0.8897. Acc.: 55.89%\n",
      "Epoch: 1025. Loss: 0.8870. Acc.: 57.88%\n",
      "Epoch: 1026. Loss: 0.8916. Acc.: 57.65%\n",
      "Epoch: 1027. Loss: 0.8897. Acc.: 57.79%\n",
      "Epoch: 1028. Loss: 0.8876. Acc.: 56.22%\n",
      "Epoch: 1029. Loss: 0.8901. Acc.: 57.98%\n",
      "Epoch: 1030. Loss: 0.8851. Acc.: 57.05%\n",
      "Epoch: 1031. Loss: 0.9021. Acc.: 58.30%\n",
      "Epoch: 1032. Loss: 0.8982. Acc.: 57.70%\n",
      "Epoch: 1033. Loss: 0.8900. Acc.: 57.56%\n",
      "Epoch: 1034. Loss: 0.8924. Acc.: 57.05%\n",
      "Epoch: 1035. Loss: 0.8909. Acc.: 55.57%\n",
      "Epoch: 1036. Loss: 0.8910. Acc.: 55.57%\n",
      "Epoch: 1037. Loss: 0.8903. Acc.: 57.28%\n",
      "Epoch: 1038. Loss: 0.8981. Acc.: 56.17%\n",
      "Epoch: 1039. Loss: 0.8910. Acc.: 56.35%\n",
      "Epoch: 1040. Loss: 0.8903. Acc.: 57.00%\n",
      "Epoch: 1041. Loss: 0.8814. Acc.: 57.70%\n",
      "Epoch: 1042. Loss: 0.8789. Acc.: 57.56%\n",
      "Epoch: 1043. Loss: 0.8833. Acc.: 57.61%\n",
      "Epoch: 1044. Loss: 0.8850. Acc.: 57.14%\n",
      "Epoch: 1045. Loss: 0.8900. Acc.: 57.14%\n",
      "Epoch: 1046. Loss: 0.8810. Acc.: 57.28%\n",
      "Epoch: 1047. Loss: 0.8864. Acc.: 57.61%\n",
      "Epoch: 1048. Loss: 0.8803. Acc.: 57.47%\n",
      "Epoch: 1049. Loss: 0.8761. Acc.: 57.00%\n",
      "Epoch: 1050. Loss: 0.8859. Acc.: 57.75%\n",
      "Epoch: 1051. Loss: 0.8807. Acc.: 56.45%\n",
      "Epoch: 1052. Loss: 0.8875. Acc.: 56.82%\n",
      "Epoch: 1053. Loss: 0.8828. Acc.: 58.35%\n",
      "Epoch: 1054. Loss: 0.8780. Acc.: 56.26%\n",
      "Epoch: 1055. Loss: 0.8809. Acc.: 58.02%\n",
      "Epoch: 1056. Loss: 0.8854. Acc.: 57.24%\n",
      "Epoch: 1057. Loss: 0.8872. Acc.: 56.59%\n",
      "Epoch: 1058. Loss: 0.8834. Acc.: 57.75%\n",
      "Epoch: 1059. Loss: 0.8797. Acc.: 57.28%\n",
      "Epoch: 1060. Loss: 0.8740. Acc.: 58.07%\n",
      "Epoch: 1061. Loss: 0.8899. Acc.: 55.71%\n",
      "Epoch: 1062. Loss: 0.8803. Acc.: 57.19%\n",
      "Epoch: 1063. Loss: 0.8691. Acc.: 57.61%\n",
      "Epoch: 1064. Loss: 0.8773. Acc.: 58.81%\n",
      "Epoch 1064 best model saved with accuracy: 58.81%\n",
      "Epoch: 1065. Loss: 0.8787. Acc.: 58.91%\n",
      "Epoch 1065 best model saved with accuracy: 58.91%\n",
      "Epoch: 1066. Loss: 0.8766. Acc.: 59.23%\n",
      "Epoch 1066 best model saved with accuracy: 59.23%\n",
      "Epoch: 1067. Loss: 0.8756. Acc.: 57.84%\n",
      "Epoch: 1068. Loss: 0.8806. Acc.: 58.91%\n",
      "Epoch: 1069. Loss: 0.8763. Acc.: 58.95%\n",
      "Epoch: 1070. Loss: 0.8755. Acc.: 57.75%\n",
      "Epoch: 1071. Loss: 0.8748. Acc.: 58.67%\n",
      "Epoch: 1072. Loss: 0.8776. Acc.: 59.69%\n",
      "Epoch 1072 best model saved with accuracy: 59.69%\n",
      "Epoch: 1073. Loss: 0.8760. Acc.: 56.54%\n",
      "Epoch: 1074. Loss: 0.8768. Acc.: 57.28%\n",
      "Epoch: 1075. Loss: 0.8704. Acc.: 57.75%\n",
      "Epoch: 1076. Loss: 0.8762. Acc.: 58.21%\n",
      "Epoch: 1077. Loss: 0.8665. Acc.: 58.63%\n",
      "Epoch: 1078. Loss: 0.8781. Acc.: 57.88%\n",
      "Epoch: 1079. Loss: 0.8689. Acc.: 57.88%\n",
      "Epoch: 1080. Loss: 0.8785. Acc.: 58.12%\n",
      "Epoch: 1081. Loss: 0.8744. Acc.: 57.61%\n",
      "Epoch: 1082. Loss: 0.8739. Acc.: 58.81%\n",
      "Epoch: 1083. Loss: 0.8726. Acc.: 57.51%\n",
      "Epoch: 1084. Loss: 0.8657. Acc.: 59.23%\n",
      "Epoch: 1085. Loss: 0.8759. Acc.: 57.51%\n",
      "Epoch: 1086. Loss: 0.8731. Acc.: 58.16%\n",
      "Epoch: 1087. Loss: 0.8723. Acc.: 58.44%\n",
      "Epoch: 1088. Loss: 0.8623. Acc.: 58.40%\n",
      "Epoch: 1089. Loss: 0.8645. Acc.: 57.75%\n",
      "Epoch: 1090. Loss: 0.8678. Acc.: 58.53%\n",
      "Epoch: 1091. Loss: 0.8605. Acc.: 58.91%\n",
      "Epoch: 1092. Loss: 0.8700. Acc.: 58.26%\n",
      "Epoch: 1093. Loss: 0.8647. Acc.: 58.95%\n",
      "Epoch: 1094. Loss: 0.8659. Acc.: 59.60%\n",
      "Epoch: 1095. Loss: 0.8643. Acc.: 59.18%\n",
      "Epoch: 1096. Loss: 0.8718. Acc.: 59.28%\n",
      "Epoch: 1097. Loss: 0.8606. Acc.: 58.77%\n",
      "Epoch: 1098. Loss: 0.8578. Acc.: 59.69%\n",
      "Epoch: 1099. Loss: 0.8666. Acc.: 58.95%\n",
      "Epoch: 1100. Loss: 0.8738. Acc.: 58.95%\n",
      "Epoch: 1101. Loss: 0.8656. Acc.: 58.40%\n",
      "Epoch: 1102. Loss: 0.8608. Acc.: 59.60%\n",
      "Epoch: 1103. Loss: 0.8717. Acc.: 59.42%\n",
      "Epoch: 1104. Loss: 0.8656. Acc.: 59.55%\n",
      "Epoch: 1105. Loss: 0.8618. Acc.: 58.35%\n",
      "Epoch: 1106. Loss: 0.8569. Acc.: 58.49%\n",
      "Epoch: 1107. Loss: 0.8662. Acc.: 59.51%\n",
      "Epoch: 1108. Loss: 0.8565. Acc.: 60.62%\n",
      "Epoch 1108 best model saved with accuracy: 60.62%\n",
      "Epoch: 1109. Loss: 0.8669. Acc.: 58.72%\n",
      "Epoch: 1110. Loss: 0.8656. Acc.: 59.60%\n",
      "Epoch: 1111. Loss: 0.8645. Acc.: 58.02%\n",
      "Epoch: 1112. Loss: 0.8682. Acc.: 59.00%\n",
      "Epoch: 1113. Loss: 0.8557. Acc.: 58.72%\n",
      "Epoch: 1114. Loss: 0.8547. Acc.: 59.32%\n",
      "Epoch: 1115. Loss: 0.8558. Acc.: 58.77%\n",
      "Epoch: 1116. Loss: 0.8554. Acc.: 57.79%\n",
      "Epoch: 1117. Loss: 0.8596. Acc.: 59.14%\n",
      "Epoch: 1118. Loss: 0.8657. Acc.: 58.95%\n",
      "Epoch: 1119. Loss: 0.8588. Acc.: 59.83%\n",
      "Epoch: 1120. Loss: 0.8515. Acc.: 59.69%\n",
      "Epoch: 1121. Loss: 0.8490. Acc.: 58.91%\n",
      "Epoch: 1122. Loss: 0.8424. Acc.: 58.77%\n",
      "Epoch: 1123. Loss: 0.8493. Acc.: 58.58%\n",
      "Epoch: 1124. Loss: 0.8602. Acc.: 58.49%\n",
      "Epoch: 1125. Loss: 0.8551. Acc.: 58.12%\n",
      "Epoch: 1126. Loss: 0.8532. Acc.: 58.72%\n",
      "Epoch: 1127. Loss: 0.8572. Acc.: 60.06%\n",
      "Epoch: 1128. Loss: 0.8558. Acc.: 59.00%\n",
      "Epoch: 1129. Loss: 0.8492. Acc.: 59.00%\n",
      "Epoch: 1130. Loss: 0.8535. Acc.: 60.06%\n",
      "Epoch: 1131. Loss: 0.8519. Acc.: 60.44%\n",
      "Epoch: 1132. Loss: 0.8493. Acc.: 57.33%\n",
      "Epoch: 1133. Loss: 0.8539. Acc.: 59.04%\n",
      "Epoch: 1134. Loss: 0.8464. Acc.: 58.81%\n",
      "Epoch: 1135. Loss: 0.8527. Acc.: 58.49%\n",
      "Epoch: 1136. Loss: 0.8455. Acc.: 59.04%\n",
      "Epoch: 1137. Loss: 0.8442. Acc.: 58.26%\n",
      "Epoch: 1138. Loss: 0.8496. Acc.: 58.49%\n",
      "Epoch: 1139. Loss: 0.8530. Acc.: 58.07%\n",
      "Epoch: 1140. Loss: 0.8481. Acc.: 57.98%\n",
      "Epoch: 1141. Loss: 0.8563. Acc.: 59.23%\n",
      "Epoch: 1142. Loss: 0.8525. Acc.: 58.02%\n",
      "Epoch: 1143. Loss: 0.8448. Acc.: 58.30%\n",
      "Epoch: 1144. Loss: 0.8540. Acc.: 59.23%\n",
      "Epoch: 1145. Loss: 0.8486. Acc.: 58.91%\n",
      "Epoch: 1146. Loss: 0.8416. Acc.: 57.70%\n",
      "Epoch: 1147. Loss: 0.8475. Acc.: 57.33%\n",
      "Epoch: 1148. Loss: 0.8451. Acc.: 58.53%\n",
      "Epoch: 1149. Loss: 0.8389. Acc.: 59.04%\n",
      "Epoch: 1150. Loss: 0.8482. Acc.: 58.16%\n",
      "Epoch: 1151. Loss: 0.8410. Acc.: 60.20%\n",
      "Epoch: 1152. Loss: 0.8551. Acc.: 57.56%\n",
      "Epoch: 1153. Loss: 0.8382. Acc.: 59.55%\n",
      "Epoch: 1154. Loss: 0.8362. Acc.: 58.58%\n",
      "Epoch: 1155. Loss: 0.8487. Acc.: 59.32%\n",
      "Epoch: 1156. Loss: 0.8447. Acc.: 58.02%\n",
      "Epoch: 1157. Loss: 0.8383. Acc.: 58.63%\n",
      "Epoch: 1158. Loss: 0.8419. Acc.: 59.51%\n",
      "Epoch: 1159. Loss: 0.8473. Acc.: 58.77%\n",
      "Epoch: 1160. Loss: 0.8403. Acc.: 58.21%\n",
      "Epoch: 1161. Loss: 0.8373. Acc.: 60.11%\n",
      "Epoch: 1162. Loss: 0.8456. Acc.: 59.14%\n",
      "Epoch: 1163. Loss: 0.8493. Acc.: 58.49%\n",
      "Epoch: 1164. Loss: 0.8447. Acc.: 59.14%\n",
      "Epoch: 1165. Loss: 0.8381. Acc.: 58.53%\n",
      "Epoch: 1166. Loss: 0.8357. Acc.: 59.04%\n",
      "Epoch: 1167. Loss: 0.8379. Acc.: 58.86%\n",
      "Epoch: 1168. Loss: 0.8401. Acc.: 59.14%\n",
      "Epoch: 1169. Loss: 0.8371. Acc.: 59.97%\n",
      "Epoch: 1170. Loss: 0.8405. Acc.: 59.74%\n",
      "Epoch: 1171. Loss: 0.8456. Acc.: 59.65%\n",
      "Epoch: 1172. Loss: 0.8413. Acc.: 59.79%\n",
      "Epoch: 1173. Loss: 0.8367. Acc.: 59.60%\n",
      "Epoch: 1174. Loss: 0.8397. Acc.: 59.32%\n",
      "Epoch: 1175. Loss: 0.8483. Acc.: 59.09%\n",
      "Epoch: 1176. Loss: 0.8326. Acc.: 59.65%\n",
      "Epoch: 1177. Loss: 0.8395. Acc.: 59.55%\n",
      "Epoch: 1178. Loss: 0.8351. Acc.: 60.11%\n",
      "Epoch: 1179. Loss: 0.8402. Acc.: 59.55%\n",
      "Epoch: 1180. Loss: 0.8381. Acc.: 58.30%\n",
      "Epoch: 1181. Loss: 0.8355. Acc.: 59.93%\n",
      "Epoch: 1182. Loss: 0.8319. Acc.: 59.51%\n",
      "Epoch: 1183. Loss: 0.8392. Acc.: 59.32%\n",
      "Epoch: 1184. Loss: 0.8339. Acc.: 59.88%\n",
      "Epoch: 1185. Loss: 0.8341. Acc.: 59.00%\n",
      "Epoch: 1186. Loss: 0.8358. Acc.: 59.46%\n",
      "Epoch: 1187. Loss: 0.8321. Acc.: 60.11%\n",
      "Epoch: 1188. Loss: 0.8379. Acc.: 59.65%\n",
      "Epoch: 1189. Loss: 0.8339. Acc.: 57.93%\n",
      "Epoch: 1190. Loss: 0.8351. Acc.: 58.21%\n",
      "Epoch: 1191. Loss: 0.8302. Acc.: 59.55%\n",
      "Epoch: 1192. Loss: 0.8337. Acc.: 58.91%\n",
      "Epoch: 1193. Loss: 0.8404. Acc.: 57.61%\n",
      "Epoch: 1194. Loss: 0.8342. Acc.: 59.28%\n",
      "Epoch: 1195. Loss: 0.8290. Acc.: 59.09%\n",
      "Epoch: 1196. Loss: 0.8301. Acc.: 58.63%\n",
      "Epoch: 1197. Loss: 0.8335. Acc.: 58.86%\n",
      "Epoch: 1198. Loss: 0.8247. Acc.: 59.23%\n",
      "Epoch: 1199. Loss: 0.8300. Acc.: 59.04%\n",
      "Epoch: 1200. Loss: 0.8331. Acc.: 59.79%\n",
      "Epoch: 1201. Loss: 0.8303. Acc.: 59.55%\n",
      "Epoch: 1202. Loss: 0.8374. Acc.: 59.04%\n",
      "Epoch: 1203. Loss: 0.8345. Acc.: 59.00%\n",
      "Epoch: 1204. Loss: 0.8301. Acc.: 59.32%\n",
      "Epoch: 1205. Loss: 0.8265. Acc.: 60.48%\n",
      "Epoch: 1206. Loss: 0.8266. Acc.: 59.28%\n",
      "Epoch: 1207. Loss: 0.8300. Acc.: 59.74%\n",
      "Epoch: 1208. Loss: 0.8320. Acc.: 59.42%\n",
      "Epoch: 1209. Loss: 0.8225. Acc.: 58.49%\n",
      "Epoch: 1210. Loss: 0.8247. Acc.: 59.32%\n",
      "Epoch: 1211. Loss: 0.8235. Acc.: 58.02%\n",
      "Epoch: 1212. Loss: 0.8221. Acc.: 58.58%\n",
      "Epoch: 1213. Loss: 0.8263. Acc.: 58.63%\n",
      "Epoch: 1214. Loss: 0.8249. Acc.: 58.44%\n",
      "Epoch: 1215. Loss: 0.8293. Acc.: 59.79%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1216. Loss: 0.8310. Acc.: 59.60%\n",
      "Epoch: 1217. Loss: 0.8353. Acc.: 59.46%\n",
      "Epoch: 1218. Loss: 0.8276. Acc.: 59.04%\n",
      "Epoch: 1219. Loss: 0.8249. Acc.: 59.14%\n",
      "Epoch: 1220. Loss: 0.8246. Acc.: 57.93%\n",
      "Epoch: 1221. Loss: 0.8253. Acc.: 58.81%\n",
      "Epoch: 1222. Loss: 0.8259. Acc.: 59.74%\n",
      "Epoch: 1223. Loss: 0.8225. Acc.: 59.65%\n",
      "Epoch: 1224. Loss: 0.8267. Acc.: 59.60%\n",
      "Epoch: 1225. Loss: 0.8136. Acc.: 60.11%\n",
      "Epoch: 1226. Loss: 0.8202. Acc.: 58.81%\n",
      "Epoch: 1227. Loss: 0.8220. Acc.: 59.88%\n",
      "Epoch: 1228. Loss: 0.8217. Acc.: 59.79%\n",
      "Epoch: 1229. Loss: 0.8267. Acc.: 59.42%\n",
      "Epoch: 1230. Loss: 0.8188. Acc.: 59.55%\n",
      "Epoch: 1231. Loss: 0.8183. Acc.: 58.86%\n",
      "Epoch: 1232. Loss: 0.8221. Acc.: 59.60%\n",
      "Epoch: 1233. Loss: 0.8199. Acc.: 59.37%\n",
      "Epoch: 1234. Loss: 0.8267. Acc.: 59.32%\n",
      "Epoch: 1235. Loss: 0.8283. Acc.: 59.79%\n",
      "Epoch: 1236. Loss: 0.8249. Acc.: 59.65%\n",
      "Epoch: 1237. Loss: 0.8216. Acc.: 59.42%\n",
      "Epoch: 1238. Loss: 0.8366. Acc.: 59.55%\n",
      "Epoch: 1239. Loss: 0.8179. Acc.: 58.91%\n",
      "Epoch: 1240. Loss: 0.8229. Acc.: 59.04%\n",
      "Epoch: 1241. Loss: 0.8207. Acc.: 59.97%\n",
      "Epoch: 1242. Loss: 0.8186. Acc.: 59.18%\n",
      "Epoch: 1243. Loss: 0.8151. Acc.: 59.28%\n",
      "Epoch: 1244. Loss: 0.8151. Acc.: 59.18%\n",
      "Epoch: 1245. Loss: 0.8126. Acc.: 58.95%\n",
      "Epoch: 1246. Loss: 0.8216. Acc.: 59.51%\n",
      "Epoch: 1247. Loss: 0.8217. Acc.: 59.04%\n",
      "Epoch: 1248. Loss: 0.8165. Acc.: 59.37%\n",
      "Epoch: 1249. Loss: 0.8106. Acc.: 59.00%\n",
      "Epoch: 1250. Loss: 0.8153. Acc.: 58.58%\n",
      "Epoch: 1251. Loss: 0.8212. Acc.: 59.28%\n",
      "Epoch: 1252. Loss: 0.8216. Acc.: 59.42%\n",
      "Epoch: 1253. Loss: 0.8168. Acc.: 59.79%\n",
      "Epoch: 1254. Loss: 0.8161. Acc.: 59.88%\n",
      "Epoch: 1255. Loss: 0.8160. Acc.: 59.23%\n",
      "Epoch: 1256. Loss: 0.8187. Acc.: 59.28%\n",
      "Epoch: 1257. Loss: 0.8115. Acc.: 58.95%\n",
      "Epoch: 1258. Loss: 0.8083. Acc.: 59.14%\n",
      "Epoch: 1259. Loss: 0.8207. Acc.: 59.83%\n",
      "Epoch: 1260. Loss: 0.8216. Acc.: 58.53%\n",
      "Epoch: 1261. Loss: 0.8148. Acc.: 59.83%\n",
      "Epoch: 1262. Loss: 0.8105. Acc.: 59.42%\n",
      "Epoch: 1263. Loss: 0.8126. Acc.: 59.51%\n",
      "Epoch: 1264. Loss: 0.8077. Acc.: 59.09%\n",
      "Epoch: 1265. Loss: 0.8115. Acc.: 58.63%\n",
      "Epoch: 1266. Loss: 0.8080. Acc.: 60.20%\n",
      "Epoch: 1267. Loss: 0.8046. Acc.: 59.74%\n",
      "Epoch: 1268. Loss: 0.8076. Acc.: 60.11%\n",
      "Epoch: 1269. Loss: 0.8105. Acc.: 58.81%\n",
      "Epoch: 1270. Loss: 0.8097. Acc.: 59.23%\n",
      "Epoch: 1271. Loss: 0.8101. Acc.: 59.37%\n",
      "Epoch: 1272. Loss: 0.8104. Acc.: 59.04%\n",
      "Epoch: 1273. Loss: 0.8085. Acc.: 59.55%\n",
      "Epoch: 1274. Loss: 0.8045. Acc.: 59.28%\n",
      "Epoch: 1275. Loss: 0.8074. Acc.: 59.46%\n",
      "Epoch: 1276. Loss: 0.8117. Acc.: 59.09%\n",
      "Epoch: 1277. Loss: 0.8126. Acc.: 59.65%\n",
      "Epoch: 1278. Loss: 0.8057. Acc.: 59.55%\n",
      "Epoch: 1279. Loss: 0.8095. Acc.: 59.00%\n",
      "Epoch: 1280. Loss: 0.8066. Acc.: 59.18%\n",
      "Epoch: 1281. Loss: 0.8113. Acc.: 58.77%\n",
      "Epoch: 1282. Loss: 0.8165. Acc.: 60.11%\n",
      "Epoch: 1283. Loss: 0.8058. Acc.: 58.95%\n",
      "Epoch: 1284. Loss: 0.8096. Acc.: 58.86%\n",
      "Epoch: 1285. Loss: 0.8114. Acc.: 60.02%\n",
      "Epoch: 1286. Loss: 0.8124. Acc.: 59.93%\n",
      "Epoch: 1287. Loss: 0.8066. Acc.: 58.95%\n",
      "Epoch: 1288. Loss: 0.8093. Acc.: 59.09%\n",
      "Epoch: 1289. Loss: 0.8044. Acc.: 59.60%\n",
      "Epoch: 1290. Loss: 0.8046. Acc.: 59.46%\n",
      "Epoch: 1291. Loss: 0.8065. Acc.: 59.69%\n",
      "Epoch: 1292. Loss: 0.8044. Acc.: 59.04%\n",
      "Epoch: 1293. Loss: 0.8035. Acc.: 59.32%\n",
      "Epoch: 1294. Loss: 0.8028. Acc.: 58.95%\n",
      "Epoch: 1295. Loss: 0.8045. Acc.: 58.63%\n",
      "Epoch: 1296. Loss: 0.8034. Acc.: 58.72%\n",
      "Epoch: 1297. Loss: 0.8033. Acc.: 59.09%\n",
      "Epoch: 1298. Loss: 0.8081. Acc.: 58.91%\n",
      "Epoch: 1299. Loss: 0.7958. Acc.: 59.37%\n",
      "Epoch: 1300. Loss: 0.8068. Acc.: 58.91%\n",
      "Epoch: 1301. Loss: 0.8062. Acc.: 58.91%\n",
      "Epoch: 1302. Loss: 0.8001. Acc.: 59.37%\n",
      "Epoch: 1303. Loss: 0.8030. Acc.: 59.28%\n",
      "Epoch: 1304. Loss: 0.8045. Acc.: 58.81%\n",
      "Epoch: 1305. Loss: 0.8018. Acc.: 59.00%\n",
      "Epoch: 1306. Loss: 0.8046. Acc.: 58.86%\n",
      "Epoch: 1307. Loss: 0.8018. Acc.: 59.37%\n",
      "Epoch: 1308. Loss: 0.8011. Acc.: 59.14%\n",
      "Epoch: 1309. Loss: 0.8017. Acc.: 59.23%\n",
      "Epoch: 1310. Loss: 0.8026. Acc.: 59.00%\n",
      "Epoch: 1311. Loss: 0.8012. Acc.: 59.69%\n",
      "Epoch: 1312. Loss: 0.7925. Acc.: 59.55%\n",
      "Epoch: 1313. Loss: 0.8035. Acc.: 59.37%\n",
      "Epoch: 1314. Loss: 0.8075. Acc.: 59.60%\n",
      "Epoch: 1315. Loss: 0.7964. Acc.: 58.91%\n",
      "Epoch: 1316. Loss: 0.7966. Acc.: 59.42%\n",
      "Epoch: 1317. Loss: 0.7952. Acc.: 59.83%\n",
      "Epoch: 1318. Loss: 0.7925. Acc.: 59.37%\n",
      "Epoch: 1319. Loss: 0.8003. Acc.: 59.60%\n",
      "Epoch: 1320. Loss: 0.7992. Acc.: 59.37%\n",
      "Epoch: 1321. Loss: 0.8044. Acc.: 59.28%\n",
      "Epoch: 1322. Loss: 0.7983. Acc.: 59.55%\n",
      "Epoch: 1323. Loss: 0.7993. Acc.: 59.32%\n",
      "Epoch: 1324. Loss: 0.7981. Acc.: 59.83%\n",
      "Epoch: 1325. Loss: 0.8002. Acc.: 59.28%\n",
      "Epoch: 1326. Loss: 0.7966. Acc.: 59.46%\n",
      "Epoch: 1327. Loss: 0.8054. Acc.: 59.46%\n",
      "Epoch: 1328. Loss: 0.7980. Acc.: 59.32%\n",
      "Epoch: 1329. Loss: 0.8014. Acc.: 58.63%\n",
      "Epoch: 1330. Loss: 0.7947. Acc.: 58.86%\n",
      "Epoch: 1331. Loss: 0.7932. Acc.: 58.86%\n",
      "Epoch: 1332. Loss: 0.7951. Acc.: 60.25%\n",
      "Epoch: 1333. Loss: 0.7917. Acc.: 59.04%\n",
      "Epoch: 1334. Loss: 0.7989. Acc.: 59.00%\n",
      "Epoch: 1335. Loss: 0.7910. Acc.: 59.00%\n",
      "Epoch: 1336. Loss: 0.7974. Acc.: 59.18%\n",
      "Epoch: 1337. Loss: 0.7941. Acc.: 59.42%\n",
      "Epoch: 1338. Loss: 0.7945. Acc.: 58.86%\n",
      "Epoch: 1339. Loss: 0.7894. Acc.: 58.63%\n",
      "Epoch: 1340. Loss: 0.7952. Acc.: 59.23%\n",
      "Epoch: 1341. Loss: 0.7889. Acc.: 59.55%\n",
      "Epoch: 1342. Loss: 0.7898. Acc.: 59.55%\n",
      "Epoch: 1343. Loss: 0.7927. Acc.: 59.46%\n",
      "Epoch: 1344. Loss: 0.7944. Acc.: 59.55%\n",
      "Epoch: 1345. Loss: 0.7979. Acc.: 59.79%\n",
      "Epoch: 1346. Loss: 0.8047. Acc.: 59.46%\n",
      "Epoch: 1347. Loss: 0.7943. Acc.: 59.28%\n",
      "Epoch: 1348. Loss: 0.7884. Acc.: 59.69%\n",
      "Epoch: 1349. Loss: 0.7902. Acc.: 59.46%\n",
      "Epoch: 1350. Loss: 0.7960. Acc.: 58.95%\n",
      "Epoch: 1351. Loss: 0.7845. Acc.: 59.09%\n",
      "Epoch: 1352. Loss: 0.7949. Acc.: 59.09%\n",
      "Epoch: 1353. Loss: 0.7914. Acc.: 58.86%\n",
      "Epoch: 1354. Loss: 0.7879. Acc.: 59.00%\n",
      "Epoch: 1355. Loss: 0.7926. Acc.: 58.86%\n",
      "Epoch: 1356. Loss: 0.7918. Acc.: 59.32%\n",
      "Epoch: 1357. Loss: 0.7997. Acc.: 59.04%\n",
      "Epoch: 1358. Loss: 0.7908. Acc.: 59.23%\n",
      "Epoch: 1359. Loss: 0.7899. Acc.: 59.51%\n",
      "Epoch: 1360. Loss: 0.7875. Acc.: 59.37%\n",
      "Epoch: 1361. Loss: 0.7887. Acc.: 58.95%\n",
      "Epoch: 1362. Loss: 0.7898. Acc.: 59.51%\n",
      "Epoch: 1363. Loss: 0.7904. Acc.: 58.77%\n",
      "Epoch: 1364. Loss: 0.7927. Acc.: 60.02%\n",
      "Epoch: 1365. Loss: 0.7895. Acc.: 58.95%\n",
      "Epoch: 1366. Loss: 0.7917. Acc.: 59.28%\n",
      "Epoch: 1367. Loss: 0.7929. Acc.: 59.51%\n",
      "Epoch: 1368. Loss: 0.7805. Acc.: 59.32%\n",
      "Epoch: 1369. Loss: 0.7889. Acc.: 58.95%\n",
      "Epoch: 1370. Loss: 0.7876. Acc.: 59.04%\n",
      "Epoch: 1371. Loss: 0.7875. Acc.: 59.28%\n",
      "Epoch: 1372. Loss: 0.7821. Acc.: 59.51%\n",
      "Epoch: 1373. Loss: 0.7859. Acc.: 59.37%\n",
      "Epoch: 1374. Loss: 0.7795. Acc.: 59.55%\n",
      "Epoch: 1375. Loss: 0.7869. Acc.: 59.14%\n",
      "Epoch: 1376. Loss: 0.7857. Acc.: 59.14%\n",
      "Epoch: 1377. Loss: 0.7877. Acc.: 59.55%\n",
      "Epoch: 1378. Loss: 0.7850. Acc.: 59.23%\n",
      "Epoch: 1379. Loss: 0.7829. Acc.: 59.60%\n",
      "Epoch: 1380. Loss: 0.7742. Acc.: 59.23%\n",
      "Epoch: 1381. Loss: 0.7931. Acc.: 59.65%\n",
      "Epoch: 1382. Loss: 0.7904. Acc.: 59.42%\n",
      "Epoch: 1383. Loss: 0.7974. Acc.: 59.42%\n",
      "Epoch: 1384. Loss: 0.7938. Acc.: 59.09%\n",
      "Epoch: 1385. Loss: 0.7823. Acc.: 59.32%\n",
      "Epoch: 1386. Loss: 0.7830. Acc.: 59.28%\n",
      "Epoch: 1387. Loss: 0.7899. Acc.: 59.23%\n",
      "Epoch: 1388. Loss: 0.7785. Acc.: 59.18%\n",
      "Epoch: 1389. Loss: 0.7766. Acc.: 59.42%\n",
      "Epoch: 1390. Loss: 0.7894. Acc.: 59.28%\n",
      "Epoch: 1391. Loss: 0.7903. Acc.: 59.04%\n",
      "Epoch: 1392. Loss: 0.7892. Acc.: 58.86%\n",
      "Epoch: 1393. Loss: 0.7873. Acc.: 59.04%\n",
      "Epoch: 1394. Loss: 0.7840. Acc.: 58.81%\n",
      "Epoch: 1395. Loss: 0.7873. Acc.: 59.14%\n",
      "Epoch: 1396. Loss: 0.7933. Acc.: 59.04%\n",
      "Epoch: 1397. Loss: 0.7884. Acc.: 59.37%\n",
      "Epoch: 1398. Loss: 0.7864. Acc.: 59.55%\n",
      "Epoch: 1399. Loss: 0.7892. Acc.: 59.32%\n",
      "Epoch: 1400. Loss: 0.7836. Acc.: 59.14%\n",
      "Epoch: 1401. Loss: 0.7756. Acc.: 59.32%\n",
      "Epoch: 1402. Loss: 0.7801. Acc.: 59.42%\n",
      "Epoch: 1403. Loss: 0.7827. Acc.: 59.04%\n",
      "Epoch: 1404. Loss: 0.7825. Acc.: 59.55%\n",
      "Epoch: 1405. Loss: 0.7848. Acc.: 59.55%\n",
      "Epoch: 1406. Loss: 0.7875. Acc.: 59.55%\n",
      "Epoch: 1407. Loss: 0.7809. Acc.: 59.65%\n",
      "Epoch: 1408. Loss: 0.7906. Acc.: 59.69%\n",
      "Epoch: 1409. Loss: 0.7796. Acc.: 59.42%\n",
      "Epoch: 1410. Loss: 0.7806. Acc.: 59.51%\n",
      "Epoch: 1411. Loss: 0.7831. Acc.: 59.69%\n",
      "Epoch: 1412. Loss: 0.7816. Acc.: 59.42%\n",
      "Epoch: 1413. Loss: 0.7891. Acc.: 59.55%\n",
      "Epoch: 1414. Loss: 0.7775. Acc.: 59.28%\n",
      "Epoch: 1415. Loss: 0.7831. Acc.: 59.28%\n",
      "Epoch: 1416. Loss: 0.7810. Acc.: 59.60%\n",
      "Epoch: 1417. Loss: 0.7891. Acc.: 59.14%\n",
      "Epoch: 1418. Loss: 0.7836. Acc.: 59.46%\n",
      "Epoch: 1419. Loss: 0.7799. Acc.: 59.23%\n",
      "Epoch: 1420. Loss: 0.7885. Acc.: 59.28%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1421. Loss: 0.7829. Acc.: 59.32%\n",
      "Epoch: 1422. Loss: 0.7834. Acc.: 59.28%\n",
      "Epoch: 1423. Loss: 0.7814. Acc.: 59.28%\n",
      "Epoch: 1424. Loss: 0.7812. Acc.: 59.46%\n",
      "Epoch: 1425. Loss: 0.7741. Acc.: 59.42%\n",
      "Epoch: 1426. Loss: 0.7722. Acc.: 59.18%\n",
      "Epoch: 1427. Loss: 0.7836. Acc.: 59.14%\n",
      "Epoch: 1428. Loss: 0.7767. Acc.: 59.04%\n",
      "Epoch: 1429. Loss: 0.7731. Acc.: 59.09%\n",
      "Epoch: 1430. Loss: 0.7835. Acc.: 59.23%\n",
      "Epoch: 1431. Loss: 0.7776. Acc.: 59.23%\n",
      "Epoch: 1432. Loss: 0.7850. Acc.: 59.23%\n",
      "Epoch: 1433. Loss: 0.7805. Acc.: 59.55%\n",
      "Epoch: 1434. Loss: 0.7850. Acc.: 59.46%\n",
      "Epoch: 1435. Loss: 0.7853. Acc.: 59.28%\n",
      "Epoch: 1436. Loss: 0.7751. Acc.: 59.69%\n",
      "Epoch: 1437. Loss: 0.7741. Acc.: 59.37%\n",
      "Epoch: 1438. Loss: 0.7784. Acc.: 59.04%\n",
      "Epoch: 1439. Loss: 0.7829. Acc.: 59.23%\n",
      "Epoch: 1440. Loss: 0.7832. Acc.: 59.18%\n",
      "Epoch: 1441. Loss: 0.7826. Acc.: 59.46%\n",
      "Epoch: 1442. Loss: 0.7869. Acc.: 59.23%\n",
      "Epoch: 1443. Loss: 0.7808. Acc.: 59.37%\n",
      "Epoch: 1444. Loss: 0.7788. Acc.: 59.51%\n",
      "Epoch: 1445. Loss: 0.7845. Acc.: 59.18%\n",
      "Epoch: 1446. Loss: 0.7784. Acc.: 59.32%\n",
      "Epoch: 1447. Loss: 0.7791. Acc.: 59.32%\n",
      "Epoch: 1448. Loss: 0.7775. Acc.: 59.18%\n",
      "Epoch: 1451. Loss: 0.7794. Acc.: 59.23%\n",
      "Epoch: 1452. Loss: 0.7752. Acc.: 59.28%\n",
      "Epoch: 1453. Loss: 0.7791. Acc.: 59.51%\n",
      "Epoch: 1454. Loss: 0.7769. Acc.: 59.46%\n",
      "Epoch: 1455. Loss: 0.7815. Acc.: 59.42%\n",
      "Epoch: 1456. Loss: 0.7705. Acc.: 59.28%\n",
      "Epoch: 1457. Loss: 0.7745. Acc.: 59.32%\n",
      "Epoch: 1458. Loss: 0.7839. Acc.: 59.28%\n",
      "Epoch: 1459. Loss: 0.7760. Acc.: 59.04%\n",
      "Epoch: 1460. Loss: 0.7833. Acc.: 59.18%\n",
      "Epoch: 1461. Loss: 0.7755. Acc.: 59.09%\n",
      "Epoch: 1462. Loss: 0.7755. Acc.: 59.32%\n",
      "Epoch: 1463. Loss: 0.7828. Acc.: 59.18%\n",
      "Epoch: 1464. Loss: 0.7765. Acc.: 59.37%\n",
      "Epoch: 1465. Loss: 0.7798. Acc.: 59.37%\n",
      "Epoch: 1466. Loss: 0.7747. Acc.: 59.32%\n",
      "Epoch: 1467. Loss: 0.7825. Acc.: 59.18%\n",
      "Epoch: 1468. Loss: 0.7758. Acc.: 59.32%\n",
      "Epoch: 1469. Loss: 0.7773. Acc.: 59.42%\n",
      "Epoch: 1470. Loss: 0.7856. Acc.: 59.46%\n",
      "Epoch: 1471. Loss: 0.7781. Acc.: 59.46%\n",
      "Epoch: 1472. Loss: 0.7839. Acc.: 59.60%\n",
      "Epoch: 1473. Loss: 0.7718. Acc.: 59.37%\n",
      "Epoch: 1474. Loss: 0.7740. Acc.: 59.23%\n",
      "Epoch: 1475. Loss: 0.7791. Acc.: 59.23%\n",
      "Epoch: 1476. Loss: 0.7742. Acc.: 59.23%\n",
      "Epoch: 1477. Loss: 0.7783. Acc.: 59.32%\n",
      "Epoch: 1478. Loss: 0.7824. Acc.: 59.14%\n",
      "Epoch: 1479. Loss: 0.7743. Acc.: 59.04%\n",
      "Epoch: 1480. Loss: 0.7746. Acc.: 59.18%\n",
      "Epoch: 1481. Loss: 0.7677. Acc.: 59.28%\n",
      "Epoch: 1482. Loss: 0.7735. Acc.: 59.14%\n",
      "Epoch: 1483. Loss: 0.7710. Acc.: 59.09%\n",
      "Epoch: 1484. Loss: 0.7779. Acc.: 59.23%\n",
      "Epoch: 1485. Loss: 0.7836. Acc.: 59.14%\n",
      "Epoch: 1486. Loss: 0.7756. Acc.: 59.18%\n",
      "Epoch: 1487. Loss: 0.7795. Acc.: 59.18%\n",
      "Epoch: 1488. Loss: 0.7743. Acc.: 59.23%\n",
      "Epoch: 1489. Loss: 0.7720. Acc.: 59.09%\n",
      "Epoch: 1490. Loss: 0.7741. Acc.: 59.09%\n",
      "Epoch: 1491. Loss: 0.7723. Acc.: 58.91%\n",
      "Epoch: 1492. Loss: 0.7729. Acc.: 59.09%\n",
      "Epoch: 1493. Loss: 0.7786. Acc.: 59.18%\n",
      "Epoch: 1494. Loss: 0.7747. Acc.: 59.23%\n",
      "Epoch: 1495. Loss: 0.7725. Acc.: 59.14%\n",
      "Epoch: 1496. Loss: 0.7736. Acc.: 59.42%\n",
      "Epoch: 1497. Loss: 0.7727. Acc.: 59.28%\n",
      "Epoch: 1498. Loss: 0.7747. Acc.: 59.28%\n",
      "Epoch: 1499. Loss: 0.7757. Acc.: 59.32%\n",
      "Epoch: 1500. Loss: 0.7823. Acc.: 59.23%\n",
      "Epoch: 1501. Loss: 0.7785. Acc.: 59.23%\n",
      "Epoch: 1502. Loss: 0.7816. Acc.: 59.14%\n",
      "Epoch: 1503. Loss: 0.7654. Acc.: 59.23%\n",
      "Epoch: 1504. Loss: 0.7744. Acc.: 59.23%\n",
      "Epoch: 1505. Loss: 0.7720. Acc.: 59.28%\n",
      "Epoch: 1506. Loss: 0.7720. Acc.: 59.28%\n",
      "Epoch: 1507. Loss: 0.7780. Acc.: 59.14%\n",
      "Epoch: 1508. Loss: 0.7755. Acc.: 59.14%\n",
      "Epoch: 1509. Loss: 0.7809. Acc.: 59.09%\n",
      "Epoch: 1510. Loss: 0.7727. Acc.: 59.28%\n",
      "Epoch: 1511. Loss: 0.7679. Acc.: 59.28%\n",
      "Epoch: 1512. Loss: 0.7739. Acc.: 59.14%\n",
      "Epoch: 1513. Loss: 0.7711. Acc.: 59.32%\n",
      "Epoch: 1514. Loss: 0.7758. Acc.: 59.23%\n",
      "Epoch: 1515. Loss: 0.7782. Acc.: 59.23%\n",
      "Epoch: 1516. Loss: 0.7737. Acc.: 59.23%\n",
      "Epoch: 1517. Loss: 0.7800. Acc.: 59.28%\n",
      "Epoch: 1518. Loss: 0.7696. Acc.: 59.28%\n",
      "Epoch: 1519. Loss: 0.7757. Acc.: 59.28%\n",
      "Epoch: 1520. Loss: 0.7743. Acc.: 59.23%\n",
      "Epoch: 1521. Loss: 0.7826. Acc.: 59.32%\n",
      "Epoch: 1522. Loss: 0.7738. Acc.: 59.32%\n",
      "Epoch: 1523. Loss: 0.7708. Acc.: 59.28%\n",
      "Epoch: 1524. Loss: 0.7735. Acc.: 59.42%\n",
      "Epoch: 1525. Loss: 0.7738. Acc.: 59.32%\n",
      "Epoch: 1526. Loss: 0.7785. Acc.: 59.28%\n",
      "Epoch: 1527. Loss: 0.7721. Acc.: 59.28%\n",
      "Epoch: 1528. Loss: 0.7760. Acc.: 59.32%\n",
      "Epoch: 1529. Loss: 0.7733. Acc.: 59.18%\n",
      "Epoch: 1530. Loss: 0.7812. Acc.: 59.18%\n",
      "Epoch: 1531. Loss: 0.7793. Acc.: 59.23%\n",
      "Epoch: 1532. Loss: 0.7727. Acc.: 59.23%\n",
      "Epoch: 1533. Loss: 0.7872. Acc.: 59.55%\n",
      "Epoch: 1534. Loss: 0.8008. Acc.: 58.67%\n",
      "Epoch: 1535. Loss: 0.7983. Acc.: 60.48%\n",
      "Epoch: 1536. Loss: 0.8021. Acc.: 59.37%\n",
      "Epoch: 1537. Loss: 0.8031. Acc.: 59.32%\n",
      "Epoch: 1538. Loss: 0.8037. Acc.: 60.06%\n",
      "Epoch: 1539. Loss: 0.8032. Acc.: 60.67%\n",
      "Epoch 1539 best model saved with accuracy: 60.67%\n",
      "Epoch: 1540. Loss: 0.8064. Acc.: 59.42%\n",
      "Epoch: 1541. Loss: 0.8029. Acc.: 59.93%\n",
      "Epoch: 1542. Loss: 0.8073. Acc.: 59.93%\n",
      "Epoch: 1543. Loss: 0.8134. Acc.: 60.44%\n",
      "Epoch: 1544. Loss: 0.8082. Acc.: 59.51%\n",
      "Epoch: 1545. Loss: 0.8039. Acc.: 59.69%\n",
      "Epoch: 1546. Loss: 0.8099. Acc.: 59.88%\n",
      "Epoch: 1547. Loss: 0.8024. Acc.: 60.39%\n",
      "Epoch: 1548. Loss: 0.8085. Acc.: 60.67%\n",
      "Epoch: 1549. Loss: 0.8076. Acc.: 58.35%\n",
      "Epoch: 1550. Loss: 0.8060. Acc.: 59.51%\n",
      "Epoch: 1551. Loss: 0.8031. Acc.: 59.69%\n",
      "Epoch: 1552. Loss: 0.7988. Acc.: 59.46%\n",
      "Epoch: 1553. Loss: 0.8135. Acc.: 59.37%\n",
      "Epoch: 1554. Loss: 0.8112. Acc.: 59.97%\n",
      "Epoch: 1555. Loss: 0.8167. Acc.: 60.34%\n",
      "Epoch: 1556. Loss: 0.8016. Acc.: 60.53%\n",
      "Epoch: 1557. Loss: 0.8130. Acc.: 59.88%\n",
      "Epoch: 1558. Loss: 0.8100. Acc.: 60.71%\n",
      "Epoch 1558 best model saved with accuracy: 60.71%\n",
      "Epoch: 1559. Loss: 0.8087. Acc.: 61.41%\n",
      "Epoch 1559 best model saved with accuracy: 61.41%\n",
      "Epoch: 1560. Loss: 0.8078. Acc.: 59.51%\n",
      "Epoch: 1561. Loss: 0.8004. Acc.: 59.74%\n",
      "Epoch: 1562. Loss: 0.8033. Acc.: 59.14%\n",
      "Epoch: 1563. Loss: 0.8044. Acc.: 59.74%\n",
      "Epoch: 1564. Loss: 0.8080. Acc.: 59.69%\n",
      "Epoch: 1565. Loss: 0.8014. Acc.: 59.74%\n",
      "Epoch: 1566. Loss: 0.8088. Acc.: 60.39%\n",
      "Epoch: 1567. Loss: 0.8012. Acc.: 59.60%\n",
      "Epoch: 1568. Loss: 0.8037. Acc.: 60.53%\n",
      "Epoch: 1569. Loss: 0.8069. Acc.: 60.06%\n",
      "Epoch: 1570. Loss: 0.8061. Acc.: 60.25%\n",
      "Epoch: 1571. Loss: 0.8028. Acc.: 59.79%\n",
      "Epoch: 1572. Loss: 0.8073. Acc.: 59.55%\n",
      "Epoch: 1573. Loss: 0.8019. Acc.: 59.93%\n",
      "Epoch: 1574. Loss: 0.8117. Acc.: 60.58%\n",
      "Epoch: 1575. Loss: 0.8061. Acc.: 60.25%\n",
      "Epoch: 1576. Loss: 0.8044. Acc.: 60.39%\n",
      "Epoch: 1577. Loss: 0.8065. Acc.: 59.09%\n",
      "Epoch: 1578. Loss: 0.8085. Acc.: 59.88%\n",
      "Epoch: 1579. Loss: 0.8043. Acc.: 58.91%\n",
      "Epoch: 1580. Loss: 0.8085. Acc.: 59.88%\n",
      "Epoch: 1581. Loss: 0.8125. Acc.: 59.46%\n",
      "Epoch: 1582. Loss: 0.8137. Acc.: 60.16%\n",
      "Epoch: 1583. Loss: 0.8013. Acc.: 59.97%\n",
      "Epoch: 1584. Loss: 0.8051. Acc.: 59.65%\n",
      "Epoch: 1585. Loss: 0.8079. Acc.: 60.58%\n",
      "Epoch: 1586. Loss: 0.8084. Acc.: 60.11%\n",
      "Epoch: 1587. Loss: 0.8023. Acc.: 60.58%\n",
      "Epoch: 1588. Loss: 0.7977. Acc.: 59.32%\n",
      "Epoch: 1589. Loss: 0.8047. Acc.: 60.90%\n",
      "Epoch: 1590. Loss: 0.8085. Acc.: 59.97%\n",
      "Epoch: 1591. Loss: 0.7979. Acc.: 60.34%\n",
      "Epoch: 1592. Loss: 0.8015. Acc.: 59.83%\n",
      "Epoch: 1593. Loss: 0.7974. Acc.: 60.39%\n",
      "Epoch: 1594. Loss: 0.8037. Acc.: 59.51%\n",
      "Epoch: 1595. Loss: 0.8074. Acc.: 59.23%\n",
      "Epoch: 1596. Loss: 0.8033. Acc.: 60.71%\n",
      "Epoch: 1597. Loss: 0.8026. Acc.: 60.53%\n",
      "Epoch: 1598. Loss: 0.7923. Acc.: 59.97%\n",
      "Epoch: 1599. Loss: 0.7969. Acc.: 60.58%\n",
      "Epoch: 1600. Loss: 0.7969. Acc.: 60.81%\n",
      "Epoch: 1601. Loss: 0.7957. Acc.: 60.58%\n",
      "Epoch: 1602. Loss: 0.8030. Acc.: 59.79%\n",
      "Epoch: 1603. Loss: 0.7985. Acc.: 60.39%\n",
      "Epoch: 1604. Loss: 0.7955. Acc.: 60.06%\n",
      "Epoch: 1605. Loss: 0.8023. Acc.: 59.79%\n",
      "Epoch: 1606. Loss: 0.7925. Acc.: 60.39%\n",
      "Epoch: 1607. Loss: 0.7970. Acc.: 60.20%\n",
      "Epoch: 1608. Loss: 0.7966. Acc.: 60.30%\n",
      "Epoch: 1609. Loss: 0.7967. Acc.: 60.39%\n",
      "Epoch: 1610. Loss: 0.7947. Acc.: 60.71%\n",
      "Epoch: 1611. Loss: 0.8016. Acc.: 59.74%\n",
      "Epoch: 1612. Loss: 0.8003. Acc.: 60.39%\n",
      "Epoch: 1613. Loss: 0.8068. Acc.: 59.79%\n",
      "Epoch: 1614. Loss: 0.7915. Acc.: 61.13%\n",
      "Epoch: 1615. Loss: 0.7956. Acc.: 59.74%\n",
      "Epoch: 1616. Loss: 0.7951. Acc.: 59.79%\n",
      "Epoch: 1617. Loss: 0.7986. Acc.: 60.85%\n",
      "Epoch: 1618. Loss: 0.7981. Acc.: 60.76%\n",
      "Epoch: 1619. Loss: 0.7943. Acc.: 60.44%\n",
      "Epoch: 1620. Loss: 0.7973. Acc.: 60.06%\n",
      "Epoch: 1621. Loss: 0.7991. Acc.: 61.04%\n",
      "Epoch: 1622. Loss: 0.8002. Acc.: 60.25%\n",
      "Epoch: 1623. Loss: 0.7976. Acc.: 60.30%\n",
      "Epoch: 1624. Loss: 0.7971. Acc.: 59.18%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1625. Loss: 0.7971. Acc.: 60.53%\n",
      "Epoch: 1626. Loss: 0.7944. Acc.: 59.51%\n",
      "Epoch: 1627. Loss: 0.7870. Acc.: 59.65%\n",
      "Epoch: 1628. Loss: 0.8036. Acc.: 59.46%\n",
      "Epoch: 1629. Loss: 0.8001. Acc.: 61.09%\n",
      "Epoch: 1630. Loss: 0.8068. Acc.: 59.65%\n",
      "Epoch: 1631. Loss: 0.7917. Acc.: 60.58%\n",
      "Epoch: 1632. Loss: 0.7960. Acc.: 61.27%\n",
      "Epoch: 1633. Loss: 0.8031. Acc.: 60.62%\n",
      "Epoch: 1634. Loss: 0.7965. Acc.: 59.65%\n",
      "Epoch: 1635. Loss: 0.8008. Acc.: 60.16%\n",
      "Epoch: 1636. Loss: 0.7959. Acc.: 62.06%\n",
      "Epoch 1636 best model saved with accuracy: 62.06%\n",
      "Epoch: 1637. Loss: 0.8002. Acc.: 60.67%\n",
      "Epoch: 1638. Loss: 0.7966. Acc.: 59.83%\n",
      "Epoch: 1639. Loss: 0.8040. Acc.: 58.81%\n",
      "Epoch: 1640. Loss: 0.7960. Acc.: 60.58%\n",
      "Epoch: 1641. Loss: 0.7960. Acc.: 60.48%\n",
      "Epoch: 1642. Loss: 0.7974. Acc.: 60.76%\n",
      "Epoch: 1643. Loss: 0.7948. Acc.: 61.04%\n",
      "Epoch: 1644. Loss: 0.7958. Acc.: 59.65%\n",
      "Epoch: 1645. Loss: 0.7949. Acc.: 59.69%\n",
      "Epoch: 1646. Loss: 0.7833. Acc.: 60.67%\n",
      "Epoch: 1647. Loss: 0.7974. Acc.: 59.46%\n",
      "Epoch: 1648. Loss: 0.7909. Acc.: 61.22%\n",
      "Epoch: 1649. Loss: 0.7849. Acc.: 60.71%\n",
      "Epoch: 1650. Loss: 0.7897. Acc.: 60.34%\n",
      "Epoch: 1651. Loss: 0.7865. Acc.: 59.69%\n",
      "Epoch: 1652. Loss: 0.7938. Acc.: 60.44%\n",
      "Epoch: 1653. Loss: 0.7893. Acc.: 60.53%\n",
      "Epoch: 1654. Loss: 0.7920. Acc.: 59.93%\n",
      "Epoch: 1655. Loss: 0.7877. Acc.: 60.53%\n",
      "Epoch: 1656. Loss: 0.7945. Acc.: 60.44%\n",
      "Epoch: 1657. Loss: 0.7928. Acc.: 60.30%\n",
      "Epoch: 1658. Loss: 0.7929. Acc.: 59.97%\n",
      "Epoch: 1659. Loss: 0.7914. Acc.: 58.95%\n",
      "Epoch: 1660. Loss: 0.7909. Acc.: 60.20%\n",
      "Epoch: 1661. Loss: 0.7835. Acc.: 60.25%\n",
      "Epoch: 1662. Loss: 0.7977. Acc.: 60.16%\n",
      "Epoch: 1663. Loss: 0.7932. Acc.: 59.79%\n",
      "Epoch: 1664. Loss: 0.7864. Acc.: 59.88%\n",
      "Epoch: 1665. Loss: 0.7908. Acc.: 61.36%\n",
      "Epoch: 1666. Loss: 0.7888. Acc.: 59.97%\n",
      "Epoch: 1667. Loss: 0.7866. Acc.: 60.95%\n",
      "Epoch: 1668. Loss: 0.7933. Acc.: 61.69%\n",
      "Epoch: 1669. Loss: 0.7884. Acc.: 60.53%\n",
      "Epoch: 1670. Loss: 0.7891. Acc.: 60.25%\n",
      "Epoch: 1671. Loss: 0.7896. Acc.: 60.58%\n",
      "Epoch: 1672. Loss: 0.7916. Acc.: 60.99%\n",
      "Epoch: 1673. Loss: 0.7902. Acc.: 60.16%\n",
      "Epoch: 1674. Loss: 0.7852. Acc.: 60.39%\n",
      "Epoch: 1675. Loss: 0.7858. Acc.: 60.62%\n",
      "Epoch: 1676. Loss: 0.7869. Acc.: 60.48%\n",
      "Epoch: 1677. Loss: 0.7811. Acc.: 60.67%\n",
      "Epoch: 1678. Loss: 0.7909. Acc.: 60.53%\n",
      "Epoch: 1679. Loss: 0.7757. Acc.: 60.48%\n",
      "Epoch: 1680. Loss: 0.7883. Acc.: 60.11%\n",
      "Epoch: 1681. Loss: 0.7859. Acc.: 60.11%\n",
      "Epoch: 1682. Loss: 0.7812. Acc.: 60.76%\n",
      "Epoch: 1683. Loss: 0.7858. Acc.: 60.11%\n",
      "Epoch: 1684. Loss: 0.7798. Acc.: 60.85%\n",
      "Epoch: 1685. Loss: 0.7881. Acc.: 60.06%\n",
      "Epoch: 1686. Loss: 0.7775. Acc.: 60.95%\n",
      "Epoch: 1687. Loss: 0.7783. Acc.: 59.97%\n",
      "Epoch: 1688. Loss: 0.7846. Acc.: 60.25%\n",
      "Epoch: 1689. Loss: 0.7865. Acc.: 60.11%\n",
      "Epoch: 1690. Loss: 0.7874. Acc.: 60.02%\n",
      "Epoch: 1691. Loss: 0.7817. Acc.: 60.85%\n",
      "Epoch: 1692. Loss: 0.7800. Acc.: 59.65%\n",
      "Epoch: 1693. Loss: 0.7882. Acc.: 60.58%\n",
      "Epoch: 1694. Loss: 0.7812. Acc.: 59.97%\n",
      "Epoch: 1695. Loss: 0.7765. Acc.: 60.71%\n",
      "Epoch: 1696. Loss: 0.7783. Acc.: 60.48%\n",
      "Epoch: 1697. Loss: 0.7773. Acc.: 60.44%\n",
      "Epoch: 1698. Loss: 0.7845. Acc.: 60.34%\n",
      "Epoch: 1699. Loss: 0.7876. Acc.: 60.62%\n",
      "Epoch: 1700. Loss: 0.7769. Acc.: 60.34%\n",
      "Epoch: 1701. Loss: 0.7818. Acc.: 60.48%\n",
      "Epoch: 1702. Loss: 0.7927. Acc.: 60.16%\n",
      "Epoch: 1703. Loss: 0.7882. Acc.: 60.81%\n",
      "Epoch: 1704. Loss: 0.7828. Acc.: 60.62%\n",
      "Epoch: 1705. Loss: 0.7832. Acc.: 60.06%\n",
      "Epoch: 1706. Loss: 0.7813. Acc.: 59.83%\n",
      "Epoch: 1707. Loss: 0.7716. Acc.: 60.95%\n",
      "Epoch: 1708. Loss: 0.7866. Acc.: 59.97%\n",
      "Epoch: 1709. Loss: 0.7839. Acc.: 60.58%\n",
      "Epoch: 1710. Loss: 0.7767. Acc.: 60.62%\n",
      "Epoch: 1711. Loss: 0.7857. Acc.: 61.41%\n",
      "Epoch: 1712. Loss: 0.7793. Acc.: 60.53%\n",
      "Epoch: 1713. Loss: 0.7850. Acc.: 59.97%\n",
      "Epoch: 1714. Loss: 0.7742. Acc.: 60.44%\n",
      "Epoch: 1715. Loss: 0.7734. Acc.: 61.27%\n",
      "Epoch: 1716. Loss: 0.7807. Acc.: 61.09%\n",
      "Epoch: 1717. Loss: 0.7793. Acc.: 61.60%\n",
      "Epoch: 1718. Loss: 0.7743. Acc.: 60.02%\n",
      "Epoch: 1719. Loss: 0.7832. Acc.: 60.81%\n",
      "Epoch: 1720. Loss: 0.7716. Acc.: 61.83%\n",
      "Epoch: 1721. Loss: 0.7737. Acc.: 60.30%\n",
      "Epoch: 1722. Loss: 0.7738. Acc.: 60.34%\n",
      "Epoch: 1723. Loss: 0.7776. Acc.: 61.18%\n",
      "Epoch: 1724. Loss: 0.7810. Acc.: 60.62%\n",
      "Epoch: 1725. Loss: 0.7648. Acc.: 61.32%\n",
      "Epoch: 1726. Loss: 0.7769. Acc.: 61.09%\n",
      "Epoch: 1727. Loss: 0.7819. Acc.: 60.95%\n",
      "Epoch: 1728. Loss: 0.7730. Acc.: 61.73%\n",
      "Epoch: 1729. Loss: 0.7710. Acc.: 60.71%\n",
      "Epoch: 1730. Loss: 0.7712. Acc.: 60.58%\n",
      "Epoch: 1731. Loss: 0.7738. Acc.: 60.53%\n",
      "Epoch: 1732. Loss: 0.7703. Acc.: 60.85%\n",
      "Epoch: 1733. Loss: 0.7756. Acc.: 61.27%\n",
      "Epoch: 1734. Loss: 0.7711. Acc.: 60.16%\n",
      "Epoch: 1735. Loss: 0.7739. Acc.: 60.90%\n",
      "Epoch: 1736. Loss: 0.7747. Acc.: 61.18%\n",
      "Epoch: 1737. Loss: 0.7774. Acc.: 60.58%\n",
      "Epoch: 1738. Loss: 0.7707. Acc.: 61.18%\n",
      "Epoch: 1739. Loss: 0.7759. Acc.: 61.13%\n",
      "Epoch: 1740. Loss: 0.7736. Acc.: 60.76%\n",
      "Epoch: 1741. Loss: 0.7646. Acc.: 60.67%\n",
      "Epoch: 1742. Loss: 0.7766. Acc.: 61.13%\n",
      "Epoch: 1743. Loss: 0.7716. Acc.: 61.36%\n",
      "Epoch: 1744. Loss: 0.7740. Acc.: 60.11%\n",
      "Epoch: 1745. Loss: 0.7686. Acc.: 60.25%\n",
      "Epoch: 1746. Loss: 0.7662. Acc.: 60.67%\n",
      "Epoch: 1748. Loss: 0.7703. Acc.: 60.71%\n",
      "Epoch: 1749. Loss: 0.7740. Acc.: 60.95%\n",
      "Epoch: 1750. Loss: 0.7653. Acc.: 61.13%\n",
      "Epoch: 1751. Loss: 0.7706. Acc.: 61.69%\n",
      "Epoch: 1752. Loss: 0.7749. Acc.: 61.92%\n",
      "Epoch: 1753. Loss: 0.7712. Acc.: 61.18%\n",
      "Epoch: 1754. Loss: 0.7673. Acc.: 61.22%\n",
      "Epoch: 1755. Loss: 0.7673. Acc.: 61.13%\n",
      "Epoch: 1756. Loss: 0.7765. Acc.: 61.13%\n",
      "Epoch: 1757. Loss: 0.7691. Acc.: 60.76%\n",
      "Epoch: 1758. Loss: 0.7693. Acc.: 61.64%\n",
      "Epoch: 1759. Loss: 0.7713. Acc.: 61.13%\n",
      "Epoch: 1760. Loss: 0.7685. Acc.: 61.60%\n",
      "Epoch: 1761. Loss: 0.7690. Acc.: 60.71%\n",
      "Epoch: 1762. Loss: 0.7654. Acc.: 61.04%\n",
      "Epoch: 1763. Loss: 0.7722. Acc.: 60.71%\n",
      "Epoch: 1764. Loss: 0.7735. Acc.: 61.09%\n",
      "Epoch: 1765. Loss: 0.7730. Acc.: 61.32%\n",
      "Epoch: 1766. Loss: 0.7689. Acc.: 60.58%\n",
      "Epoch: 1767. Loss: 0.7565. Acc.: 60.48%\n",
      "Epoch: 1768. Loss: 0.7694. Acc.: 61.83%\n",
      "Epoch: 1769. Loss: 0.7683. Acc.: 60.30%\n",
      "Epoch: 1770. Loss: 0.7690. Acc.: 61.18%\n",
      "Epoch: 1771. Loss: 0.7633. Acc.: 60.99%\n",
      "Epoch: 1772. Loss: 0.7623. Acc.: 61.55%\n",
      "Epoch: 1773. Loss: 0.7682. Acc.: 61.18%\n",
      "Epoch: 1774. Loss: 0.7587. Acc.: 60.85%\n",
      "Epoch: 1775. Loss: 0.7622. Acc.: 61.32%\n",
      "Epoch: 1776. Loss: 0.7706. Acc.: 60.81%\n",
      "Epoch: 1777. Loss: 0.7561. Acc.: 61.55%\n",
      "Epoch: 1778. Loss: 0.7693. Acc.: 60.53%\n",
      "Epoch: 1779. Loss: 0.7678. Acc.: 61.09%\n",
      "Epoch: 1780. Loss: 0.7704. Acc.: 60.95%\n",
      "Epoch: 1781. Loss: 0.7610. Acc.: 60.81%\n",
      "Epoch: 1782. Loss: 0.7684. Acc.: 60.30%\n",
      "Epoch: 1783. Loss: 0.7748. Acc.: 60.71%\n",
      "Epoch: 1784. Loss: 0.7599. Acc.: 60.48%\n",
      "Epoch: 1785. Loss: 0.7561. Acc.: 60.48%\n",
      "Epoch: 1786. Loss: 0.7639. Acc.: 60.30%\n",
      "Epoch: 1787. Loss: 0.7575. Acc.: 60.39%\n",
      "Epoch: 1788. Loss: 0.7657. Acc.: 60.30%\n",
      "Epoch: 1789. Loss: 0.7640. Acc.: 60.44%\n",
      "Epoch: 1790. Loss: 0.7569. Acc.: 59.74%\n",
      "Epoch: 1791. Loss: 0.7589. Acc.: 60.02%\n",
      "Epoch: 1792. Loss: 0.7615. Acc.: 60.62%\n",
      "Epoch: 1793. Loss: 0.7638. Acc.: 61.78%\n",
      "Epoch: 1794. Loss: 0.7656. Acc.: 60.81%\n",
      "Epoch: 1795. Loss: 0.7637. Acc.: 60.67%\n",
      "Epoch: 1796. Loss: 0.7554. Acc.: 60.81%\n",
      "Epoch: 1797. Loss: 0.7648. Acc.: 60.85%\n",
      "Epoch: 1798. Loss: 0.7537. Acc.: 61.18%\n",
      "Epoch: 1799. Loss: 0.7663. Acc.: 61.04%\n",
      "Epoch: 1800. Loss: 0.7526. Acc.: 60.76%\n",
      "Epoch: 1801. Loss: 0.7556. Acc.: 61.64%\n",
      "Epoch: 1802. Loss: 0.7514. Acc.: 60.99%\n",
      "Epoch: 1803. Loss: 0.7631. Acc.: 61.41%\n",
      "Epoch: 1804. Loss: 0.7506. Acc.: 61.32%\n",
      "Epoch: 1805. Loss: 0.7551. Acc.: 60.90%\n",
      "Epoch: 1806. Loss: 0.7552. Acc.: 61.32%\n",
      "Epoch: 1807. Loss: 0.7612. Acc.: 61.60%\n",
      "Epoch: 1808. Loss: 0.7567. Acc.: 60.90%\n",
      "Epoch: 1809. Loss: 0.7544. Acc.: 60.71%\n",
      "Epoch: 1810. Loss: 0.7521. Acc.: 60.90%\n",
      "Epoch: 1811. Loss: 0.7598. Acc.: 61.60%\n",
      "Epoch: 1812. Loss: 0.7536. Acc.: 60.95%\n",
      "Epoch: 1813. Loss: 0.7591. Acc.: 60.99%\n",
      "Epoch: 1814. Loss: 0.7584. Acc.: 60.62%\n",
      "Epoch: 1815. Loss: 0.7493. Acc.: 60.39%\n",
      "Epoch: 1816. Loss: 0.7619. Acc.: 60.67%\n",
      "Epoch: 1817. Loss: 0.7630. Acc.: 61.22%\n",
      "Epoch: 1818. Loss: 0.7642. Acc.: 61.09%\n",
      "Epoch: 1819. Loss: 0.7583. Acc.: 60.53%\n",
      "Epoch: 1820. Loss: 0.7648. Acc.: 61.27%\n",
      "Epoch: 1821. Loss: 0.7510. Acc.: 60.53%\n",
      "Epoch: 1822. Loss: 0.7474. Acc.: 60.48%\n",
      "Epoch: 1823. Loss: 0.7533. Acc.: 60.58%\n",
      "Epoch: 1824. Loss: 0.7564. Acc.: 60.95%\n",
      "Epoch: 1825. Loss: 0.7530. Acc.: 60.11%\n",
      "Epoch: 1826. Loss: 0.7591. Acc.: 60.99%\n",
      "Epoch: 1827. Loss: 0.7473. Acc.: 61.18%\n",
      "Epoch: 1828. Loss: 0.7537. Acc.: 60.62%\n",
      "Epoch: 1829. Loss: 0.7594. Acc.: 60.58%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1830. Loss: 0.7526. Acc.: 61.04%\n",
      "Epoch: 1831. Loss: 0.7561. Acc.: 60.76%\n",
      "Epoch: 1832. Loss: 0.7497. Acc.: 61.32%\n",
      "Epoch: 1833. Loss: 0.7548. Acc.: 60.67%\n",
      "Epoch: 1834. Loss: 0.7568. Acc.: 61.41%\n",
      "Epoch: 1835. Loss: 0.7550. Acc.: 61.50%\n",
      "Epoch: 1836. Loss: 0.7536. Acc.: 61.09%\n",
      "Epoch: 1837. Loss: 0.7524. Acc.: 60.71%\n",
      "Epoch: 1838. Loss: 0.7561. Acc.: 61.50%\n",
      "Epoch: 1839. Loss: 0.7578. Acc.: 60.58%\n",
      "Epoch: 1840. Loss: 0.7533. Acc.: 61.36%\n",
      "Epoch: 1841. Loss: 0.7487. Acc.: 61.18%\n",
      "Epoch: 1842. Loss: 0.7516. Acc.: 61.41%\n",
      "Epoch: 1843. Loss: 0.7513. Acc.: 61.36%\n",
      "Epoch: 1844. Loss: 0.7518. Acc.: 60.67%\n",
      "Epoch: 1845. Loss: 0.7430. Acc.: 60.85%\n",
      "Epoch: 1847. Loss: 0.7504. Acc.: 61.09%\n",
      "Epoch: 1848. Loss: 0.7538. Acc.: 60.81%\n",
      "Epoch: 1849. Loss: 0.7440. Acc.: 61.32%\n",
      "Epoch: 1850. Loss: 0.7477. Acc.: 60.58%\n",
      "Epoch: 1851. Loss: 0.7447. Acc.: 60.90%\n",
      "Epoch: 1852. Loss: 0.7585. Acc.: 61.18%\n",
      "Epoch: 1853. Loss: 0.7451. Acc.: 61.13%\n",
      "Epoch: 1854. Loss: 0.7428. Acc.: 60.99%\n",
      "Epoch: 1855. Loss: 0.7565. Acc.: 60.85%\n",
      "Epoch: 1856. Loss: 0.7470. Acc.: 60.99%\n",
      "Epoch: 1857. Loss: 0.7569. Acc.: 61.69%\n",
      "Epoch: 1858. Loss: 0.7560. Acc.: 61.22%\n",
      "Epoch: 1859. Loss: 0.7526. Acc.: 61.78%\n",
      "Epoch: 1860. Loss: 0.7463. Acc.: 60.90%\n",
      "Epoch: 1861. Loss: 0.7486. Acc.: 61.09%\n",
      "Epoch: 1862. Loss: 0.7467. Acc.: 61.04%\n",
      "Epoch: 1863. Loss: 0.7476. Acc.: 60.71%\n",
      "Epoch: 1864. Loss: 0.7487. Acc.: 61.18%\n",
      "Epoch: 1865. Loss: 0.7502. Acc.: 61.09%\n",
      "Epoch: 1866. Loss: 0.7436. Acc.: 60.71%\n",
      "Epoch: 1867. Loss: 0.7529. Acc.: 60.67%\n",
      "Epoch: 1868. Loss: 0.7396. Acc.: 60.90%\n",
      "Epoch: 1869. Loss: 0.7502. Acc.: 60.62%\n",
      "Epoch: 1870. Loss: 0.7371. Acc.: 61.09%\n",
      "Epoch: 1871. Loss: 0.7495. Acc.: 60.67%\n",
      "Epoch: 1872. Loss: 0.7460. Acc.: 60.39%\n",
      "Epoch: 1873. Loss: 0.7447. Acc.: 61.09%\n",
      "Epoch: 1874. Loss: 0.7448. Acc.: 60.95%\n",
      "Epoch: 1875. Loss: 0.7438. Acc.: 61.13%\n",
      "Epoch: 1876. Loss: 0.7390. Acc.: 61.27%\n",
      "Epoch: 1877. Loss: 0.7445. Acc.: 60.95%\n",
      "Epoch: 1878. Loss: 0.7404. Acc.: 60.76%\n",
      "Epoch: 1879. Loss: 0.7432. Acc.: 60.76%\n",
      "Epoch: 1880. Loss: 0.7451. Acc.: 60.99%\n",
      "Epoch: 1881. Loss: 0.7438. Acc.: 60.81%\n",
      "Epoch: 1882. Loss: 0.7360. Acc.: 60.95%\n",
      "Epoch: 1883. Loss: 0.7385. Acc.: 61.55%\n",
      "Epoch: 1884. Loss: 0.7491. Acc.: 61.32%\n",
      "Epoch: 1885. Loss: 0.7402. Acc.: 60.99%\n",
      "Epoch: 1886. Loss: 0.7422. Acc.: 60.53%\n",
      "Epoch: 1887. Loss: 0.7451. Acc.: 61.13%\n",
      "Epoch: 1888. Loss: 0.7417. Acc.: 60.71%\n",
      "Epoch: 1889. Loss: 0.7446. Acc.: 60.85%\n",
      "Epoch: 1890. Loss: 0.7458. Acc.: 61.46%\n",
      "Epoch: 1891. Loss: 0.7451. Acc.: 61.55%\n",
      "Epoch: 1892. Loss: 0.7446. Acc.: 60.90%\n",
      "Epoch: 1893. Loss: 0.7478. Acc.: 60.90%\n",
      "Epoch: 1894. Loss: 0.7447. Acc.: 60.53%\n",
      "Epoch: 1895. Loss: 0.7411. Acc.: 60.58%\n",
      "Epoch: 1896. Loss: 0.7393. Acc.: 60.58%\n",
      "Epoch: 1897. Loss: 0.7401. Acc.: 60.67%\n",
      "Epoch: 1898. Loss: 0.7431. Acc.: 60.53%\n",
      "Epoch: 1899. Loss: 0.7389. Acc.: 60.95%\n",
      "Epoch: 1900. Loss: 0.7317. Acc.: 60.85%\n",
      "Epoch: 1901. Loss: 0.7464. Acc.: 60.62%\n",
      "Epoch: 1902. Loss: 0.7323. Acc.: 61.09%\n",
      "Epoch: 1903. Loss: 0.7415. Acc.: 60.62%\n",
      "Epoch: 1904. Loss: 0.7465. Acc.: 60.95%\n",
      "Epoch: 1905. Loss: 0.7432. Acc.: 60.90%\n",
      "Epoch: 1906. Loss: 0.7418. Acc.: 60.81%\n",
      "Epoch: 1907. Loss: 0.7407. Acc.: 61.32%\n",
      "Epoch: 1908. Loss: 0.7363. Acc.: 61.13%\n",
      "Epoch: 1909. Loss: 0.7389. Acc.: 60.67%\n",
      "Epoch: 1910. Loss: 0.7423. Acc.: 60.67%\n",
      "Epoch: 1911. Loss: 0.7458. Acc.: 60.85%\n",
      "Epoch: 1912. Loss: 0.7474. Acc.: 60.99%\n",
      "Epoch: 1913. Loss: 0.7357. Acc.: 61.04%\n",
      "Epoch: 1914. Loss: 0.7357. Acc.: 61.36%\n",
      "Epoch: 1915. Loss: 0.7371. Acc.: 61.22%\n",
      "Epoch: 1916. Loss: 0.7367. Acc.: 61.04%\n",
      "Epoch: 1917. Loss: 0.7382. Acc.: 61.09%\n",
      "Epoch: 1918. Loss: 0.7368. Acc.: 61.04%\n",
      "Epoch: 1919. Loss: 0.7378. Acc.: 60.85%\n",
      "Epoch: 1920. Loss: 0.7455. Acc.: 60.81%\n",
      "Epoch: 1921. Loss: 0.7414. Acc.: 60.76%\n",
      "Epoch: 1922. Loss: 0.7383. Acc.: 60.62%\n",
      "Epoch: 1923. Loss: 0.7291. Acc.: 60.48%\n",
      "Epoch: 1924. Loss: 0.7399. Acc.: 60.99%\n",
      "Epoch: 1925. Loss: 0.7429. Acc.: 61.04%\n",
      "Epoch: 1926. Loss: 0.7332. Acc.: 60.81%\n",
      "Epoch: 1927. Loss: 0.7427. Acc.: 60.53%\n",
      "Epoch: 1928. Loss: 0.7348. Acc.: 60.48%\n",
      "Epoch: 1929. Loss: 0.7336. Acc.: 60.67%\n",
      "Epoch: 1930. Loss: 0.7356. Acc.: 60.76%\n",
      "Epoch: 1931. Loss: 0.7415. Acc.: 60.76%\n",
      "Epoch: 1932. Loss: 0.7363. Acc.: 60.85%\n",
      "Epoch: 1933. Loss: 0.7377. Acc.: 60.67%\n",
      "Epoch: 1934. Loss: 0.7491. Acc.: 60.67%\n",
      "Epoch: 1935. Loss: 0.7414. Acc.: 60.30%\n",
      "Epoch: 1936. Loss: 0.7386. Acc.: 60.76%\n",
      "Epoch: 1937. Loss: 0.7349. Acc.: 60.90%\n",
      "Epoch: 1938. Loss: 0.7408. Acc.: 60.62%\n",
      "Epoch: 1939. Loss: 0.7267. Acc.: 60.90%\n",
      "Epoch: 1940. Loss: 0.7360. Acc.: 60.95%\n",
      "Epoch: 1941. Loss: 0.7364. Acc.: 60.48%\n",
      "Epoch: 1942. Loss: 0.7439. Acc.: 60.67%\n",
      "Epoch: 1943. Loss: 0.7372. Acc.: 60.81%\n",
      "Epoch: 1944. Loss: 0.7429. Acc.: 60.71%\n",
      "Epoch: 1945. Loss: 0.7389. Acc.: 60.58%\n",
      "Epoch: 1946. Loss: 0.7348. Acc.: 60.48%\n",
      "Epoch: 1947. Loss: 0.7348. Acc.: 60.44%\n",
      "Epoch: 1948. Loss: 0.7352. Acc.: 60.62%\n",
      "Epoch: 1949. Loss: 0.7381. Acc.: 60.76%\n",
      "Epoch: 1950. Loss: 0.7440. Acc.: 60.95%\n",
      "Epoch: 1951. Loss: 0.7313. Acc.: 60.95%\n",
      "Epoch: 1952. Loss: 0.7362. Acc.: 60.81%\n",
      "Epoch: 1953. Loss: 0.7388. Acc.: 60.95%\n",
      "Epoch: 1954. Loss: 0.7372. Acc.: 60.71%\n",
      "Epoch: 1955. Loss: 0.7314. Acc.: 60.58%\n",
      "Epoch: 1956. Loss: 0.7378. Acc.: 60.53%\n",
      "Epoch: 1957. Loss: 0.7292. Acc.: 61.04%\n",
      "Epoch: 1958. Loss: 0.7401. Acc.: 60.99%\n",
      "Epoch: 1959. Loss: 0.7332. Acc.: 60.85%\n",
      "Epoch: 1960. Loss: 0.7418. Acc.: 60.85%\n",
      "Epoch: 1961. Loss: 0.7412. Acc.: 60.81%\n",
      "Epoch: 1962. Loss: 0.7355. Acc.: 60.90%\n",
      "Epoch: 1963. Loss: 0.7305. Acc.: 61.04%\n",
      "Epoch: 1964. Loss: 0.7346. Acc.: 60.71%\n",
      "Epoch: 1965. Loss: 0.7459. Acc.: 60.81%\n",
      "Epoch: 1966. Loss: 0.7323. Acc.: 60.71%\n",
      "Epoch: 1967. Loss: 0.7381. Acc.: 60.81%\n",
      "Epoch: 1968. Loss: 0.7456. Acc.: 60.76%\n",
      "Epoch: 1969. Loss: 0.7329. Acc.: 60.71%\n",
      "Epoch: 1970. Loss: 0.7360. Acc.: 60.71%\n",
      "Epoch: 1971. Loss: 0.7419. Acc.: 60.62%\n",
      "Epoch: 1972. Loss: 0.7319. Acc.: 60.76%\n",
      "Epoch: 1973. Loss: 0.7328. Acc.: 60.48%\n",
      "Epoch: 1974. Loss: 0.7412. Acc.: 60.58%\n",
      "Epoch: 1975. Loss: 0.7332. Acc.: 60.90%\n",
      "Epoch: 1976. Loss: 0.7340. Acc.: 60.76%\n",
      "Epoch: 1977. Loss: 0.7323. Acc.: 60.62%\n",
      "Epoch: 1978. Loss: 0.7378. Acc.: 60.48%\n",
      "Epoch: 1979. Loss: 0.7374. Acc.: 60.58%\n",
      "Epoch: 1980. Loss: 0.7344. Acc.: 60.44%\n",
      "Epoch: 1981. Loss: 0.7357. Acc.: 60.76%\n",
      "Epoch: 1982. Loss: 0.7411. Acc.: 60.62%\n",
      "Epoch: 1983. Loss: 0.7370. Acc.: 60.71%\n",
      "Epoch: 1984. Loss: 0.7344. Acc.: 60.67%\n",
      "Epoch: 1985. Loss: 0.7358. Acc.: 60.67%\n",
      "Epoch: 1986. Loss: 0.7329. Acc.: 60.53%\n",
      "Epoch: 1987. Loss: 0.7291. Acc.: 60.81%\n",
      "Epoch: 1988. Loss: 0.7376. Acc.: 60.76%\n",
      "Epoch: 1989. Loss: 0.7402. Acc.: 60.67%\n",
      "Epoch: 1990. Loss: 0.7365. Acc.: 60.81%\n",
      "Epoch: 1991. Loss: 0.7327. Acc.: 60.76%\n",
      "Epoch: 1992. Loss: 0.7284. Acc.: 60.62%\n",
      "Epoch: 1993. Loss: 0.7307. Acc.: 60.62%\n",
      "Epoch: 1994. Loss: 0.7310. Acc.: 60.85%\n",
      "Epoch: 1995. Loss: 0.7380. Acc.: 60.71%\n",
      "Epoch: 1996. Loss: 0.7391. Acc.: 60.76%\n",
      "Epoch: 1997. Loss: 0.7361. Acc.: 60.85%\n",
      "Epoch: 1998. Loss: 0.7324. Acc.: 60.85%\n",
      "Epoch: 1999. Loss: 0.7362. Acc.: 60.81%\n",
      "Epoch: 2000. Loss: 0.7339. Acc.: 60.67%\n",
      "Epoch: 2001. Loss: 0.7349. Acc.: 60.81%\n",
      "Epoch: 2002. Loss: 0.7347. Acc.: 60.53%\n",
      "Epoch: 2003. Loss: 0.7241. Acc.: 60.62%\n",
      "Epoch: 2004. Loss: 0.7359. Acc.: 60.76%\n",
      "Epoch: 2005. Loss: 0.7333. Acc.: 60.71%\n",
      "Epoch: 2006. Loss: 0.7340. Acc.: 60.58%\n",
      "Epoch: 2007. Loss: 0.7326. Acc.: 60.85%\n",
      "Epoch: 2008. Loss: 0.7354. Acc.: 60.67%\n",
      "Epoch: 2009. Loss: 0.7338. Acc.: 60.81%\n",
      "Epoch: 2010. Loss: 0.7339. Acc.: 60.81%\n",
      "Epoch: 2011. Loss: 0.7353. Acc.: 60.81%\n",
      "Epoch: 2012. Loss: 0.7381. Acc.: 60.85%\n",
      "Epoch: 2013. Loss: 0.7416. Acc.: 60.85%\n",
      "Epoch: 2014. Loss: 0.7303. Acc.: 60.81%\n",
      "Epoch: 2015. Loss: 0.7340. Acc.: 60.71%\n",
      "Epoch: 2016. Loss: 0.7303. Acc.: 60.71%\n",
      "Epoch: 2017. Loss: 0.7324. Acc.: 60.71%\n",
      "Epoch: 2018. Loss: 0.7349. Acc.: 60.67%\n",
      "Epoch: 2019. Loss: 0.7327. Acc.: 60.62%\n",
      "Epoch: 2020. Loss: 0.7353. Acc.: 60.76%\n",
      "Epoch: 2021. Loss: 0.7349. Acc.: 60.81%\n",
      "Epoch: 2022. Loss: 0.7231. Acc.: 60.81%\n",
      "Epoch: 2023. Loss: 0.7284. Acc.: 60.76%\n",
      "Epoch: 2024. Loss: 0.7306. Acc.: 60.85%\n",
      "Epoch: 2025. Loss: 0.7361. Acc.: 60.81%\n",
      "Epoch: 2026. Loss: 0.7327. Acc.: 60.81%\n",
      "Epoch: 2027. Loss: 0.7277. Acc.: 60.81%\n",
      "Epoch: 2028. Loss: 0.7367. Acc.: 60.85%\n",
      "Epoch: 2029. Loss: 0.7378. Acc.: 60.81%\n",
      "Epoch: 2030. Loss: 0.7332. Acc.: 60.85%\n",
      "Epoch: 2031. Loss: 0.7315. Acc.: 60.90%\n",
      "Epoch: 2032. Loss: 0.7350. Acc.: 60.90%\n",
      "Epoch: 2033. Loss: 0.7254. Acc.: 60.85%\n",
      "Epoch: 2034. Loss: 0.7369. Acc.: 60.81%\n",
      "Epoch: 2035. Loss: 0.7369. Acc.: 60.81%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2036. Loss: 0.7332. Acc.: 60.90%\n",
      "Epoch: 2037. Loss: 0.7395. Acc.: 60.85%\n",
      "Epoch: 2038. Loss: 0.7344. Acc.: 60.85%\n",
      "Epoch: 2039. Loss: 0.7328. Acc.: 60.81%\n",
      "Epoch: 2040. Loss: 0.7345. Acc.: 60.90%\n",
      "Epoch: 2041. Loss: 0.7376. Acc.: 60.90%\n",
      "Epoch: 2042. Loss: 0.7301. Acc.: 60.76%\n",
      "Epoch: 2043. Loss: 0.7295. Acc.: 60.85%\n",
      "Epoch: 2044. Loss: 0.7515. Acc.: 60.44%\n",
      "Epoch: 2045. Loss: 0.7590. Acc.: 60.99%\n",
      "Epoch: 2046. Loss: 0.7536. Acc.: 60.53%\n",
      "Epoch: 2047. Loss: 0.7647. Acc.: 60.71%\n",
      "Epoch: 2048. Loss: 0.7564. Acc.: 60.90%\n",
      "Epoch: 2049. Loss: 0.7636. Acc.: 60.16%\n",
      "Epoch: 2050. Loss: 0.7759. Acc.: 60.85%\n",
      "Epoch: 2051. Loss: 0.7835. Acc.: 59.65%\n",
      "Epoch: 2052. Loss: 0.7718. Acc.: 59.79%\n",
      "Epoch: 2053. Loss: 0.7711. Acc.: 60.67%\n",
      "Epoch: 2054. Loss: 0.7786. Acc.: 60.44%\n",
      "Epoch: 2055. Loss: 0.7753. Acc.: 60.34%\n",
      "Epoch: 2056. Loss: 0.7656. Acc.: 60.53%\n",
      "Epoch: 2057. Loss: 0.7734. Acc.: 60.62%\n",
      "Epoch: 2058. Loss: 0.7749. Acc.: 61.36%\n",
      "Epoch: 2059. Loss: 0.7724. Acc.: 61.69%\n",
      "Epoch: 2060. Loss: 0.7568. Acc.: 61.13%\n",
      "Epoch: 2061. Loss: 0.7620. Acc.: 61.27%\n",
      "Epoch: 2062. Loss: 0.7655. Acc.: 61.64%\n",
      "Epoch: 2063. Loss: 0.7615. Acc.: 60.76%\n",
      "Epoch: 2064. Loss: 0.7581. Acc.: 61.22%\n",
      "Epoch: 2065. Loss: 0.7647. Acc.: 62.52%\n",
      "Epoch 2065 best model saved with accuracy: 62.52%\n",
      "Epoch: 2066. Loss: 0.7628. Acc.: 60.44%\n",
      "Epoch: 2067. Loss: 0.7668. Acc.: 60.81%\n",
      "Epoch: 2068. Loss: 0.7640. Acc.: 60.30%\n",
      "Epoch: 2069. Loss: 0.7610. Acc.: 60.39%\n",
      "Epoch: 2070. Loss: 0.7695. Acc.: 61.97%\n",
      "Epoch: 2071. Loss: 0.7689. Acc.: 62.38%\n",
      "Epoch: 2072. Loss: 0.7628. Acc.: 61.09%\n",
      "Epoch: 2073. Loss: 0.7676. Acc.: 61.46%\n",
      "Epoch: 2074. Loss: 0.7733. Acc.: 60.95%\n",
      "Epoch: 2075. Loss: 0.7686. Acc.: 60.53%\n",
      "Epoch: 2076. Loss: 0.7688. Acc.: 59.88%\n",
      "Epoch: 2077. Loss: 0.7756. Acc.: 60.99%\n",
      "Epoch: 2078. Loss: 0.7613. Acc.: 59.60%\n",
      "Epoch: 2079. Loss: 0.7747. Acc.: 60.81%\n",
      "Epoch: 2080. Loss: 0.7697. Acc.: 61.13%\n",
      "Epoch: 2081. Loss: 0.7699. Acc.: 60.34%\n",
      "Epoch: 2082. Loss: 0.7702. Acc.: 60.16%\n",
      "Epoch: 2083. Loss: 0.7687. Acc.: 60.81%\n",
      "Epoch: 2084. Loss: 0.7721. Acc.: 61.27%\n",
      "Epoch: 2085. Loss: 0.7681. Acc.: 61.04%\n",
      "Epoch: 2086. Loss: 0.7636. Acc.: 60.25%\n",
      "Epoch: 2087. Loss: 0.7655. Acc.: 60.39%\n",
      "Epoch: 2088. Loss: 0.7710. Acc.: 60.34%\n",
      "Epoch: 2089. Loss: 0.7705. Acc.: 60.30%\n",
      "Epoch: 2090. Loss: 0.7655. Acc.: 60.02%\n",
      "Epoch: 2091. Loss: 0.7658. Acc.: 60.67%\n",
      "Epoch: 2092. Loss: 0.7682. Acc.: 61.18%\n",
      "Epoch: 2093. Loss: 0.7615. Acc.: 60.99%\n",
      "Epoch: 2094. Loss: 0.7684. Acc.: 60.53%\n",
      "Epoch: 2095. Loss: 0.7644. Acc.: 60.02%\n",
      "Epoch: 2096. Loss: 0.7687. Acc.: 60.62%\n",
      "Epoch: 2097. Loss: 0.7600. Acc.: 61.18%\n",
      "Epoch: 2098. Loss: 0.7646. Acc.: 60.53%\n",
      "Epoch: 2099. Loss: 0.7686. Acc.: 61.64%\n",
      "Epoch: 2100. Loss: 0.7620. Acc.: 61.83%\n",
      "Epoch: 2101. Loss: 0.7730. Acc.: 60.76%\n",
      "Epoch: 2102. Loss: 0.7707. Acc.: 60.30%\n",
      "Epoch: 2103. Loss: 0.7756. Acc.: 59.93%\n",
      "Epoch: 2104. Loss: 0.7697. Acc.: 61.55%\n",
      "Epoch: 2105. Loss: 0.7732. Acc.: 60.30%\n",
      "Epoch: 2106. Loss: 0.7700. Acc.: 61.41%\n",
      "Epoch: 2107. Loss: 0.7714. Acc.: 61.32%\n",
      "Epoch: 2108. Loss: 0.7620. Acc.: 60.39%\n",
      "Epoch: 2109. Loss: 0.7667. Acc.: 61.22%\n",
      "Epoch: 2110. Loss: 0.7722. Acc.: 60.85%\n",
      "Epoch: 2111. Loss: 0.7643. Acc.: 60.30%\n",
      "Epoch: 2112. Loss: 0.7643. Acc.: 60.16%\n",
      "Epoch: 2113. Loss: 0.7693. Acc.: 60.20%\n",
      "Epoch: 2114. Loss: 0.7670. Acc.: 61.50%\n",
      "Epoch: 2115. Loss: 0.7666. Acc.: 60.71%\n",
      "Epoch: 2116. Loss: 0.7696. Acc.: 60.71%\n",
      "Epoch: 2117. Loss: 0.7581. Acc.: 60.99%\n",
      "Epoch: 2118. Loss: 0.7660. Acc.: 60.34%\n",
      "Epoch: 2119. Loss: 0.7753. Acc.: 60.16%\n",
      "Epoch: 2120. Loss: 0.7652. Acc.: 60.48%\n",
      "Epoch: 2121. Loss: 0.7679. Acc.: 60.20%\n",
      "Epoch: 2122. Loss: 0.7660. Acc.: 61.50%\n",
      "Epoch: 2123. Loss: 0.7565. Acc.: 61.18%\n",
      "Epoch: 2124. Loss: 0.7709. Acc.: 61.41%\n",
      "Epoch: 2125. Loss: 0.7600. Acc.: 60.58%\n",
      "Epoch: 2126. Loss: 0.7647. Acc.: 60.11%\n",
      "Epoch: 2128. Loss: 0.7687. Acc.: 60.44%\n",
      "Epoch: 2129. Loss: 0.7611. Acc.: 60.90%\n",
      "Epoch: 2130. Loss: 0.7628. Acc.: 60.02%\n",
      "Epoch: 2131. Loss: 0.7595. Acc.: 60.53%\n",
      "Epoch: 2132. Loss: 0.7604. Acc.: 60.99%\n",
      "Epoch: 2133. Loss: 0.7616. Acc.: 60.20%\n",
      "Epoch: 2134. Loss: 0.7524. Acc.: 60.44%\n",
      "Epoch: 2135. Loss: 0.7626. Acc.: 61.60%\n",
      "Epoch: 2136. Loss: 0.7643. Acc.: 61.22%\n",
      "Epoch: 2137. Loss: 0.7649. Acc.: 60.02%\n",
      "Epoch: 2138. Loss: 0.7670. Acc.: 60.76%\n",
      "Epoch: 2139. Loss: 0.7580. Acc.: 61.13%\n",
      "Epoch: 2140. Loss: 0.7587. Acc.: 61.04%\n",
      "Epoch: 2141. Loss: 0.7608. Acc.: 60.81%\n",
      "Epoch: 2142. Loss: 0.7579. Acc.: 60.30%\n",
      "Epoch: 2143. Loss: 0.7637. Acc.: 60.58%\n",
      "Epoch: 2144. Loss: 0.7582. Acc.: 60.44%\n",
      "Epoch: 2145. Loss: 0.7580. Acc.: 60.99%\n",
      "Epoch: 2146. Loss: 0.7607. Acc.: 61.13%\n",
      "Epoch: 2147. Loss: 0.7658. Acc.: 60.25%\n",
      "Epoch: 2148. Loss: 0.7602. Acc.: 61.18%\n",
      "Epoch: 2149. Loss: 0.7573. Acc.: 60.76%\n",
      "Epoch: 2150. Loss: 0.7520. Acc.: 60.58%\n",
      "Epoch: 2151. Loss: 0.7627. Acc.: 60.85%\n",
      "Epoch: 2152. Loss: 0.7523. Acc.: 61.78%\n",
      "Epoch: 2153. Loss: 0.7521. Acc.: 59.97%\n",
      "Epoch: 2154. Loss: 0.7629. Acc.: 60.20%\n",
      "Epoch: 2155. Loss: 0.7551. Acc.: 61.09%\n",
      "Epoch: 2156. Loss: 0.7517. Acc.: 61.32%\n",
      "Epoch: 2157. Loss: 0.7559. Acc.: 60.06%\n",
      "Epoch: 2158. Loss: 0.7574. Acc.: 60.53%\n",
      "Epoch: 2159. Loss: 0.7593. Acc.: 59.93%\n",
      "Epoch: 2160. Loss: 0.7538. Acc.: 60.85%\n",
      "Epoch: 2161. Loss: 0.7572. Acc.: 61.64%\n",
      "Epoch: 2162. Loss: 0.7555. Acc.: 60.81%\n",
      "Epoch: 2163. Loss: 0.7571. Acc.: 60.48%\n",
      "Epoch: 2164. Loss: 0.7635. Acc.: 61.09%\n",
      "Epoch: 2165. Loss: 0.7482. Acc.: 60.62%\n",
      "Epoch: 2166. Loss: 0.7682. Acc.: 61.64%\n",
      "Epoch: 2167. Loss: 0.7598. Acc.: 60.85%\n",
      "Epoch: 2168. Loss: 0.7660. Acc.: 61.13%\n",
      "Epoch: 2169. Loss: 0.7567. Acc.: 60.71%\n",
      "Epoch: 2170. Loss: 0.7638. Acc.: 60.85%\n",
      "Epoch: 2171. Loss: 0.7516. Acc.: 60.53%\n",
      "Epoch: 2172. Loss: 0.7516. Acc.: 60.81%\n",
      "Epoch: 2173. Loss: 0.7575. Acc.: 61.46%\n",
      "Epoch: 2174. Loss: 0.7539. Acc.: 59.79%\n",
      "Epoch: 2175. Loss: 0.7532. Acc.: 61.18%\n",
      "Epoch: 2176. Loss: 0.7589. Acc.: 61.22%\n",
      "Epoch: 2177. Loss: 0.7621. Acc.: 61.13%\n",
      "Epoch: 2178. Loss: 0.7570. Acc.: 60.99%\n",
      "Epoch: 2179. Loss: 0.7536. Acc.: 59.79%\n",
      "Epoch: 2180. Loss: 0.7568. Acc.: 59.97%\n",
      "Epoch: 2181. Loss: 0.7548. Acc.: 60.95%\n",
      "Epoch: 2182. Loss: 0.7560. Acc.: 60.90%\n",
      "Epoch: 2183. Loss: 0.7515. Acc.: 60.34%\n",
      "Epoch: 2184. Loss: 0.7540. Acc.: 60.16%\n",
      "Epoch: 2185. Loss: 0.7546. Acc.: 60.58%\n",
      "Epoch: 2186. Loss: 0.7555. Acc.: 59.69%\n",
      "Epoch: 2187. Loss: 0.7617. Acc.: 60.99%\n",
      "Epoch: 2188. Loss: 0.7563. Acc.: 61.18%\n",
      "Epoch: 2189. Loss: 0.7487. Acc.: 60.53%\n",
      "Epoch: 2190. Loss: 0.7580. Acc.: 59.97%\n",
      "Epoch: 2191. Loss: 0.7587. Acc.: 59.97%\n",
      "Epoch: 2192. Loss: 0.7489. Acc.: 59.79%\n",
      "Epoch: 2193. Loss: 0.7628. Acc.: 60.62%\n",
      "Epoch: 2194. Loss: 0.7446. Acc.: 60.90%\n",
      "Epoch: 2195. Loss: 0.7564. Acc.: 60.53%\n",
      "Epoch: 2196. Loss: 0.7607. Acc.: 59.60%\n",
      "Epoch: 2197. Loss: 0.7571. Acc.: 60.11%\n",
      "Epoch: 2198. Loss: 0.7511. Acc.: 59.00%\n",
      "Epoch: 2199. Loss: 0.7577. Acc.: 59.79%\n",
      "Epoch: 2200. Loss: 0.7421. Acc.: 59.69%\n",
      "Epoch: 2201. Loss: 0.7638. Acc.: 60.90%\n",
      "Epoch: 2202. Loss: 0.7568. Acc.: 60.20%\n",
      "Epoch: 2203. Loss: 0.7506. Acc.: 60.90%\n",
      "Epoch: 2204. Loss: 0.7519. Acc.: 59.97%\n",
      "Epoch: 2205. Loss: 0.7563. Acc.: 60.53%\n",
      "Epoch: 2206. Loss: 0.7514. Acc.: 59.88%\n",
      "Epoch: 2207. Loss: 0.7509. Acc.: 61.50%\n",
      "Epoch: 2208. Loss: 0.7445. Acc.: 60.20%\n",
      "Epoch: 2209. Loss: 0.7471. Acc.: 61.13%\n",
      "Epoch: 2210. Loss: 0.7565. Acc.: 60.90%\n",
      "Epoch: 2211. Loss: 0.7465. Acc.: 60.90%\n",
      "Epoch: 2212. Loss: 0.7437. Acc.: 62.06%\n",
      "Epoch: 2213. Loss: 0.7548. Acc.: 61.36%\n",
      "Epoch: 2214. Loss: 0.7486. Acc.: 60.62%\n",
      "Epoch: 2215. Loss: 0.7553. Acc.: 61.73%\n",
      "Epoch: 2216. Loss: 0.7417. Acc.: 61.04%\n",
      "Epoch: 2217. Loss: 0.7440. Acc.: 60.71%\n",
      "Epoch: 2218. Loss: 0.7513. Acc.: 60.67%\n",
      "Epoch: 2219. Loss: 0.7420. Acc.: 61.55%\n",
      "Epoch: 2220. Loss: 0.7526. Acc.: 60.99%\n",
      "Epoch: 2221. Loss: 0.7509. Acc.: 60.99%\n",
      "Epoch: 2222. Loss: 0.7512. Acc.: 60.58%\n",
      "Epoch: 2223. Loss: 0.7485. Acc.: 60.53%\n",
      "Epoch: 2224. Loss: 0.7477. Acc.: 60.16%\n",
      "Epoch: 2225. Loss: 0.7456. Acc.: 61.87%\n",
      "Epoch: 2226. Loss: 0.7495. Acc.: 60.48%\n",
      "Epoch: 2227. Loss: 0.7523. Acc.: 60.16%\n",
      "Epoch: 2228. Loss: 0.7507. Acc.: 60.85%\n",
      "Epoch: 2229. Loss: 0.7529. Acc.: 61.04%\n",
      "Epoch: 2230. Loss: 0.7521. Acc.: 61.22%\n",
      "Epoch: 2231. Loss: 0.7498. Acc.: 60.53%\n",
      "Epoch: 2232. Loss: 0.7464. Acc.: 59.93%\n",
      "Epoch: 2233. Loss: 0.7501. Acc.: 60.99%\n",
      "Epoch: 2234. Loss: 0.7493. Acc.: 60.71%\n",
      "Epoch: 2235. Loss: 0.7489. Acc.: 59.79%\n",
      "Epoch: 2236. Loss: 0.7449. Acc.: 60.39%\n",
      "Epoch: 2237. Loss: 0.7484. Acc.: 60.85%\n",
      "Epoch: 2238. Loss: 0.7419. Acc.: 61.13%\n",
      "Epoch: 2239. Loss: 0.7386. Acc.: 60.71%\n",
      "Epoch: 2240. Loss: 0.7452. Acc.: 59.65%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2241. Loss: 0.7481. Acc.: 59.65%\n",
      "Epoch: 2242. Loss: 0.7437. Acc.: 61.55%\n",
      "Epoch: 2243. Loss: 0.7495. Acc.: 60.34%\n",
      "Epoch: 2244. Loss: 0.7345. Acc.: 60.44%\n",
      "Epoch: 2245. Loss: 0.7453. Acc.: 60.39%\n",
      "Epoch: 2246. Loss: 0.7395. Acc.: 60.85%\n",
      "Epoch: 2247. Loss: 0.7521. Acc.: 62.01%\n",
      "Epoch: 2248. Loss: 0.7468. Acc.: 60.81%\n",
      "Epoch: 2249. Loss: 0.7466. Acc.: 60.99%\n",
      "Epoch: 2250. Loss: 0.7467. Acc.: 61.09%\n",
      "Epoch: 2251. Loss: 0.7464. Acc.: 60.34%\n",
      "Epoch: 2252. Loss: 0.7479. Acc.: 61.22%\n",
      "Epoch: 2253. Loss: 0.7456. Acc.: 62.06%\n",
      "Epoch: 2254. Loss: 0.7419. Acc.: 60.76%\n",
      "Epoch: 2255. Loss: 0.7421. Acc.: 60.76%\n",
      "Epoch: 2256. Loss: 0.7453. Acc.: 60.81%\n",
      "Epoch: 2257. Loss: 0.7316. Acc.: 60.99%\n",
      "Epoch: 2258. Loss: 0.7411. Acc.: 60.48%\n",
      "Epoch: 2259. Loss: 0.7475. Acc.: 59.97%\n",
      "Epoch: 2260. Loss: 0.7387. Acc.: 61.50%\n",
      "Epoch: 2261. Loss: 0.7415. Acc.: 60.06%\n",
      "Epoch: 2262. Loss: 0.7418. Acc.: 60.44%\n",
      "Epoch: 2263. Loss: 0.7439. Acc.: 61.50%\n",
      "Epoch: 2264. Loss: 0.7368. Acc.: 61.09%\n",
      "Epoch: 2265. Loss: 0.7301. Acc.: 61.04%\n",
      "Epoch: 2266. Loss: 0.7407. Acc.: 60.81%\n",
      "Epoch: 2267. Loss: 0.7362. Acc.: 60.30%\n",
      "Epoch: 2268. Loss: 0.7426. Acc.: 61.97%\n",
      "Epoch: 2269. Loss: 0.7386. Acc.: 60.99%\n",
      "Epoch: 2270. Loss: 0.7372. Acc.: 61.32%\n",
      "Epoch: 2271. Loss: 0.7470. Acc.: 60.39%\n",
      "Epoch: 2272. Loss: 0.7461. Acc.: 60.53%\n",
      "Epoch: 2273. Loss: 0.7383. Acc.: 60.81%\n",
      "Epoch: 2274. Loss: 0.7390. Acc.: 60.71%\n",
      "Epoch: 2275. Loss: 0.7451. Acc.: 61.09%\n",
      "Epoch: 2276. Loss: 0.7411. Acc.: 60.76%\n",
      "Epoch: 2277. Loss: 0.7313. Acc.: 60.76%\n",
      "Epoch: 2278. Loss: 0.7311. Acc.: 60.48%\n",
      "Epoch: 2279. Loss: 0.7332. Acc.: 60.30%\n",
      "Epoch: 2280. Loss: 0.7287. Acc.: 61.09%\n",
      "Epoch: 2281. Loss: 0.7374. Acc.: 61.04%\n",
      "Epoch: 2282. Loss: 0.7333. Acc.: 60.62%\n",
      "Epoch: 2283. Loss: 0.7401. Acc.: 60.11%\n",
      "Epoch: 2284. Loss: 0.7385. Acc.: 60.62%\n",
      "Epoch: 2285. Loss: 0.7345. Acc.: 60.76%\n",
      "Epoch: 2286. Loss: 0.7395. Acc.: 60.81%\n",
      "Epoch: 2287. Loss: 0.7338. Acc.: 60.39%\n",
      "Epoch: 2288. Loss: 0.7278. Acc.: 60.06%\n",
      "Epoch: 2289. Loss: 0.7287. Acc.: 61.13%\n",
      "Epoch: 2290. Loss: 0.7334. Acc.: 60.71%\n",
      "Epoch: 2291. Loss: 0.7370. Acc.: 60.06%\n",
      "Epoch: 2292. Loss: 0.7324. Acc.: 60.02%\n",
      "Epoch: 2293. Loss: 0.7398. Acc.: 60.25%\n",
      "Epoch: 2294. Loss: 0.7372. Acc.: 60.99%\n",
      "Epoch: 2295. Loss: 0.7334. Acc.: 60.34%\n",
      "Epoch: 2296. Loss: 0.7299. Acc.: 61.55%\n",
      "Epoch: 2297. Loss: 0.7372. Acc.: 61.22%\n",
      "Epoch: 2298. Loss: 0.7359. Acc.: 60.06%\n",
      "Epoch: 2299. Loss: 0.7351. Acc.: 60.11%\n",
      "Epoch: 2300. Loss: 0.7300. Acc.: 60.02%\n",
      "Epoch: 2301. Loss: 0.7318. Acc.: 60.48%\n",
      "Epoch: 2302. Loss: 0.7398. Acc.: 60.62%\n",
      "Epoch: 2303. Loss: 0.7315. Acc.: 60.53%\n",
      "Epoch: 2304. Loss: 0.7315. Acc.: 60.34%\n",
      "Epoch: 2305. Loss: 0.7275. Acc.: 60.58%\n",
      "Epoch: 2306. Loss: 0.7375. Acc.: 61.64%\n",
      "Epoch: 2307. Loss: 0.7241. Acc.: 61.55%\n",
      "Epoch: 2308. Loss: 0.7297. Acc.: 60.48%\n",
      "Epoch: 2309. Loss: 0.7333. Acc.: 60.44%\n",
      "Epoch: 2310. Loss: 0.7337. Acc.: 61.87%\n",
      "Epoch: 2311. Loss: 0.7408. Acc.: 60.81%\n",
      "Epoch: 2312. Loss: 0.7245. Acc.: 60.20%\n",
      "Epoch: 2313. Loss: 0.7284. Acc.: 60.67%\n",
      "Epoch: 2314. Loss: 0.7340. Acc.: 60.02%\n",
      "Epoch: 2315. Loss: 0.7219. Acc.: 60.67%\n",
      "Epoch: 2316. Loss: 0.7255. Acc.: 60.81%\n",
      "Epoch: 2317. Loss: 0.7301. Acc.: 60.44%\n",
      "Epoch: 2318. Loss: 0.7315. Acc.: 61.04%\n",
      "Epoch: 2319. Loss: 0.7302. Acc.: 60.85%\n",
      "Epoch: 2320. Loss: 0.7253. Acc.: 60.16%\n",
      "Epoch: 2321. Loss: 0.7322. Acc.: 61.64%\n",
      "Epoch: 2322. Loss: 0.7261. Acc.: 61.04%\n",
      "Epoch: 2323. Loss: 0.7323. Acc.: 60.02%\n",
      "Epoch: 2324. Loss: 0.7325. Acc.: 60.76%\n",
      "Epoch: 2325. Loss: 0.7342. Acc.: 60.95%\n",
      "Epoch: 2326. Loss: 0.7313. Acc.: 60.85%\n",
      "Epoch: 2327. Loss: 0.7314. Acc.: 61.18%\n",
      "Epoch: 2328. Loss: 0.7294. Acc.: 60.53%\n",
      "Epoch: 2329. Loss: 0.7245. Acc.: 61.04%\n",
      "Epoch: 2330. Loss: 0.7253. Acc.: 59.79%\n",
      "Epoch: 2331. Loss: 0.7284. Acc.: 61.36%\n",
      "Epoch: 2332. Loss: 0.7293. Acc.: 60.39%\n",
      "Epoch: 2333. Loss: 0.7306. Acc.: 60.39%\n",
      "Epoch: 2334. Loss: 0.7312. Acc.: 60.90%\n",
      "Epoch: 2335. Loss: 0.7371. Acc.: 59.83%\n",
      "Epoch: 2336. Loss: 0.7220. Acc.: 60.30%\n",
      "Epoch: 2337. Loss: 0.7306. Acc.: 60.67%\n",
      "Epoch: 2338. Loss: 0.7282. Acc.: 60.99%\n",
      "Epoch: 2339. Loss: 0.7298. Acc.: 59.88%\n",
      "Epoch: 2340. Loss: 0.7254. Acc.: 61.55%\n",
      "Epoch: 2341. Loss: 0.7335. Acc.: 60.95%\n",
      "Epoch: 2342. Loss: 0.7260. Acc.: 60.34%\n",
      "Epoch: 2343. Loss: 0.7205. Acc.: 61.27%\n",
      "Epoch: 2344. Loss: 0.7311. Acc.: 60.58%\n",
      "Epoch: 2345. Loss: 0.7261. Acc.: 60.67%\n",
      "Epoch: 2346. Loss: 0.7264. Acc.: 59.74%\n",
      "Epoch: 2347. Loss: 0.7237. Acc.: 60.62%\n",
      "Epoch: 2348. Loss: 0.7174. Acc.: 61.32%\n",
      "Epoch: 2349. Loss: 0.7238. Acc.: 61.13%\n",
      "Epoch: 2350. Loss: 0.7299. Acc.: 60.39%\n",
      "Epoch: 2351. Loss: 0.7266. Acc.: 60.02%\n",
      "Epoch: 2352. Loss: 0.7271. Acc.: 60.85%\n",
      "Epoch: 2353. Loss: 0.7251. Acc.: 59.88%\n",
      "Epoch: 2354. Loss: 0.7200. Acc.: 60.58%\n",
      "Epoch: 2355. Loss: 0.7229. Acc.: 60.85%\n",
      "Epoch: 2356. Loss: 0.7243. Acc.: 60.81%\n",
      "Epoch: 2357. Loss: 0.7259. Acc.: 61.18%\n",
      "Epoch: 2358. Loss: 0.7207. Acc.: 60.67%\n",
      "Epoch: 2359. Loss: 0.7209. Acc.: 60.16%\n",
      "Epoch: 2360. Loss: 0.7218. Acc.: 60.39%\n",
      "Epoch: 2361. Loss: 0.7215. Acc.: 60.95%\n",
      "Epoch: 2362. Loss: 0.7205. Acc.: 61.64%\n",
      "Epoch: 2363. Loss: 0.7250. Acc.: 60.85%\n",
      "Epoch: 2364. Loss: 0.7259. Acc.: 60.71%\n",
      "Epoch: 2365. Loss: 0.7205. Acc.: 60.62%\n",
      "Epoch: 2366. Loss: 0.7280. Acc.: 60.34%\n",
      "Epoch: 2367. Loss: 0.7213. Acc.: 60.99%\n",
      "Epoch: 2368. Loss: 0.7196. Acc.: 60.81%\n",
      "Epoch: 2369. Loss: 0.7217. Acc.: 60.16%\n",
      "Epoch: 2370. Loss: 0.7216. Acc.: 60.90%\n",
      "Epoch: 2371. Loss: 0.7166. Acc.: 61.13%\n",
      "Epoch: 2372. Loss: 0.7260. Acc.: 60.62%\n",
      "Epoch: 2373. Loss: 0.7222. Acc.: 61.04%\n",
      "Epoch: 2374. Loss: 0.7177. Acc.: 60.85%\n",
      "Epoch: 2375. Loss: 0.7233. Acc.: 61.36%\n",
      "Epoch: 2376. Loss: 0.7186. Acc.: 60.53%\n",
      "Epoch: 2377. Loss: 0.7175. Acc.: 60.44%\n",
      "Epoch: 2378. Loss: 0.7211. Acc.: 60.81%\n",
      "Epoch: 2379. Loss: 0.7259. Acc.: 60.62%\n",
      "Epoch: 2380. Loss: 0.7160. Acc.: 61.04%\n",
      "Epoch: 2381. Loss: 0.7205. Acc.: 60.99%\n",
      "Epoch: 2382. Loss: 0.7141. Acc.: 61.09%\n",
      "Epoch: 2383. Loss: 0.7287. Acc.: 61.50%\n",
      "Epoch: 2384. Loss: 0.7139. Acc.: 60.62%\n",
      "Epoch: 2385. Loss: 0.7225. Acc.: 59.88%\n",
      "Epoch: 2386. Loss: 0.7201. Acc.: 60.39%\n",
      "Epoch: 2387. Loss: 0.7248. Acc.: 60.53%\n",
      "Epoch: 2388. Loss: 0.7240. Acc.: 60.11%\n",
      "Epoch: 2389. Loss: 0.7164. Acc.: 61.04%\n",
      "Epoch: 2390. Loss: 0.7199. Acc.: 60.16%\n",
      "Epoch: 2391. Loss: 0.7163. Acc.: 59.93%\n",
      "Epoch: 2392. Loss: 0.7191. Acc.: 60.48%\n",
      "Epoch: 2393. Loss: 0.7150. Acc.: 60.58%\n",
      "Epoch: 2394. Loss: 0.7132. Acc.: 60.85%\n",
      "Epoch: 2395. Loss: 0.7219. Acc.: 60.25%\n",
      "Epoch: 2396. Loss: 0.7128. Acc.: 60.39%\n",
      "Epoch: 2397. Loss: 0.7113. Acc.: 60.58%\n",
      "Epoch: 2398. Loss: 0.7109. Acc.: 60.44%\n",
      "Epoch: 2399. Loss: 0.7144. Acc.: 59.97%\n",
      "Epoch: 2400. Loss: 0.7157. Acc.: 60.53%\n",
      "Epoch: 2401. Loss: 0.7168. Acc.: 60.81%\n",
      "Epoch: 2402. Loss: 0.7211. Acc.: 60.44%\n",
      "Epoch: 2403. Loss: 0.7155. Acc.: 59.65%\n",
      "Epoch: 2404. Loss: 0.7186. Acc.: 61.18%\n",
      "Epoch: 2405. Loss: 0.7096. Acc.: 60.99%\n",
      "Epoch: 2406. Loss: 0.7190. Acc.: 60.44%\n",
      "Epoch: 2407. Loss: 0.7211. Acc.: 60.39%\n",
      "Epoch: 2408. Loss: 0.7186. Acc.: 60.62%\n",
      "Epoch: 2409. Loss: 0.7139. Acc.: 60.90%\n",
      "Epoch: 2410. Loss: 0.7114. Acc.: 60.58%\n",
      "Epoch: 2411. Loss: 0.7150. Acc.: 60.62%\n",
      "Epoch: 2412. Loss: 0.7170. Acc.: 60.39%\n",
      "Epoch: 2413. Loss: 0.7245. Acc.: 60.20%\n",
      "Epoch: 2414. Loss: 0.7243. Acc.: 60.11%\n",
      "Epoch: 2415. Loss: 0.7181. Acc.: 60.58%\n",
      "Epoch: 2416. Loss: 0.7151. Acc.: 60.25%\n",
      "Epoch: 2417. Loss: 0.7123. Acc.: 60.62%\n",
      "Epoch: 2418. Loss: 0.7217. Acc.: 60.85%\n",
      "Epoch: 2419. Loss: 0.7190. Acc.: 60.30%\n",
      "Epoch: 2420. Loss: 0.7130. Acc.: 60.71%\n",
      "Epoch: 2421. Loss: 0.7113. Acc.: 60.30%\n",
      "Epoch: 2422. Loss: 0.7123. Acc.: 60.44%\n",
      "Epoch: 2423. Loss: 0.7160. Acc.: 60.44%\n",
      "Epoch: 2424. Loss: 0.7192. Acc.: 60.20%\n",
      "Epoch: 2425. Loss: 0.7104. Acc.: 61.32%\n",
      "Epoch: 2426. Loss: 0.7190. Acc.: 60.48%\n",
      "Epoch: 2427. Loss: 0.7107. Acc.: 60.67%\n",
      "Epoch: 2428. Loss: 0.7097. Acc.: 60.67%\n",
      "Epoch: 2429. Loss: 0.7112. Acc.: 60.39%\n",
      "Epoch: 2430. Loss: 0.7081. Acc.: 60.30%\n",
      "Epoch: 2431. Loss: 0.7039. Acc.: 60.81%\n",
      "Epoch: 2432. Loss: 0.7125. Acc.: 60.67%\n",
      "Epoch: 2433. Loss: 0.7170. Acc.: 60.81%\n",
      "Epoch: 2434. Loss: 0.7203. Acc.: 60.62%\n",
      "Epoch: 2435. Loss: 0.7083. Acc.: 60.71%\n",
      "Epoch: 2436. Loss: 0.7094. Acc.: 60.71%\n",
      "Epoch: 2437. Loss: 0.7111. Acc.: 60.71%\n",
      "Epoch: 2438. Loss: 0.7201. Acc.: 60.53%\n",
      "Epoch: 2439. Loss: 0.7159. Acc.: 60.95%\n",
      "Epoch: 2440. Loss: 0.7084. Acc.: 60.71%\n",
      "Epoch: 2441. Loss: 0.7201. Acc.: 60.44%\n",
      "Epoch: 2442. Loss: 0.7095. Acc.: 60.90%\n",
      "Epoch: 2443. Loss: 0.7120. Acc.: 60.95%\n",
      "Epoch: 2444. Loss: 0.7171. Acc.: 60.99%\n",
      "Epoch: 2445. Loss: 0.7066. Acc.: 60.76%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2446. Loss: 0.7129. Acc.: 60.44%\n",
      "Epoch: 2447. Loss: 0.7240. Acc.: 60.76%\n",
      "Epoch: 2448. Loss: 0.7142. Acc.: 60.67%\n",
      "Epoch: 2449. Loss: 0.7030. Acc.: 60.25%\n",
      "Epoch: 2450. Loss: 0.7131. Acc.: 60.48%\n",
      "Epoch: 2451. Loss: 0.7169. Acc.: 61.09%\n",
      "Epoch: 2452. Loss: 0.7190. Acc.: 60.62%\n",
      "Epoch: 2453. Loss: 0.7114. Acc.: 60.53%\n",
      "Epoch: 2454. Loss: 0.7177. Acc.: 60.67%\n",
      "Epoch: 2455. Loss: 0.7146. Acc.: 60.99%\n",
      "Epoch: 2456. Loss: 0.7035. Acc.: 60.48%\n",
      "Epoch: 2457. Loss: 0.7179. Acc.: 60.71%\n",
      "Epoch: 2458. Loss: 0.7125. Acc.: 60.85%\n",
      "Epoch: 2459. Loss: 0.7155. Acc.: 61.13%\n",
      "Epoch: 2460. Loss: 0.7130. Acc.: 60.99%\n",
      "Epoch: 2461. Loss: 0.7150. Acc.: 60.90%\n",
      "Epoch: 2462. Loss: 0.7069. Acc.: 60.67%\n",
      "Epoch: 2463. Loss: 0.7130. Acc.: 60.62%\n",
      "Epoch: 2464. Loss: 0.7073. Acc.: 60.48%\n",
      "Epoch: 2465. Loss: 0.7021. Acc.: 60.76%\n",
      "Epoch: 2466. Loss: 0.7121. Acc.: 60.53%\n",
      "Epoch: 2467. Loss: 0.7076. Acc.: 60.62%\n",
      "Epoch: 2468. Loss: 0.7211. Acc.: 60.76%\n",
      "Epoch: 2469. Loss: 0.7138. Acc.: 60.95%\n",
      "Epoch: 2470. Loss: 0.7116. Acc.: 60.81%\n",
      "Epoch: 2471. Loss: 0.7149. Acc.: 60.67%\n",
      "Epoch: 2472. Loss: 0.7042. Acc.: 60.48%\n",
      "Epoch: 2473. Loss: 0.7065. Acc.: 60.71%\n",
      "Epoch: 2474. Loss: 0.7101. Acc.: 60.71%\n",
      "Epoch: 2475. Loss: 0.7082. Acc.: 60.30%\n",
      "Epoch: 2476. Loss: 0.7067. Acc.: 60.99%\n",
      "Epoch: 2477. Loss: 0.7088. Acc.: 60.99%\n",
      "Epoch: 2478. Loss: 0.7098. Acc.: 60.76%\n",
      "Epoch: 2479. Loss: 0.7084. Acc.: 60.76%\n",
      "Epoch: 2480. Loss: 0.7110. Acc.: 60.58%\n",
      "Epoch: 2481. Loss: 0.7081. Acc.: 60.71%\n",
      "Epoch: 2482. Loss: 0.7068. Acc.: 60.58%\n",
      "Epoch: 2483. Loss: 0.7114. Acc.: 60.67%\n",
      "Epoch: 2484. Loss: 0.7035. Acc.: 60.62%\n",
      "Epoch: 2485. Loss: 0.7081. Acc.: 60.44%\n",
      "Epoch: 2486. Loss: 0.7078. Acc.: 60.76%\n",
      "Epoch: 2487. Loss: 0.7068. Acc.: 60.85%\n",
      "Epoch: 2488. Loss: 0.7063. Acc.: 60.44%\n",
      "Epoch: 2489. Loss: 0.7119. Acc.: 60.67%\n",
      "Epoch: 2490. Loss: 0.7054. Acc.: 60.67%\n",
      "Epoch: 2491. Loss: 0.7151. Acc.: 60.85%\n",
      "Epoch: 2492. Loss: 0.7083. Acc.: 60.85%\n",
      "Epoch: 2493. Loss: 0.7108. Acc.: 60.95%\n",
      "Epoch: 2494. Loss: 0.7161. Acc.: 60.67%\n",
      "Epoch: 2495. Loss: 0.7074. Acc.: 60.67%\n",
      "Epoch: 2496. Loss: 0.7100. Acc.: 60.85%\n",
      "Epoch: 2497. Loss: 0.7115. Acc.: 60.90%\n",
      "Epoch: 2498. Loss: 0.7093. Acc.: 60.95%\n",
      "Epoch: 2499. Loss: 0.7091. Acc.: 60.90%\n",
      "Epoch: 2500. Loss: 0.6995. Acc.: 60.76%\n",
      "Epoch: 2501. Loss: 0.7143. Acc.: 60.76%\n",
      "Epoch: 2502. Loss: 0.7109. Acc.: 60.85%\n",
      "Epoch: 2503. Loss: 0.7140. Acc.: 60.58%\n",
      "Epoch: 2504. Loss: 0.7113. Acc.: 60.67%\n",
      "Epoch: 2505. Loss: 0.7141. Acc.: 60.71%\n",
      "Epoch: 2506. Loss: 0.7068. Acc.: 60.76%\n",
      "Epoch: 2507. Loss: 0.7067. Acc.: 60.67%\n",
      "Epoch: 2508. Loss: 0.7093. Acc.: 60.71%\n",
      "Epoch: 2509. Loss: 0.7065. Acc.: 60.90%\n",
      "Epoch: 2510. Loss: 0.7009. Acc.: 60.76%\n",
      "Epoch: 2511. Loss: 0.7140. Acc.: 60.53%\n",
      "Epoch: 2512. Loss: 0.7152. Acc.: 60.67%\n",
      "Epoch: 2513. Loss: 0.7127. Acc.: 60.48%\n",
      "Epoch: 2514. Loss: 0.7064. Acc.: 60.58%\n",
      "Epoch: 2515. Loss: 0.7087. Acc.: 60.81%\n",
      "Epoch: 2516. Loss: 0.7066. Acc.: 60.62%\n",
      "Epoch: 2517. Loss: 0.7022. Acc.: 60.67%\n",
      "Epoch: 2518. Loss: 0.7095. Acc.: 60.76%\n",
      "Epoch: 2519. Loss: 0.7009. Acc.: 60.71%\n",
      "Epoch: 2520. Loss: 0.7047. Acc.: 60.71%\n",
      "Epoch: 2521. Loss: 0.7102. Acc.: 60.81%\n",
      "Epoch: 2522. Loss: 0.7090. Acc.: 60.76%\n",
      "Epoch: 2523. Loss: 0.7040. Acc.: 60.58%\n",
      "Epoch: 2524. Loss: 0.7027. Acc.: 60.48%\n",
      "Epoch: 2525. Loss: 0.7178. Acc.: 60.58%\n",
      "Epoch: 2526. Loss: 0.7100. Acc.: 60.85%\n",
      "Epoch: 2527. Loss: 0.6992. Acc.: 60.81%\n",
      "Epoch: 2528. Loss: 0.7038. Acc.: 60.76%\n",
      "Epoch: 2529. Loss: 0.7130. Acc.: 60.76%\n",
      "Epoch: 2530. Loss: 0.7143. Acc.: 60.71%\n",
      "Epoch: 2531. Loss: 0.7030. Acc.: 60.71%\n",
      "Epoch: 2532. Loss: 0.7044. Acc.: 60.81%\n",
      "Epoch: 2533. Loss: 0.7052. Acc.: 60.76%\n",
      "Epoch: 2534. Loss: 0.7131. Acc.: 60.67%\n",
      "Epoch: 2535. Loss: 0.6994. Acc.: 60.71%\n",
      "Epoch: 2536. Loss: 0.7051. Acc.: 60.71%\n",
      "Epoch: 2537. Loss: 0.7093. Acc.: 60.67%\n",
      "Epoch: 2538. Loss: 0.7063. Acc.: 60.62%\n",
      "Epoch: 2539. Loss: 0.7037. Acc.: 60.71%\n",
      "Epoch: 2540. Loss: 0.7051. Acc.: 60.67%\n",
      "Epoch: 2541. Loss: 0.7062. Acc.: 60.53%\n",
      "Epoch: 2542. Loss: 0.7139. Acc.: 60.67%\n",
      "Epoch: 2543. Loss: 0.7051. Acc.: 60.81%\n",
      "Epoch: 2544. Loss: 0.7104. Acc.: 60.71%\n",
      "Epoch: 2545. Loss: 0.7085. Acc.: 60.67%\n",
      "Epoch: 2546. Loss: 0.7048. Acc.: 60.71%\n",
      "Epoch: 2547. Loss: 0.7111. Acc.: 60.67%\n",
      "Epoch: 2548. Loss: 0.7066. Acc.: 60.81%\n",
      "Epoch: 2549. Loss: 0.7068. Acc.: 60.85%\n",
      "Epoch: 2550. Loss: 0.7035. Acc.: 60.90%\n",
      "Epoch: 2551. Loss: 0.7054. Acc.: 60.81%\n",
      "Epoch: 2552. Loss: 0.7030. Acc.: 60.81%\n",
      "Epoch: 2553. Loss: 0.7074. Acc.: 60.85%\n",
      "Epoch: 2554. Loss: 0.7047. Acc.: 60.76%\n",
      "Epoch: 2555. Loss: 0.7273. Acc.: 61.18%\n",
      "Epoch: 2556. Loss: 0.7374. Acc.: 60.95%\n",
      "Epoch: 2557. Loss: 0.7312. Acc.: 60.90%\n",
      "Epoch: 2558. Loss: 0.7361. Acc.: 61.69%\n",
      "Epoch: 2559. Loss: 0.7336. Acc.: 61.13%\n",
      "Epoch: 2560. Loss: 0.7311. Acc.: 59.83%\n",
      "Epoch: 2561. Loss: 0.7410. Acc.: 60.71%\n",
      "Epoch: 2562. Loss: 0.7394. Acc.: 59.51%\n",
      "Epoch: 2563. Loss: 0.7476. Acc.: 59.32%\n",
      "Epoch: 2564. Loss: 0.7451. Acc.: 59.79%\n",
      "Epoch: 2565. Loss: 0.7446. Acc.: 60.11%\n",
      "Early stopping on epoch 2565\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print('Start model training')\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(train_dl):\n",
    "        x_raw, x_fft, y_batch = [t.to(device) for t in batch]\n",
    "        \n",
    "        opt.zero_grad()\n",
    "\n",
    "        out = model(x_raw, x_fft)\n",
    "        \n",
    "        loss = criterion(out, y_batch)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        #sched.step()\n",
    "        \n",
    "    epoch_loss /= len(train_ds)\n",
    "    loss_history.append(epoch_loss)\n",
    "    \n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for batch in valid_dl:\n",
    "        x_raw, x_fft, y_batch = [t.to(device) for t in batch]\n",
    "        out = model(x_raw, x_fft)\n",
    "        \n",
    "        preds = F.softmax(out, dim=1).argmax(dim=1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "    \n",
    "    acc = correct / total\n",
    "    acc_history.append(acc)\n",
    "\n",
    "    #if epoch % base == 0:\n",
    "    print(f'Epoch: {epoch:3d}. Loss: {epoch_loss:.4f}. Acc.: {acc:2.2%}')\n",
    "    #base *= step\n",
    "\n",
    "    if acc > best_acc:\n",
    "        trials = 0\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), 'best.pth')\n",
    "        print(f'Epoch {epoch} best model saved with accuracy: {best_acc:2.2%}')\n",
    "    else:\n",
    "        trials += 1\n",
    "        if trials >= patience:\n",
    "            print(f'Early stopping on epoch {epoch}')\n",
    "            break\n",
    "            \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on validation dataset\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "y_eval = []\n",
    "print('Predicting on validation dataset')\n",
    "for batch in valid_dl:\n",
    "    x_raw, x_fft, y_batch = [t.to(device) for t in batch]\n",
    "    out = model(x_raw, x_fft)\n",
    "    #print(out.view())\n",
    "    preds += F.softmax(out, dim=1).tolist()\n",
    "preds = np.array(preds,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import evaluate_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   macro_auc      Fmax\n",
      "0   0.834179  0.697502\n"
     ]
    }
   ],
   "source": [
    "tr_df_point = evaluate_experiment(np.array(y_val_eval,dtype=np.float32), np.array(preds,dtype=np.float32))\n",
    "print(tr_df_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To test: add a branch with wavelet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To test: Search for something like mel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To test: add a branch with images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
