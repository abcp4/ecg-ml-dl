{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys, os\n",
    "import tsai\n",
    "from tsai.all import *\n",
    "display(HTML(\"<style>.container {width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('../../'))\n",
    "from utils import utils\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from timeseries_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tsai       : 0.1.0\n",
      "fastai2    : 0.0.17\n",
      "fastcore   : 0.1.17\n",
      "torch      : 1.4.0\n",
      "scipy      : 1.4.1\n",
      "numpy      : 1.18.1\n",
      "pandas     : 1.0.4\n",
      "Total RAM  : 62.75 GB\n",
      "Used RAM   :  4.04 GB\n",
      "n_cpus     : 16\n",
      "device     : cuda (GeForce GTX 1080)\n"
     ]
    }
   ],
   "source": [
    "print('tsai       :', tsai.__version__)\n",
    "print('fastai2    :', fastai2.__version__)\n",
    "print('fastcore   :', fastcore.__version__)\n",
    "print('torch      :', torch.__version__)\n",
    "print('scipy      :', sp.__version__)\n",
    "print('numpy      :', np.__version__)\n",
    "print('pandas     :', pd.__version__)\n",
    "print(f'Total RAM  : {bytes2GB(psutil.virtual_memory().total):5.2f} GB')\n",
    "print(f'Used RAM   : {bytes2GB(psutil.virtual_memory().used):5.2f} GB')\n",
    "print('n_cpus     :', cpus)\n",
    "iscuda = torch.cuda.is_available()\n",
    "if iscuda: print('device     : {} ({})'.format(device, torch.cuda.get_device_name(0)))\n",
    "else: print('device     :', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data='../../../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.load(os.path.abspath(path_to_data+'data-002.npy'),allow_pickle=True)\n",
    "Y=np.load(path_to_data+'Y.npy',allow_pickle=True)\n",
    "labels =pd.read_csv(path_to_data+'labels.csv')\n",
    "\n",
    "train_fold=8\n",
    "val_fold=9\n",
    "test_fold=10\n",
    "\n",
    "# 10th fold for testing (9th for now)\n",
    "X_test = data[labels.strat_fold == test_fold]\n",
    "y_test = Y[labels.strat_fold == test_fold]\n",
    "# 9th fold for validation (8th for now)\n",
    "X_val = data[labels.strat_fold == val_fold]\n",
    "y_val = Y[labels.strat_fold == val_fold]\n",
    "# rest for training\n",
    "X_train = data[labels.strat_fold <= train_fold]\n",
    "y_train = Y[labels.strat_fold <= train_fold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess signal data\n",
    "X_train, X_val, X_test = utils.preprocess_signals(X_train, X_val, X_test,'/content/')\n",
    "n_classes = y_train.shape[1]\n",
    "X_train = np.reshape(X_train,[X_train.shape[0],X_train.shape[2],X_train.shape[1]])\n",
    "X_val = np.reshape(X_val,[X_val.shape[0],X_val.shape[2],X_val.shape[1]])\n",
    "X_test = np.reshape(X_test,[X_test.shape[0],X_test.shape[2],X_test.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X_train.shape)\n",
    "_, features, seq_len = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32, device=device)\n",
    "labels = np.unique(y_train)\n",
    "transform = {}\n",
    "for i, l in enumerate(labels): transform[l] = i\n",
    "y_train = np.vectorize(transform.get)(y_train)\n",
    "y_val = np.vectorize(transform.get)(y_val)\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UCR:(torch.Size([160, 10, 400]), (160,), torch.Size([74, 10, 400]), (74,))\n",
    "\n",
    "ECG:(torch.Size([17111, 12, 1000]),(17111, 5),torch.Size([2156, 12, 1000]),(2156, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_kernels=10_000\n",
    "kss=[7, 9, 11]\n",
    "model = ROCKET(features, seq_len, n_kernels=n_kernels, kss=kss).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfm = model(X_train).cpu().numpy()\n",
    "X_val_tfm = model(X_val).cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_to_data + 'features_X_train.pkl','wb') as handle:\n",
    "    pickle.dump(X_train_tfm,handle,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(path_to_data + 'features_X_val.pkl','wb') as handle:\n",
    "    pickle.dump(X_val_tfm,handle,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(path_to_data + 'y_train.pkl','wb') as handle:\n",
    "    pickle.dump(y_train,handle,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(path_to_data + 'y_val.pkl','wb') as handle:\n",
    "    pickle.dump(y_val,handle,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_to_data + 'y_val.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_to_data + 'features_X_train.pkl','rb') as handle:\n",
    "    X_train_tfm = pickle.load(handle)\n",
    "\n",
    "with open(path_to_data + 'features_X_val.pkl','rb') as handle:\n",
    "    X_val_tfm = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_to_data + 'y_train.pkl','rb') as handle:\n",
    "    y_train = pickle.load(handle)\n",
    "\n",
    "with open(path_to_data + 'y_val.pkl','rb') as handle:\n",
    "    y_val = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfm = X_train_tfm.reshape((X_train_tfm.shape[0],X_train_tfm.shape[1],1))\n",
    "X_val_tfm = X_val_tfm.reshape((X_val_tfm.shape[0],X_val_tfm.shape[1],1))\n",
    "y_train = y_train.reshape((y_train.shape[0], 1, y_train.shape[1]))\n",
    "y_val = y_val.reshape((y_val.shape[0], 1, y_val.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_tfm.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actially running a classifier on the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(random_state=1,hidden_layer_sizes=(2000, 3000,1000,1000,))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mlp.fit(X_train_tfm, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mlp.score(X_train_tfm, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pred = mlp.predict(X_val_tfm)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch MLP"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import tsai\n",
    "from tsai.all import *\n",
    "from tsai.imports import *\n",
    "from tsai.models.layers import *\n",
    "from fastai.layers import *\n",
    "from fastai.core import *\n",
    "dsid = 'OliveOil'\n",
    "bs = 30\n",
    "eps = 1e-6"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# extract data\n",
    "X_train, y_train, X_val, y_val = get_UCR_data(dsid)\n",
    "\n",
    "# normalize data 'per sample'\n",
    "X_train = (X_train - X_train.mean(axis=(1, 2), keepdims=True)) / (\n",
    "    X_train.std(axis=(1, 2), keepdims=True) + eps)\n",
    "X_val = (X_val - X_val.mean(axis=(1, 2), keepdims=True)) / (\n",
    "    X_val.std(axis=(1, 2), keepdims=True) + eps)\n",
    "\n",
    "# calculate 20k features\n",
    "_, features, seq_len = X_train.shape\n",
    "model = ROCKET(features, seq_len, n_kernels=10000, kss=[7, 9, 11]).to(device)\n",
    "X_train_tfm = model(torch.tensor(X_train, device=device)).unsqueeze(1).cpu().numpy()\n",
    "X_val_tfm = model(torch.tensor(X_val, device=device)).unsqueeze(1).cpu().numpy()\n",
    "\n",
    "# normalize 'per feature'\n",
    "f_mean = X_train_tfm.mean(axis=0, keepdims=True)\n",
    "f_std = X_train_tfm.std(axis=0, keepdims=True) + eps\n",
    "X_train_tfm_norm = (X_train_tfm - f_mean) / f_std\n",
    "X_val_tfm_norm = (X_val_tfm - f_mean) / f_std\n",
    "\n",
    "# To use fastai v2 we concatenate train and val, and calculate the splits:\n",
    "X = np.concatenate((X_train_tfm_norm, X_val_tfm_norm))\n",
    "y = np.concatenate((y_train, y_val))\n",
    "splits = (L(list(np.arange(len(X_train)))), L(list(np.arange(len(X_train), len(X)))))\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X, y, splits = combine_split_data([X_train_tfm, X_val_tfm], [y_train.astype(int), y_val.astype(int)])\n",
    "tfms  = [None, [Categorize()]]\n",
    "dsets = TSDatasets(X, y, tfms=tfms, splits=splits, inplace=True)\n",
    "dls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[20], batch_tfms=[TSStandardize()], num_workers=0)\n",
    "learn = Learner(dls, model,metrics=accuracy)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tfms  = [None, [Categorize()]]\n",
    "dsets = TSDatasets(X, y, tfms=tfms, splits=splits, inplace=True)\n",
    "dls   = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=30, num_workers=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# normalize 'per feature'\n",
    "f_mean = X_train_tfm.mean(axis=0, keepdims=True)\n",
    "f_std = X_train_tfm.std(axis=0, keepdims=True) + eps\n",
    "X_train_tfm_norm = (X_train_tfm - f_mean) / f_std\n",
    "X_val_tfm_norm = (X_val_tfm - f_mean) / f_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list = [l.astype(np.float32) for l in X_train_tfm]\n",
    "X_val_list = [l.astype(np.float32) for l in X_val_tfm]\n",
    "y_train_list = [l.astype(np.float32) for l in y_train]\n",
    "y_val_list = [l.astype(np.float32) for l in y_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfm.shape, X_val_tfm.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train_list),len(y_train_list),len(X_val_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.basic_data import *\n",
    "from fastai.basic_train import *\n",
    "from fastai.train import *\n",
    "from fastai.metrics import *\n",
    "from fastai.torch_core import *\n",
    "metrics = []\n",
    "wd=1e-2\n",
    "input_size=X_train_tfm.shape[1]\n",
    "#batchsize\n",
    "bs=128\n",
    "\n",
    "chunkify_train=False\n",
    "chunkify_valid=True\n",
    "chunk_length_valid=input_size\n",
    "min_chunk_length=input_size#chunk_length\n",
    "\n",
    "stride_length_train=input_size#chunk_length_train//8\n",
    "stride_length_valid=input_size//2#chunk_length_valid\n",
    "\n",
    "\n",
    "df_train = pd.DataFrame({\"data\":range(len(X_train_list)),\"label\":y_train_list})\n",
    "df_valid = pd.DataFrame({\"data\":range(len(X_val_list)),\"label\":y_val_list})\n",
    "\n",
    "tfms_ptb_xl = [ToTensor()]\n",
    "\n",
    "ds_train=TimeseriesDatasetCrops(df_train,input_size,num_classes=5,chunk_length= 0,min_chunk_length=min_chunk_length,stride=stride_length_train,transforms=tfms_ptb_xl,annotation=False,col_lbl =\"label\",npy_data=X_train_list)\n",
    "ds_valid=TimeseriesDatasetCrops(df_valid,input_size,num_classes=5,chunk_length=chunk_length_valid,min_chunk_length=min_chunk_length,stride=stride_length_valid,transforms=tfms_ptb_xl,annotation=False,col_lbl =\"label\",npy_data=X_val_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DataBunch.create(ds_train,ds_valid,bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer perceptron\n",
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(20000, 3000, bias=True) \n",
    "        self.lin2 = nn.Linear(3000, 3000, bias=True)\n",
    "        self.lin3 = nn.Linear(3000, 1000, bias=True)\n",
    "        self.lin4 = nn.Linear(1000, 5, bias=True)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        xb = xb.float()\n",
    "        xb = F.relu(self.lin1(xb))\n",
    "        xb = F.relu(self.lin2(xb))\n",
    "        xb = F.relu(self.lin3(xb))\n",
    "        xb = self.lin4(xb)\n",
    "        return xb#xb.view(xb.size()[0], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultilayerPerceptron() #nn.Sequential(*[nn.Linear(20_000, dls.c,), Squeeze(1)])\n",
    "loss = F.binary_cross_entropy_with_logits\n",
    "\n",
    "#learn = Learner(db,model, loss_func=loss, metrics=metrics,wd=wd,path='/content/models')\n",
    "learn = Learner(db, model, metrics=accuracy, loss_func=loss)\n",
    "\n",
    "learn.save('stage-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_tfm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_tensor = torch.tensor(X_val_tfm[4], dtype=torch.float32, device=device)\n",
    "model(X_val_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.load('stage-0')\n",
    "learn.fit_one_cycle(10, max_lr=3e-3, wd=1e2)\n",
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.shape,pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_result = utils.evaluate_experiment(y_val,pred)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
